{"meta":{"title":"lwy的博客","subtitle":"欢迎大家与我交流","description":"茫茫人海，总会与你相遇","author":"lwy","url":"https://lwy0518.github.io","root":"/"},"pages":[{"title":"与我相关","date":"2021-12-07T02:15:02.000Z","updated":"2021-12-22T03:33:24.416Z","comments":true,"path":"about/index.html","permalink":"https://lwy0518.github.io/about/index.html","excerpt":"","text":"🏠基本信息 姓名：李文远 性别：男 爱好：打乒乓球、听歌 … … ✔技能 java SQL ❓ 了解我 Github：https://github.com/lwy0518 QQ：1024325635 WeChat：15946947900 知乎： https://www.zhihu.com/people/nan-cheng-ni-liu-86-1 🏫教育经历 2020.9 - 至今 ： 华南师范大学 2016.9 - 2020.7 ： 江西理工大学 2013.9 - 2016.7 ：临川一中 2010.9 - 2013.7 ： 大岗中学 … …"},{"title":"404","date":"2021-12-06T07:38:28.000Z","updated":"2021-12-06T07:39:37.088Z","comments":true,"path":"404/index.html","permalink":"https://lwy0518.github.io/404/index.html","excerpt":"","text":"title: ‘404’permalink: /404date: 2020-10-16 15:19:35comments: falselayout: false ---"},{"title":"book","date":"2021-12-07T11:10:56.000Z","updated":"2021-12-07T11:10:56.396Z","comments":true,"path":"book/index.html","permalink":"https://lwy0518.github.io/book/index.html","excerpt":"","text":""},{"title":"留言板","date":"2021-12-07T06:33:36.000Z","updated":"2021-12-08T07:21:43.539Z","comments":true,"path":"guestbook/index.html","permalink":"https://lwy0518.github.io/guestbook/index.html","excerpt":"","text":"欢迎来到我的博客！ 欢迎在这里留言！任何问题都可以在这里留言，我会及时回复的，添加QQ或微信可以获得更快的回复速度，在侧边栏扫描二维码即可。"},{"title":"categories","date":"2021-12-06T12:13:37.000Z","updated":"2021-12-06T12:14:34.617Z","comments":true,"path":"categories/index.html","permalink":"https://lwy0518.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2021-12-06T12:14:59.000Z","updated":"2021-12-06T12:15:17.173Z","comments":true,"path":"tags/index.html","permalink":"https://lwy0518.github.io/tags/index.html","excerpt":"","text":""},{"title":"Metaverse（元宇宙）","date":"2021-12-07T09:22:30.000Z","updated":"2021-12-07T11:32:55.974Z","comments":true,"path":"top/index.html","permalink":"https://lwy0518.github.io/top/index.html","excerpt":"","text":"元宇宙"},{"title":"tools","date":"2021-12-07T10:51:44.000Z","updated":"2022-01-16T07:56:09.699Z","comments":true,"path":"tools/index.html","permalink":"https://lwy0518.github.io/tools/index.html","excerpt":"","text":"latex 在线表格生成器 latex 在线公式生成器 markdown 在线编辑 抽象语法树在线生成 阿里云加速 ReadPaper PapersWithCode 在线EVM字节码转Wasm字节码"},{"title":"navigate","date":"2022-01-18T03:42:45.000Z","updated":"2022-01-18T03:42:45.151Z","comments":true,"path":"navigate/index.html","permalink":"https://lwy0518.github.io/navigate/index.html","excerpt":"","text":""}],"posts":[{"title":"docker高级篇学习笔记","slug":"docker高级篇学习笔记","date":"2022-02-13T13:06:53.000Z","updated":"2022-02-13T13:50:15.506Z","comments":true,"path":"2022/02/13/docker高级篇学习笔记/","link":"","permalink":"https://lwy0518.github.io/2022/02/13/docker%E9%AB%98%E7%BA%A7%E7%AF%87%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"目录","text":"目录 Docker 高级篇一、Docker复杂安装1.1 安装mysql主从复制1.1.1 主从复制原理1默认你已经了解 1.1.2 主从搭建步骤 1、新建主服务器容器实例3307 123456&gt;docker run -p 3307:3306 --name mysql-master \\ &gt;-v /mydata/mysql-master/log:/var/log/mysql \\ &gt;-v /mydata/mysql-master/data:/var/lib/mysql \\ &gt;-v /mydata/mysql-master/conf:/etc/mysql \\ &gt;-e MYSQL_ROOT_PASSWORD=root \\ &gt;-d mysql:5.7 2、进入/mydata/mysql-master/conf目录下新建my.cnf 12345678910111213141516171819202122&gt;[mysqld] &gt;## 设置server_id，同一局域网中需要唯一 &gt;server_id=101 &gt;## 指定不需要同步的数据库名称 &gt;binlog-ignore-db=mysql &gt;## 开启二进制日志功能 &gt;log-bin=mall-mysql-bin &gt;## 设置二进制日志使用内存大小（事务） &gt;binlog_cache_size=1M &gt;## 设置使用的二进制日志格式（mixed,statement,row） &gt;binlog_format=mixed &gt;## 二进制日志过期清理时间。默认值为0，表示不自动清理。 &gt;expire_logs_days=7 &gt;## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 &gt;## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 &gt;slave_skip_errors=1062 &gt;## 设置utf8&gt;collation_server = utf8_general_ci &gt;## 设置server字符集&gt;character_set_server = utf8 &gt;[client]&gt;default_character_set=utf8 3、修改完配置后重启master实例 1&gt;docker restart mysql-master 4、进入mysql-master容器 123&gt;docker exec -it mysql-master /bin/bash&gt;mysql -uroot -proot 5、maser容器实例内创建数据同步用户 12345&gt;# 创建同步用户&gt;CREATE USER &#x27;slave&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;123456&#x27;;&gt;# 同步用户授权&gt;GRANT REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO &#x27;slave&#x27;@&#x27;%&#x27;; 6、新建从服务容器实例3308 123456&gt;docker run -p 3308:3306 --name mysql-slave \\ &gt;-v /mydata/mysql-slave/log:/var/log/mysql \\ &gt;-v /mydata/mysql-slave/data:/var/lib/mysql \\ &gt;-v /mydata/mysql-slave/conf:/etc/mysql \\ &gt;-e MYSQL_ROOT_PASSWORD=root \\ &gt;-d mysql:5.7 7、进入/mydata/mysql-slave/conf目录下新建my.cnf 1234567891011121314151617181920212223242526272829&gt;# 添加配置文件&gt;[mysqld] &gt;## 设置server_id，同一局域网中需要唯一 &gt;server_id=102 &gt;## 指定不需要同步的数据库名称 &gt;binlog-ignore-db=mysql &gt;## 开启二进制日志功能，以备Slave作为其它数据库实例的Master时使用 &gt;log-bin=mall-mysql-slave1-bin &gt;## 设置二进制日志使用内存大小（事务） &gt;binlog_cache_size=1M &gt;## 设置使用的二进制日志格式（mixed,statement,row） &gt;binlog_format=mixed &gt;## 二进制日志过期清理时间。默认值为0，表示不自动清理。 &gt;expire_logs_days=7 &gt;## 跳过主从复制中遇到的所有错误或指定类型的错误，避免slave端复制中断。 &gt;## 如：1062错误是指一些主键重复，1032错误是因为主从数据库数据不一致 &gt;slave_skip_errors=1062 &gt;## relay_log配置中继日志 &gt;relay_log=mall-mysql-relay-bin &gt;## log_slave_updates表示slave将复制事件写进自己的二进制日志 &gt;log_slave_updates=1 &gt;## slave设置为只读（具有super权限的用户除外） &gt;read_only=1 &gt;## 设置utf8&gt;collation_server = utf8_general_ci &gt;## 设置server字符集&gt;character_set_server = utf8 &gt;[client]&gt;default_character_set=utf8 8、修改完配置后重启slave实例 1&gt;docker restart mysql-slave 9、在主数据库中查看主从同步状态 1&gt;show master status; 10、进入mysql-slave容器 12&gt;docker exec -it mysql-slave /bin/bash&gt;mysql -uroot -proot 11、在从数据库中配置主从复制 1&gt;change master to master_host=&#x27;宿主机ip&#x27;, master_user=&#x27;slave&#x27;, master_password=&#x27;123456&#x27;, master_port=3307, master_log_file=&#x27;mall-mysql-bin.000001&#x27;, master_log_pos=617, master_connect_retry=30; 12、在从数据库中查看主从同步状态 1&gt;show slave status\\G; 13、在从数据库中开启主从同步 1&gt;start slave; 14、查看从数据库状态发现已经同步 15、主从复制测试 12&gt;1.主机新建数据库 ---&gt; 使用数据库 ---&gt; 新建表 ---&gt;插入数据 ， ok&gt;2.从机使用库 ---&gt; 查看记录 ok 1.2 安装redis集群(大厂面试题第4季-分布式存储案例真题)1.2.1 cluster(集群)模式-docker版哈希槽分区进行亿级数据存储 一、面试题 问题：1~2亿条数据需要缓存，请问如何设计这个存储案例 回答：单机单台100%不可能，肯定是分布式存储，用redis如何落地？ 上述问题阿里P6~P7工程案例和场景设计类必考题目，一般业界有3种解决方案 哈希取余分区 12345678910&gt;2亿条记录就是2亿个k,v，我们单机不行必须要分布式多机，假设有3台机器构成一个集群，用户每次读写操作都是根据公式：&gt;hash(key) % N个机器台数，计算出哈希值，用来决定数据映射到哪一个节点上。 &gt;优点：简单粗暴，直接有效，只需要预估好数据规划好节点，例如3台、8台、10台，就能保证一段时间的数据支撑。使用Hash算法让固定的一部分请求落到同一台服务器上，这样每台服务器固定处理一部分请求（并维护这些请求的信息），起到负载均衡+分而治之的作用。 &gt;缺点： 原来规划好的节点，进行扩容或者缩容就比较麻烦了额，不管扩缩，每次数据变动导致节点有变动，映射关系需要重新进行计算，在服务器个数固定不变时没有问题，如果需要弹性扩容或故障停机的情况下，原来的取模公式就会发生变化：Hash(key)/3会变成Hash(key) /?。此时地址经过取余运算的结果将发生很大变化，根据公式获取的服务器也会变得不可控。 &gt;某个redis机器宕机了，由于台数数量变化，会导致hash取余全部数据重新洗牌。 一致性哈希算法分区 1、是什么 一致性Hash算法背景 一致性哈希算法在1997年由麻省理工学院中提出的，设计目标是为了解决 分布式缓存数据 变动和映射问题 ，某个机器宕机了，分母数量改变了，自然取余数不OK了。 2、能干什么 提出一致性Hash解决方案。目的是当服务器个数发生变动时，尽量减少影响客户端到服务器的映射关系。 3、3大步骤 【算法构建一致性哈希环】 一致性哈希算法必然有个hash函数并按照算法产生hash值，这个算法的所有可能哈希值会构成一个全量集，这个集合可以成为一个hash空间[0,2^32-1]，这个是一个线性空间，但是在算法中，我们通过适当的逻辑控制将它首尾相连(0 = 2^32),这样让它逻辑上形成了一个环形空间。 它也是按照使用取模的方法，前面笔记介绍的节点取模法是对节点（服务器）的数量进行取模。而一致性Hash算法是对2^32取模，简单来说， 一致性Hash算法将整个哈希值空间组织成一个虚拟的圆环 ，如假设某哈希函数H的值空间为0-2^32-1（即哈希值是一个32位无符号整形），整个哈希环如下图：整个空间 按顺时针方向组织 ，圆环的正上方的点代表0，0点右侧的第一个点代表1，以此类推，2、3、4、……直到2^32-1，也就是说0点左侧的第一个点代表2^32-1， 0和2^32-1在零点中方向重合，我们把这个由2^32个点组成的圆环称为Hash环。 【服务器IP节点映射】 将集群中各个IP节点映射到环上的某一个位置。 将各个服务器使用Hash进行一个哈希，具体可以选择服务器的IP或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置。假如4个节点NodeA、B、C、D，经过IP地址的 哈希函数 计算(hash(ip))，使用IP地址哈希后在环空间的位置如下： 【key落到服务器的落键规则】 当我们需要存储一个kv键值对时，首先计算key的hash值，hash(key)，将这个key使用相同的函数Hash计算出哈希值并确定此数据在环上的位置， 从此位置沿环顺时针“行走” ，第一台遇到的服务器就是其应该定位到的服务器，并将该键值对存储在该节点上。如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下：根据一致性Hash算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。 4、优点 一致性哈希算法的容错性 1&gt;假设Node C宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性Hash算法中，如果一台服务器不可用，则 受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据 ，其它不会受到影响。简单说，就是C挂了，受到影响的只是B、C之间的数据，并且这些数据会转移到D进行存储。 一致性哈希算法的扩展性 12&gt;数据量增加了，需要增加一台节点NodeX，X的位置在A和B之间，那收到影响的也就是A到X之间的数据，重新把A到X的数据录入到X上即可， &gt;不会导致hash取余全部数据重新洗牌。 5、缺点 一致性哈希算法的数据倾斜问题 12&gt;一致性Hash算法在服务 节点太少时 ，容易因为节点分布不均匀而造成 数据倾斜 （被缓存的对象大部分集中缓存在某一台服务器上）问题， &gt;例如系统中只有两台服务器： 6、小总结 1234567891011&gt;为了在节点数目发生改变时尽可能少的迁移数据 &gt;将所有的存储节点排列在收尾相接的Hash环上，每个key在计算Hash后会 顺时针 找到临近的存储节点存放。 &gt;而当有节点加入或退出时仅影响该节点在Hash环上 顺时针相邻的后续节点 。 &gt;优点 &gt;加入和删除节点只影响哈希环中顺时针方向的相邻的节点，对其他节点无影响。 &gt;缺点 &gt;数据的分布和节点的位置有关，因为这些节点不是均匀的分布在哈希环上的，所以数据在进行存储时达不到均匀分布的效果。 哈希槽分区 1234561. 为什么出现一致性哈希算法的数据倾斜问题哈希槽是指就是一个数组，数组[0,2^14-1]形成的hash slot空间。2. 能干什么解决均匀分配的问题，在数据和节点之间有加入了一层，把这层称为哈希槽(slot)，用于管理数据和节点之间的关系，现在就相当于节点上放的是槽，槽里放的是数据。 1234槽解决的是粒度问题，相当于把粒度变大了，这样便于数据移动。 哈希解决的是映射问题，使用key的哈希值来计算所在的槽，便于数据分配。3. 多少个hash槽 一个集群只能有16384个槽，编号0-16383（0-2^14-1）。这些槽会分配给集群中的所有主节点，分配策略没有要求。可以指定哪些编号的槽分配给哪个主节点。集群会记录节点和槽的对应关系。解决了节点和槽的关系后，接下来就需要对key求哈希值，然后对16384取余，余数是几key就落入对应的槽里。slot = CRC16(key) % 16384。以槽为单位移动数据，因为槽的数目是固定的，处理起来比较容易，这样数据移动问题就解决了。 1.2.2 redis集群3主3从扩缩容配置案例 一、关闭防火墙+启动docker后台服务 1&gt;systemctl start docker 二、新建6个docker容器redis实例 12345678910111213141516&gt;# 创建并运行docker容器实例&gt;docker run &gt;# 容器名字&gt;--name redis-node-6&gt;# 使用宿主机的IP和端口，默认&gt;--net host&gt;# 获取宿主机root用户权限&gt;--privileged=true&gt;# 容器卷，宿主机地址:docker内部地址&gt;-v /data/redis/share/redis-node-6:/data&gt;# redis镜像和版本号&gt;redis:6.0.8&gt;# 开启redis集群&gt;--cluster-enabled yes &gt;# 开启持久化&gt;--applendonly yes 1234567891011&gt;docker run -d --name redis-node-1 --net host --privileged=true -v /data/redis/share/redis-node-1:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6381 &gt;docker run -d --name redis-node-2 --net host --privileged=true -v /data/redis/share/redis-node-2:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6382 &gt;docker run -d --name redis-node-3 --net host --privileged=true -v /data/redis/share/redis-node-3:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6383 &gt;docker run -d --name redis-node-4 --net host --privileged=true -v /data/redis/share/redis-node-4:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6384 &gt;docker run -d --name redis-node-5 --net host --privileged=true -v /data/redis/share/redis-node-5:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6385 &gt;docker run -d --name redis-node-6 --net host --privileged=true -v /data/redis/share/redis-node-6:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6386 三、进入容器redis-node-1并为6台机器构建集群关系 12&gt;# 进入容器&gt;docker exec -it redis-node-1 /bin/bash //注意，进入docker容器后才能执行一下命令，且注意自己的真实IP地址 1&gt;redis-cli --cluster create 192.168.111.147:6381 192.168.111.147:6382 192.168.111.147:6383 192.168.111.147:6384 192.168.111.147:6385 192.168.111.147:6386 --cluster-replicas 1 –cluster-replicas 1 表示为每个master创建一个slave节点 四、连接进入6318作为切入点，查看集群状态 123&gt;cluster info&gt;cluster nodes 1.2.3 主从容错切换迁移案例 一、数据读写存储 启动6机构成的集群并通过exec进入 对6381新增两个key 防止路由时效加参数-c并新增连个key 查看集群信息 1&gt;redis-cli --cluster check 192.168.111.147:6381 二、容错切换迁移 主6381和从机切换，先停止主机6381 12&gt;6381主机停了，对应的真实从机上位&gt;6381作为1号主机分配的从机以实际情况为准，具体是几号机器就是几号 再次查看集群信息 6381宕机了，6385上位成为了新的master。 备注：本次脑图笔记6381为主下面挂从6385 。 每次案例下面挂的从机以实际情况为准，具体是几号机器就是几号 先还原之前的3主3从 1234567&gt;# 先启6381&gt;docker start redis-node-1&gt;# 再停6385 &gt;docker stop redis-node-5&gt;# 再起6385&gt;docker start redis-node-5&gt;主从机器分配情况一实际情况为准 查看集群状态 1redis-cli --cluster check 自己IP:6381 1.2.4 主从扩容案例 一、新建6387、6388两个节点+新建后启动+查看是否8节点 12345&gt;docker run -d --name redis-node-7 --net host --privileged=true -v /data/redis/share/redis-node-7:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6387 &gt;docker run -d --name redis-node-8 --net host --privileged=true -v /data/redis/share/redis-node-8:/data redis:6.0.8 --cluster-enabled yes --appendonly yes --port 6388 &gt;docker ps 二、进入6387容器实例内部 1&gt;docker exec -it redis-node-7 /bin/bash 三、将新增的6387节点(空槽号)作为master节点加入原集群 1234&gt;将新增的6387作为master节点加入集群&gt;redis-cli --cluster add-node 自己实际IP地址: 6387 自己实际IP地址: 6381 &gt;6387 就是将要作为master新增节点 &gt;6381 就是原来集群节点里面的领路人，相当于6387拜拜6381的码头从而找到组织加入集群 四、检查集群情况第1次 123&gt;redis-cli --cluster check 真实ip地址:6381 &gt;# 例如&gt;redis-cli --cluster check 192.168.111.147:6381 五、重新分派槽号 123&gt;重新分派槽号&gt;命令:redis-cli --cluster reshard IP地址:端口号 &gt;redis-cli --cluster reshard 192.168.111.147:6381 六、检查集群情况第2次 1234&gt;redis-cli --cluster check 真实ip地址:6381 &gt;为什么6387是3个新的区间，以前的还是连续？&gt;重新分配成本太高，所以前3家各自匀出来一部分，从6381/6382/6383三个旧节点分别匀出1364个坑位给新节点6387 七、为主节点6387分配从节点6388 123&gt;命令：redis-cli --cluster add-node ip:新slave端口 ip:新master端口 --cluster-slave --cluster-master-id 新主机节点ID&gt;redis-cli --cluster add-node 192.168.111.147:6388 192.168.111.147:6387 --cluster-slave --cluster-master-id e4781f644d4a4e4d4b4d107157b9ba8144631451-------这个是6387的编号，按照自己实际情况 八、检查集群情况第3次 1&gt;redis-cli --cluster check 192.168.111.147:6382 1.2.5 主从缩容案例 一、目的：6387和6388下线 二、检查集群情况1获得6388的节点ID 1&gt;redis-cli --cluster check 192.168.111.147:6382 三、将6388删除 从集群中将4号从节点6388删除 1234567&gt;命令：redis-cli --cluster del-node ip:从机端口 从机6388节点ID&gt;redis-cli --cluster del-node 192.168.111.147:6388 5d149074b7e57b802287d1797a874ed7a1a284a8 &gt;redis-cli --cluster check 192.168.111.147:6382 &gt;检查一下发现，6388被删除了，只剩下7台机器了。 四、将6387的槽号清空，重新分配，本例将清出来的槽号都给6381 1&gt;redis-cli --cluster reshard 192.168.111.147:6381 五、检查集群情况第二次 123&gt;redis-cli --cluster check 192.168.111.147:6381&gt;4096个槽位都指给6381，它变成了8192个槽位，相当于全部都给6381了，不然要输入3次，一锅端 六、将6387删除 12&gt;# 命令：redis-cli --cluster del-node ip:端口 6387节点ID&gt;redis-cli --cluster del-node 192.168.111.147:6387 e4781f644d4a4e4d4b4d107157b9ba8144631451 七、检查集群情况第三次 1&gt;redis-cli --cluster check 192.168.111.147:6381 二、DockerFile解析2.1 DockerFile是什么DockerFile是用来构建Docker镜像的文本文件，是有一条条构建镜像所需的指令和参数构成的脚本。 官网：https://docs.docker.com/engine/reference/builder/ 构建三步骤 1231、编写DockerFile文件2、docker build命令构建镜像3、docker run 依镜像运行容器实例 2.2 DockerFile构建过程解析DockerFile内容基础知识 12341. 每条保留字指令都必须为大写字母且后面跟随至少一个参数2. 指令按照从上到下，顺序执行3. #表示注释4. 每条指令都会创建一个新的镜像层并对镜像进行提交。 Docker执行DockerFile的大致流程 123451. docker从技术镜像运行一个容器2. 执行一条指令比鞥对容器做出修改3. 执行类似docker commit 的操作提交一个新的镜像层4. docker 在基于刚提交的镜像运行一个新容器5. 执行dockerfile中的下一条指令直到所有执行执行完成。 2.3 小总结 从应用软件的角度来看，Dockerfile、Docker镜像与Docker容器分别代表软件的三个不同阶段， Dockerfile是软件的原材料 Docker镜像是软件的交付品 Docker容器则可以认为是软件镜像的运行态，也即依照镜像运行的容器实例 Dockerfile面向开发，Docker镜像成为交付标准，Docker容器则涉及部署与运维，三者缺一不可，合力充当Docker体系的基石。 Dockerfile，需要定义一个Dockerfile，Dockerfile定义了进程需要的一切东西。Dockerfile涉及的内容包括执行代码或者是文件、环境变量、依赖包、运行时环境、动态链接库、操作系统的发行版、服务进程和内核进程(当应用进程需要和系统服务和内核进程打交道，这时需要考虑如何设计namespace的权限控制)等等; Docker镜像，在用Dockerfile定义一个文件之后，docker build时会产生一个Docker镜像，当运行 Docker镜像时会真正开始提供服务; Docker容器，容器是直接提供服务的。 2.4 DockerFile常用保留字指令 参考tomcat8的dockerfile入门 https://github.com/docker-library/tomcat From 1基础镜像，当前新镜像是基于哪个镜像的，指定一个已经存在的镜像作为模板，第一条必须是from MANINTAINER 镜像维护者的姓名和邮箱地址 Run 容器构建时需要运行的命令 两种格式： shell格式 123&lt;命令行命令&gt;等同于，在终端操作的shell命令RUN yum -y install vim exec格式 RUN是在docker build时运行 EXPOSE 当前容器对外暴露出的端口 WORKDIR 指定在创建容器后。终端默认登录的进来工作目录，一个落脚点。 USER 指定该镜像以什么样的用户去执行，如果都不指定，默认是root ENV 用来在构建镜像过程中设置环境变量 12345ENV MY_PATH /usr/mytest 这个环境变量可以在后续的任何RUN指令中使用，这就如同在命令前面指定了环境变量前缀一样； 也可以在其它指令中直接使用这些环境变量， 比如：WORKDIR $MY_PATH ADD 将宿主机目录下的文件拷贝进镜像且会自动处理URL和解压tar压缩包 COPY 类似ADD，拷贝文件和目录到镜像中。将从构建上下文目录中&lt;源路径&gt;的文件/目录复制到新的一层镜像内的&lt;目标路径&gt;位置 1234567COPY src destCOPY[&quot;src&quot;,&quot;dest&quot;]&lt;src源路径&gt;：源文件或源目录&lt;dest目标路径&gt;: 容器内的指定路径，该路径不用事先建好，路径不存在的话，会自动创建。 VOLUME 容器数据卷，用于数据保存和持久化的工作 CMD 指定容器启动后的要干的事情。 【注意】 1Dockerfile 中可以由多个CMD指令，但是只有最后一个生效，CMD会被docker run 之后的参数替换。 参考官网Tomcat的dockerfile演示讲解 官网最后一行命令 12EXPOSE 8080CMD [&quot;catalina.sh&quot;,&quot;run&quot;] 我们演示自己的覆盖操作 1docker run -it -p 8080:8080 容器ID /bin/bash 他和前面RUN命令的区别 123CMD 是在 docker run 时运行。RUN 是在docker build 时运行 ENTRYPOINT 也是用来指定一个容器启动时要运行的命令 类似于CMD指令，但是ENTRYPOINT不会被docker run 后面的命令覆盖，而且这些命令行参数会被当作参数送给ENTRYPOINT指令指定的程序。 命令格式和案例说明 1234命令格式：ENTRYPOINT[&quot;&lt;executeable&gt;&quot;,&quot;&lt;param1&gt;&quot;,&quot;&lt;param2&gt;&quot;,...] ENTRYPOINT 可以和CMD一起用，一般是 变参 才会使用 CMD ，这里的CMD等于是在给 ENTRYPOINT 传参。当制定了 ENTRYPOINT 后，CMD的含义就发生了变化，不再是直接运行其命令而是将 CMD 的内容作为参数传递给 ENTRYPOINT 指定，他两个组合会变成&lt;ENTRYPOINT&gt; &quot;&lt;CMD&gt;&quot;案例如下：假设已通过 Dockerfile 构建了 nginx:test 镜像 是否传参 按照dockerfile编写执行 传参运行 Docker命令 docker run nginx:test docker run nginx:test -c /etc/nginx/ new.conf 衍生出的实际命令 nginx -c /etc/nginx/nginx.conf nginx -c /etc/nginx/ new.conf 优点：在执行docker run 的时候可以指定 ENTRYPOINT 运行所需的参数。 注意：如果Dockerfile 中如果存在多个 ENTRYPOINT 指令，进最后一个生效。 小总结 2.5 案例2.5.1 自定义镜像mycentosjava8要求 12345Centos7镜像具备 vim + ifconfig + jdk8JDK下载镜像地址官网：https://www.oracle.com/java/technologies/downloads/#java8 https://mirrors.yangxingzhen.com/jdk/ 编写 1234567891011121314151617181920212223242526272829准备编写Dockerfile文件 【注意】大写字母D FROM centosMAINTAINER zzyy&lt;zzyybs@126.com&gt; ENV MYPATH /usr/local WORKDIR $MYPATH #安装vim编辑器 RUN yum -y install vim #安装ifconfig命令查看网络IP RUN yum -y install net-tools #安装java8及lib库 RUN yum -y install glibc.i686 RUN mkdir /usr/local/java #ADD 是相对路径jar,把jdk-8u171-linux-x64.tar.gz添加到容器中,安装包必须要和Dockerfile文件在同一位置 ADD jdk-8u171-linux-x64.tar.gz /usr/local/java/ #配置java环境变量 ENV JAVA_HOME /usr/local/java/jdk1.8.0_171 ENV JRE_HOME $JAVA_HOME/jre ENV CLASSPATH $JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATH ENV PATH $JAVA_HOME/bin:$PATH EXPOSE 80 CMD echo $MYPATH CMD echo &quot;success--------------ok&quot; CMD /bin/bash 构建 123456docker build -t 新镜像名字: TAG例如：docker build -t centosjava8:1.5 .【注意】上面TAG 后面有个空格，有个点 运行 123docker run -it 新镜像名字:TAGdocker run -it centosjava8:1.5 /bin/bash 再体会下UnionFS（联合文件系统） 123UnionFS（联合文件系统）：Union文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持 对文件系统的修改作为一次提交来一层层的叠加， 同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem)。Union 文件系统是 Docker 镜像的基础。 镜像可以通过分层来进行继承 ，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录 2.5.2 虚悬镜像是什么 123仓库名，标签都是 &lt;none&gt; 的镜像，俗称dangling imageDockerfile 写一个 查看 12docker image ls -f dangling=true命令结果如下图： 删除 123docker image prune 虚悬镜像已经市区存在价值，可以删除 2.5.3 家庭作业自定义myubuntu123456789101112131415161718192021222324252627# 编写准备编写DockerFile文件vim Dockerfile----------------------FROM ubuntuMAINTAINER zzyy&lt;zzyybs@126.com&gt; ENV MYPATH /usr/local WORKDIR $MYPATH RUN apt-get update RUN apt-get install net-tools #RUN apt-get install -y iproute2 #RUN apt-get install -y inetutils-ping EXPOSE 80 CMD echo $MYPATH CMD echo &quot;install inconfig cmd into ubuntu success--------------ok&quot; CMD /bin/bash ------------------------# 构建docker build -t 新镜像名字:TAG#运行docker run -it 新镜像名字:TAG 三、Docker微服务实战3.1 通过IDEA新建一个普通微服务模块建Module 1docker_boot 修改POM 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&lt;?xml version =&quot;1.0&quot; encoding =&quot;UTF-8&quot;?&gt; &lt;project xmlns =&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns: xsi =&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi :schemaLocation =&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt; 4.0.0 &lt;/ modelVersion&gt; &lt;parent&gt; &lt;groupId&gt; org.springframework.boot &lt;/ groupId&gt; &lt;artifactId&gt; spring-boot-starter-parent &lt;/ artifactId&gt; &lt;version&gt; 2.5.6 &lt;/ version&gt; &lt;relativePath /&gt; &lt;/ parent&gt; &lt;groupId&gt; com.atguigu.docker &lt;/ groupId&gt; &lt;artifactId&gt; docker_boot &lt;/ artifactId&gt; &lt;version&gt; 0.0.1-SNAPSHOT &lt;/ version&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt; UTF-8 &lt;/ project.build.sourceEncoding&gt; &lt;maven.compiler.source&gt; 1.8 &lt;/ maven.compiler.source&gt; &lt;maven.compiler.target&gt; 1.8 &lt;/ maven.compiler.target&gt; &lt;junit.version&gt; 4.12 &lt;/ junit.version&gt; &lt;log4j.version&gt; 1.2.17 &lt;/ log4j.version&gt; &lt;lombok.version&gt; 1.16.18 &lt;/ lombok.version&gt; &lt;mysql.version&gt; 5.1.47 &lt;/ mysql.version&gt; &lt;druid.version&gt; 1.1.16 &lt;/ druid.version&gt; &lt;mapper.version&gt; 4.1.5 &lt;/ mapper.version&gt; &lt;mybatis.spring.boot.version&gt; 1.3.0 &lt;/ mybatis.spring.boot.version&gt; &lt;/ properties&gt; &lt;dependencies&gt; &lt;!--SpringBoot 通用依赖模块 --&gt; &lt;dependency&gt; &lt;groupId&gt; org.springframework.boot &lt;/ groupId&gt; &lt;artifactId&gt; spring-boot-starter-web &lt;/ artifactId&gt; &lt;/ dependency&gt; &lt;dependency&gt; &lt;groupId&gt; org.springframework.boot &lt;/ groupId&gt; &lt;artifactId&gt; spring-boot-starter-actuator &lt;/ artifactId&gt; &lt;/ dependency&gt; &lt;!--test--&gt; &lt;dependency&gt; &lt;groupId&gt; org.springframework.boot &lt;/ groupId&gt; &lt;artifactId&gt; spring-boot-starter-test &lt;/ artifactId&gt; &lt;scope&gt; test &lt;/ scope&gt; &lt;/ dependency&gt; &lt;/ dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt; org.springframework.boot &lt;/ groupId&gt; &lt;artifactId&gt; spring-boot-maven-plugin &lt;/ artifactId&gt; &lt;/ plugin&gt; &lt;plugin&gt; &lt;groupId&gt; org.apache.maven.plugins &lt;/ groupId&gt; &lt;artifactId&gt; maven-resources-plugin &lt;/ artifactId&gt; &lt;version&gt; 3.1.0 &lt;/ version&gt; &lt;/ plugin&gt; &lt;/ plugins&gt; &lt;/ build&gt; &lt;/ project&gt; 写YML 1server.port=6001 主启动 123456789package com.atguigu.docker;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure. SpringBootApplication ;@SpringBootApplicationpublic class DockerBootApplication &#123; public static void main(String[] args) &#123; SpringApplication. run (DockerBootApplication. class , args); &#125; &#125; 业务类 12345678910111213141516171819202122232425262728293031package com.atguigu.docker.controller;import org.springframework.beans.factory.annotation. Value ;import org.springframework.web.bind.annotation. RequestMapping ;import org.springframework.web.bind.annotation.RequestMethod;import org.springframework.web.bind.annotation. RestController ;import java.util.UUID;/ *@auther zzyy *@create 2021-10-25 17:43*/@RestControllerpublic class OrderController&#123; @Value ( &quot;$&#123;server.port&#125;&quot; ) private String port ; @RequestMapping ( &quot;/order/docker&quot; ) public String helloDocker() &#123; return &quot;hello docker&quot; + &quot; \\t &quot; + port + &quot; \\t &quot; + UUID. *randomUUID* ().toString(); &#125; @RequestMapping (value = &quot;/order/index&quot; ,method = RequestMethod. *GET\\* ) public String index() &#123; return &quot; 服务端口号 : &quot; + &quot; \\t &quot; + port + &quot; \\t &quot; +UUID. *randomUUID* ().toString(); &#125;&#125; 3.2 通过dockerfile 发布微服务部署到docker容器3.2.1 IDEA工具里面搞定微服务jar包 3.2.2 编写Dockerfile Dockerfile内容 12345678910111213# 基础镜像使用java FROM java:8 # 作者 MAINTAINER zzyy # VOLUME 指定临时文件目录为/tmp，在主机/var/lib/docker目录下创建了一个临时文件并链接到容器的/tmp VOLUME /tmp # 将jar包添加到容器中并更名为zzyy_docker.jar ADD docker_boot-0.0.1-SNAPSHOT.jar /zzyy_docker.jar # 运行jar包 RUN bash -c &#x27;touch /zzyy_docker.jar&#x27; ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/zzyy_docker.jar&quot;] #暴露6001端口作为微服务 EXPOSE 6001 将微服务jar包和Dockerfile文件上传到同一个目录下/mydocker 12docker build -t zzyy_docker:1.6 . 构建镜像 12345docker build -t zzyy_docker:1.6 .打包成镜像文件# 命令docker build -t zzyy_docker:1.6 . 运行容器 1234# 运行命令docker run -d -p 6001:6001 zzyy_docker:1.6# 查看镜像运行命令docker images 访问测试 四、Docker网络4.1 Docker 网络是什么4.1.1 docker不启动，默认网络情况123ens 33lovirbr0 1234在CentOS7的安装过程中如果有 选择相关虚拟化的的服务安装系统后 ，启动网卡时会发现有一个以网桥连接的私网地址的virbr0网卡(virbr0网卡：它还有一个固定的默认IP地址192.168.122.1)，是做虚拟机网桥的使用的，其作用是为连接其上的虚机网卡提供 NAT访问外网的功能。 我们之前学习Linux安装，勾选安装系统的时候附带了libvirt服务才会生成的一个东西，如果不需要可以直接将libvirtd服务卸载， yum remove libvirt-libs.x86_64 4.1.2 docker启动后，网络情况查看docker网络模式命令 4.2 常用基本命令4.2.1 All 命令 4.2.2 查看网络1docker network ls 4.2.3 查看网络源数据1docker network inspect XXX网络名字 4.2.4 删除网络1docker network rm XXX网络名字 4.2.5 案例 4.3 能干嘛12容器间的互联和通信以及端口映射容器IP变动时候可以通过服务名直接网络通信而不受到影响 4.4 网络模式4.4.1 总体介绍1234567bridge模式：使用--network bridge指定，默认使用docker()host模式：使用 --network host指定none模式：使用 --network none指定container模式：使用 --network container:Name或者容器ID指定 4.4.2 容器实例内默认网络IP生产规则 1 先启动两个ubuntu容器实例 2 docker inspect 容器ID or 容器名字 3 关闭u2实例，新建u3，查看ip变化 4.4.3 案例说明bridge 123456Docker 服务默认会创建一个 docker0 网桥（其上有一个 docker0 内部接口），该桥接网络的名称为docker0，它在 内核层 连通了其他的物理或虚拟网卡，这就将所有容器和本地主机都放到 同一个物理网络 。Docker 默认指定了 docker0 接口 的 IP 地址和子网掩码， 让主机和容器之间可以通过网桥相互通信。 # 查看 bridge 网络的详细信息，并通过 grep 获取名称项 docker network inspect bridge | grep name ifconfig 案例 1234567891 Docker使用Linux桥接，在宿主机虚拟一个Docker容器网桥(docker0)，Docker启动一个容器时会根据Docker网桥的网段分配给容器一个IP地址，称为Container-IP，同时Docker网桥是每个容器的默认网关。因为在同一宿主机内的容器都接入同一个网桥，这样容器之间就能够通过容器的Container-IP直接通信。 2 docker run 的时候，没有指定network的话默认使用的网桥模式就是bridge，使用的就是docker0 。在宿主机ifconfig,就可以看到docker0和自己create的network(后面讲)eth0，eth1，eth2……代表网卡一，网卡二，网卡三…… ，lo代表127.0.0.1，即localhost ，inet addr用来表示网卡的IP地址 3 网桥docker0创建一对对等虚拟设备接口一个叫veth，另一个叫eth0，成对匹配。 3.1 整个宿主机的网桥模式都是docker0，类似一个交换机有一堆接口，每个接口叫veth，在本地主机和容器内分别创建一个虚拟接口，并让他们彼此联通（这样一对接口叫veth pair）； 3.2 每个容器实例内部也有一块网卡，每个接口叫eth0； 3.3 docker0上面的每个veth匹配某个容器实例内部的eth0，两两配对，一一匹配。 通过上述，将宿主机上的所有容器都连接到这个内部网络上，两个容器在同一个网络下,会从这个网关下各自拿到分配的ip，此时两个容器的网络是互通的。 【代码】 123docker run -d -p 8081:8080 --name tomcat81 billygoo/tomcat8-jdk8docker run -d -p 8082:8080 --name tomcat82 billygoo/tomcat8-jdk8 两两匹配验证 Host 一、是什么 直接使用宿主机的IP地址与外界进行通信，不再需要额外进行NAT转换。 二、案例 说明 容器将 不会获得 一个独立的Network Namespace， 而是和宿主机共用一个Network Namespace。 容器将不会虚拟出自己的网卡而是使用宿主机的IP和端口。 代码 12345警告： docker run -d -p 8083:8080 --network host --name tomcat83 billygoo/tomcat8-jdk8正确： docker run -d --network host --name tomcat83 billygoo/tomcat8-jdk8 无之前的配对显示了，看容器实例内部 没有设置-p的端口映射了，如何访问启动的tomcat83？ 1234http://宿主机IP:8080/ 在CentOS里面用默认的火狐浏览器访问容器内的tomcat83看到访问成功，因为此时容器的IP借用主机的， 所以容器共享宿主机网络IP，这样的好处是外部主机与容器可以直接通信。 none 一、是什么 禁用网络功能，只有lo标识（就是127.0.0.1表示本地回环） 二、案例 docker run -d -p8084:8080 –network none –name tomcat84 billygoo/tomcat8-jdk8 container 一、是什么 container⽹络模式 新建的容器和已经存在的一个容器共享一个网络ip配置而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的IP，而是和一个指定的容器共享IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。 二、❎案例 123456789&gt;docker run -d -p 8085:8080 --name tomcat85 billygoo/tomcat8-jdk8&gt;docker run -d -p 8086:8080 --network container:tomcat85 --name tomcat86 billygoo/tomcat8-jdk8&gt;运行结果&gt;docker：Error response from daemon: conflicting optisons: port ...........&gt;# 相当于tomcat86和tomcat85公用同一个ip同一个端口，导致端口冲突 三、✅案例2 1234567&gt;Alpine操作系统是一个面向安全的轻型 Linux发行版&gt;docker run -it --name alpine1 alpine /bin/sh&gt;docker run -it --network container:alpine1 --name alpine2 alpine /bin/sh 运行结果，验证共用搭桥 假如此时关闭alpine1，再看看alpine2 自定义网络 一、过时的link 二、是什么 三、案例 【before】 123456&gt;案例：&gt;docker run -d -p 8081:8080 --name tomcat81 billygoo/tomcat8-jdk8&gt;docker run -d -p 8082:8080 --name tomcat82 billygoo/tomcat8-jdk8&gt;上述成功启动并用docker exec进入各自容器实例内部 1234&gt;问题：&gt;1. 按照IP地址ping是OK的&gt;2. 按照服务名ping结果???ping： tocmat82：Name or service not known 【after】 1234&gt;案例&gt;自定义桥接网络,自定义网络默认使用的是桥接网络bridge&gt;新建自定义网络 新建容器加入上一步新建的自定义网络 1234&gt;docker run -d -p 8081:8080 --network zzyy_network --name tomcat81 billygoo/tomcat8-jdk8&gt;docker run -d -p 8082:8080 --network zzyy_network --name tomcat82 billygoo/tomcat8-jdk8 互相ping测试 问题结论 123&gt;1、自定义网络本身就维护好了主机名和ip的对应关系（ip和域名都能通）&gt;2、自定义网络本身就维护好了主机名和ip的对应关系（ip和域名都能通）&gt;3、自定义网络本身就维护好了主机名和ip的对应关系（ip和域名都能通） 4.5 Docker平台架构图解1234567891011从其架构和运行流程来看，Docker 是一个 C/S 模式的架构，后端是一个松耦合架构，众多模块各司其职。 Docker 运行的基本流程为： 1 用户是使用 Docker Client 与 Docker Daemon 建立通信，并发送请求给后者。 2 Docker Daemon 作为 Docker 架构中的主体部分，首先提供 Docker Server 的功能使其可以接受 Docker Client 的请求。 3 Docker Engine 执行 Docker 内部的一系列工作，每一项工作都是以一个 Job 的形式的存在。 4 Job 的运行过程中，当需要容器镜像时，则从 Docker Registry 中下载镜像，并通过镜像管理驱动 Graph driver将下载镜像以Graph的形式存储。 5 当需要为 Docker 创建网络环境时，通过网络管理驱动 Network driver 创建并配置 Docker 容器网络环境。 6 当需要限制 Docker 容器运行资源或执行用户指令等操作时，则通过 Execdriver 来完成。 7 Libcontainer是一项独立的容器管理包，Network driver以及Exec driver都是通过Libcontainer来实现具体对容器进行的操作。 五、Docker-compose容器编排5.1 Docker-compose是什么1Docker-Compose是Docker官方的开源项目，负责实现对Docker容器集群的快速编排。 5.2 能干什么123456789 docker建议我们每一个容器中只运行一个服务,因为docker容器本身占用资源极少,所以最好是将每个服务单独的分割开来但是这样我们又面临了一个问题？ 如果我需要同时部署好多个服务,难道要每个服务单独写Dockerfile然后在构建镜像,构建容器,这样累都累死了,所以docker官方给我们提供了docker-compose多服务部署的工具 例如要实现一个Web微服务项目，除了Web服务容器本身，往往还需要再加上后端的数据库mysql服务容器，redis服务器，注册中心eureka，甚至还包括负载均衡容器等等。。。。。。 Compose允许用户通过一个单独的 docker-compose.yml模板文件 （YAML 格式）来定义 一组相关联的应用容器为一个项目（project）。 可以很容易地用一个配置文件定义一个多容器的应用，然后使用一条指令安装这个应用的所有依赖，完成构建。Docker-Compose 解决了容器与容器之间如何管理编排的问题。 5.3 去哪里5.3.1 官网：https://docs.docker.com/compose/compose-file/compose-file-v3/ 5.3.2 官网下载https://docs.docker.com/compose/install/ 5.3.3 安装步骤123curl -L &quot;https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)&quot; -o /usr/local/bin/docker-compose chmod +x /usr/local/bin/docker-compose docker-compose --version 5.3.4 卸载步骤1sudo rm /usr/local/bin/docker-compose 5.4 Compose 核心概念5.4.1 一文件1docker-compose.yml 5.4.2 两要素12345服务(service):一个个应用容器实例，比如订单微服务、库存微服务、mysql容器、nginx容器或者redis容器。工程(project):由一组关联的应用容器组成的一个完整业务单元，在 docker-compose.yml 文件中定义。 5.5 Compose 使用的三个步骤12341. 编写Dockerfile定义各个微服务应用并构建出对应的镜像文件2. 使用 docker-compose.yml 定义一个完整业务单元，安排好整体应用中的各个容器服务。3. 最后，执行docker-compose up命令 来启动并运行整个应用程序，完成一键部署上线 5.6 Compose常用命令123456789101112131415Compose 常用命令 docker-compose -h # 查看帮助 docker-compose up # 启动所有 docker-compose服务 docker-compose up -d # 启动所有 docker-compose服务 并后台运行 docker-compose down # 停止并删除容器、网络、卷、镜像。 docker-compose exec yml里面的服务id # 进入容器实例内部 docker-compose exec docker-compose.yml文件中写的服务id /bin/bash docker-compose ps # 展示当前docker-compose编排过的运行的所有容器 docker-compose top # 展示当前docker-compose编排过的容器进程 docker-compose logs yml里面的服务id # 查看容器输出日志 docker-compose config # 检查配置 docker-compose config -q # 检查配置，有问题才有输出 docker-compose restart # 重启服务 docker-compose start # 启动服务 docker-compose stop # 停止服务 5.5 Componse 编排微服务5.5.1 改造升级微服务工程docker_boot以前的基础版 SQL建表建库 12345678910CREATE TABLE `t_user` ( `id` int(10) unsigned NOT NULL AUTO_INCREMENT, `username` varchar(50) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;用户名&#x27;, `password` varchar(50) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;密码&#x27;, `sex` tinyint(4) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;性别 0=女 1=男 &#x27;, `deleted` tinyint(4) unsigned NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;删除标志，默认0不删除，1删除&#x27;, `update_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;更新时间&#x27;, `create_time` timestamp NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, PRIMARY KEY (`id`) ) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT=&#x27;用户表&#x27; 改POM 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179&lt;? xml version =&quot;1.0&quot; encoding =&quot;UTF-8&quot; ?&gt;&lt; project xmlns =&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns: xsi =&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi :schemaLocation =&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot; &gt; &lt; modelVersion &gt; 4.0.0 &lt;/ modelVersion &gt; &lt; parent &gt; &lt; groupId &gt; org.springframework.boot &lt;/ groupId &gt; &lt; artifactId &gt; spring-boot-starter-parent &lt;/ artifactId &gt; &lt; version &gt; 2.5.6 &lt;/ version &gt; &lt;!--&lt;version&gt;2.3.10.RELEASE&lt;/version&gt;--&gt; &lt; relativePath /&gt; &lt;!-- lookup parent from repository --&gt; &lt;/ parent &gt; &lt; groupId &gt; com.atguigu.docker &lt;/ groupId &gt; &lt; artifactId &gt; docker_boot &lt;/ artifactId &gt; &lt; version &gt; 0.0.1-SNAPSHOT &lt;/ version &gt; &lt; properties &gt; &lt; project.build.sourceEncoding &gt; UTF-8 &lt;/ project.build.sourceEncoding &gt; &lt; maven.compiler.source &gt; 1.8 &lt;/ maven.compiler.source &gt; &lt; maven.compiler.target &gt; 1.8 &lt;/ maven.compiler.target &gt; &lt; junit.version &gt; 4.12 &lt;/ junit.version &gt; &lt; log4j.version &gt; 1.2.17 &lt;/ log4j.version &gt; &lt; lombok.version &gt; 1.16.18 &lt;/ lombok.version &gt; &lt; mysql.version &gt; 5.1.47 &lt;/ mysql.version &gt; &lt; druid.version &gt; 1.1.16 &lt;/ druid.version &gt; &lt; mapper.version &gt; 4.1.5 &lt;/ mapper.version &gt; &lt; mybatis.spring.boot.version &gt; 1.3.0 &lt;/ mybatis.spring.boot.version &gt; &lt;/ properties &gt; &lt; dependencies &gt; &lt;!--guava Google 开源的 Guava 中自带的布隆过滤器 --&gt; &lt; dependency &gt; &lt; groupId &gt; com.google.guava &lt;/ groupId &gt; &lt; artifactId &gt; guava &lt;/ artifactId &gt; &lt; version &gt; 23.0 &lt;/ version &gt; &lt;/ dependency &gt; &lt;!-- redisson --&gt; &lt; dependency &gt; &lt; groupId &gt; org.redisson &lt;/ groupId &gt; &lt; artifactId &gt; redisson &lt;/ artifactId &gt; &lt; version &gt; 3.13.4 &lt;/ version &gt; &lt;/ dependency &gt; &lt;!--SpringBoot 通用依赖模块 --&gt; &lt; dependency &gt; &lt; groupId &gt; org.springframework.boot &lt;/ groupId &gt; &lt; artifactId &gt; spring-boot-starter-web &lt;/ artifactId &gt; &lt;/ dependency &gt; &lt; dependency &gt; &lt; groupId &gt; org.springframework.boot &lt;/ groupId &gt; &lt; artifactId &gt; spring-boot-starter-actuator &lt;/ artifactId &gt; &lt;/ dependency &gt; &lt;!--swagger2--&gt; &lt; dependency &gt; &lt; groupId &gt; io.springfox &lt;/ groupId &gt; &lt; artifactId &gt; springfox-swagger2 &lt;/ artifactId &gt; &lt; version &gt; 2.9.2 &lt;/ version &gt; &lt;/ dependency &gt; &lt; dependency &gt; &lt; groupId &gt; io.springfox &lt;/ groupId &gt; &lt; artifactId &gt; springfox-swagger-ui &lt;/ artifactId &gt; &lt; version &gt; 2.9.2 &lt;/ version &gt; &lt;/ dependency &gt; &lt;!--SpringBoot 与 Redis 整合依赖 --&gt; &lt; dependency &gt; &lt; groupId &gt; org.springframework.boot &lt;/ groupId &gt; &lt; artifactId &gt; spring-boot-starter-data-redis &lt;/ artifactId &gt; &lt;/ dependency &gt; &lt;!--springCache--&gt; &lt; dependency &gt; &lt; groupId &gt; org.springframework.boot &lt;/ groupId &gt; &lt; artifactId &gt; spring-boot-starter-cache &lt;/ artifactId &gt; &lt;/ dependency &gt; &lt;!--springCache 连接池依赖包 --&gt; &lt; dependency &gt; &lt; groupId &gt; org.apache.commons &lt;/ groupId &gt; &lt; artifactId &gt; commons-pool2 &lt;/ artifactId &gt; &lt;/ dependency &gt; &lt;!-- jedis --&gt; &lt; dependency &gt; &lt; groupId &gt; redis.clients &lt;/ groupId &gt; &lt; artifactId &gt; jedis &lt;/ artifactId &gt; &lt; version &gt; 3.1.0 &lt;/ version &gt; &lt;/ dependency &gt; &lt;!--Mysql 数据库驱动 --&gt; &lt; dependency &gt; &lt; groupId &gt; mysql &lt;/ groupId &gt; &lt; artifactId &gt; mysql-connector-java &lt;/ artifactId &gt; &lt; version &gt; 5.1.47 &lt;/ version &gt; &lt;/ dependency &gt; &lt;!--SpringBoot 集成 druid 连接池 --&gt; &lt; dependency &gt; &lt; groupId &gt; com.alibaba &lt;/ groupId &gt; &lt; artifactId &gt; druid-spring-boot-starter &lt;/ artifactId &gt; &lt; version &gt; 1.1.10 &lt;/ version &gt; &lt;/ dependency &gt; &lt; dependency &gt; &lt; groupId &gt; com.alibaba &lt;/ groupId &gt; &lt; artifactId &gt; druid &lt;/ artifactId &gt; &lt; version &gt; $&#123;druid.version&#125; &lt;/ version &gt; &lt;/ dependency &gt; &lt;!--mybatis 和 springboot 整合 --&gt; &lt; dependency &gt; &lt; groupId &gt; org.mybatis.spring.boot &lt;/ groupId &gt; &lt; artifactId &gt; mybatis-spring-boot-starter &lt;/ artifactId &gt; &lt; version &gt; $&#123;mybatis.spring.boot.version&#125; &lt;/ version &gt; &lt;/ dependency &gt; &lt;!-- 添加 springboot 对 amqp 的支持 --&gt; &lt; dependency &gt; &lt; groupId &gt; org.springframework.boot &lt;/ groupId &gt; &lt; artifactId &gt; spring-boot-starter-amqp &lt;/ artifactId &gt; &lt;/ dependency &gt; &lt; dependency &gt; &lt; groupId &gt; commons-codec &lt;/ groupId &gt; &lt; artifactId &gt; commons-codec &lt;/ artifactId &gt; &lt; version &gt; 1.10 &lt;/ version &gt; &lt;/ dependency &gt; &lt;!-- 通用基础配置 junit/devtools/test/log4j/lombok/hutool--&gt; &lt;!--hutool--&gt; &lt; dependency &gt; &lt; groupId &gt; cn.hutool &lt;/ groupId &gt; &lt; artifactId &gt; hutool-all &lt;/ artifactId &gt; &lt; version &gt; 5.2.3 &lt;/ version &gt; &lt;/ dependency &gt; &lt; dependency &gt; &lt; groupId &gt; junit &lt;/ groupId &gt; &lt; artifactId &gt; junit &lt;/ artifactId &gt; &lt; version &gt; $&#123;junit.version&#125; &lt;/ version &gt; &lt;/ dependency &gt; &lt; dependency &gt; &lt; groupId &gt; org.springframework.boot &lt;/ groupId &gt; &lt; artifactId &gt; spring-boot-devtools &lt;/ artifactId &gt; &lt; scope &gt; runtime &lt;/ scope &gt; &lt; optional &gt; true &lt;/ optional &gt; &lt;/ dependency &gt; &lt; dependency &gt; &lt; groupId &gt; org.springframework.boot &lt;/ groupId &gt; &lt; artifactId &gt; spring-boot-starter-test &lt;/ artifactId &gt; &lt; scope &gt; test &lt;/ scope &gt; &lt;/ dependency &gt; &lt; dependency &gt; &lt; groupId &gt; log4j &lt;/ groupId &gt; &lt; artifactId &gt; log4j &lt;/ artifactId &gt; &lt; version &gt; $&#123;log4j.version&#125; &lt;/ version &gt; &lt;/ dependency &gt; &lt; dependency &gt; &lt; groupId &gt; org.projectlombok &lt;/ groupId &gt; &lt; artifactId &gt; lombok &lt;/ artifactId &gt; &lt; version &gt; $&#123;lombok.version&#125; &lt;/ version &gt; &lt; optional &gt; true &lt;/ optional &gt; &lt;/ dependency &gt; &lt;!--persistence--&gt; &lt; dependency &gt; &lt; groupId &gt; javax.persistence &lt;/ groupId &gt; &lt; artifactId &gt; persistence-api &lt;/ artifactId &gt; &lt; version &gt; 1.0.2 &lt;/ version &gt; &lt;/ dependency &gt; &lt;!-- 通用 Mapper--&gt; &lt; dependency &gt; &lt; groupId &gt; tk.mybatis &lt;/ groupId &gt; &lt; artifactId &gt; mapper &lt;/ artifactId &gt; &lt; version &gt; $&#123;mapper.version&#125; &lt;/ version &gt; &lt;/ dependency &gt; &lt;/ dependencies &gt; &lt; build &gt; &lt; plugins &gt; &lt; plugin &gt; &lt; groupId &gt; org.springframework.boot &lt;/ groupId &gt; &lt; artifactId &gt; spring-boot-maven-plugin &lt;/ artifactId &gt; &lt;/ plugin &gt; &lt; plugin &gt; &lt; groupId &gt; org.apache.maven.plugins &lt;/ groupId &gt; &lt; artifactId &gt; maven-resources-plugin &lt;/ artifactId &gt; &lt; version &gt; 3.1.0 &lt;/ version &gt; &lt;/ plugin &gt; &lt;/ plugins &gt; &lt;/ build &gt; &lt;/ project &gt; 写YML 1234567891011121314151617181920212223242526272829server.port = 6001========================alibaba.druid* 相关配置 *=====================spring.datasource.type = com.alibaba.druid.pool.DruidDataSourcespring.datasource.driver-class-name = com.mysql.jdbc.Driverspring.datasource.url= jdbc:mysql://192.168.111.169 :3306/db2021?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=falsespring.datasource.username = rootspring.datasource.password = 123456spring.datasource.druid.test-while-idle = false========================redis* 相关配置 *=====================spring.redis.database = 0spring.redis.host = 192.168.111.169spring.redis.port = 6379spring.redis.password =spring.redis.lettuce.pool.max-active = 8spring.redis.lettuce.pool.max-wait = -1msspring.redis.lettuce.pool.max-idle = 8spring.redis.lettuce.pool.min-idle = 0========================mybatis 相关配置 *===================*=mybatis.mapper-locations = classpath:mapper/\\*.xmlmybatis.type-aliases-package = com.atguigu.docker.entities========================swagger=====================spring.swagger2.enabled = true 主启动 业务类 一、config配置类 RedisConfig 123456789101112131415161718192021222324252627282930313233343536373839&gt;package com.atguigu.docker.config;&gt;import lombok.extern.slf4j. Slf4j ;&gt;import org.springframework.context.annotation.Bean ;&gt;import org.springframework.context.annotation.Configuration ;&gt;import org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory;&gt;import org.springframework.data.redis.core.RedisTemplate;&gt;import org.springframework.data.redis.serializer.GenericJackson2JsonRedisSerializer;&gt;import org.springframework.data.redis.serializer.StringRedisSerializer;&gt;import java.io.Serializable;/*** @auther zzyy* @create 2021-10-27 17:19*/ @Configuration@Slf4jpublic class RedisConfig&#123; /*** @param lettuceConnectionFactory * @return * redis 序列化的工具配置类，下面这个请一定开启配置 * 127.0.0.1:6379&gt; keys * * 1) &quot;ord:102&quot; 序列化过 * 2) &quot;\\xac\\xed\\x00\\x05t\\x00\\aord:102&quot; 野生，没有序列化过 */ &gt;@Bean &gt;public RedisTemplate&lt;String,Serializable&gt; redisTemplate(LettuceConnectionFactory lettuceConnectionFactory) &#123; RedisTemplate&lt;String,Serializable&gt; redisTemplate = new RedisTemplate&lt;&gt;();redisTemplate.setConnectionFactory(lettuceConnectionFactory); // 设置 key 序列化方式 string redisTemplate.setKeySerializer( new StringRedisSerializer()); // 设置 value 的序列化方式 json redisTemplate.setValueSerializer(new GenericJackson2JsonRedisSerializer());redisTemplate.setHashKeySerializer(new StringRedisSerializer());redisTemplate.setHashValueSerializer( new GenericJackson2JsonRedisSerializer());redisTemplate.afterPropertiesSet();return redisTemplate; &#125;&#125; SwaggerConfig 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647package com.atguigu.docker.config; import org.springframework.beans.factory.annotation. Value ; import org.springframework.context.annotation. Bean ; import org.springframework.context.annotation. Configuration ; import springfox.documentation.builders.ApiInfoBuilder; import springfox.documentation.builders.PathSelectors; import springfox.documentation.builders.RequestHandlerSelectors; import springfox.documentation.service.ApiInfo; import springfox.documentation.spi.DocumentationType; import springfox.documentation.spring.web.plugins.Docket; import springfox.documentation.swagger2.annotations. EnableSwagger2 ; import java.text.SimpleDateFormat; import java.util.Date; /** *@auther zzyy *@create 2021-05-01 16:18 */ @Configuration @EnableSwagger2 public class SwaggerConfig &#123; @Value ( &quot;$&#123;spring.swagger2.enabled&#125;&quot; ) private Boolean enabled ; @Bean public Docket createRestApi() &#123; return new Docket(DocumentationType. *SWAGGER_2\\* ) .apiInfo(apiInfo()) .enable( enabled ) .select() .apis(RequestHandlerSelectors. *basePackage* ( &quot;com.atguigu.docker&quot; )) *//* 你自己的 *package .paths(PathSelectors. *any* ()) .build(); &#125; public ApiInfo apiInfo() &#123; return new ApiInfoBuilder() .title( &quot; 尚硅谷 Java 大厂技术 &quot; + &quot; \\t &quot; + new SimpleDateFormat( &quot;yyyy-MM-dd&quot; ).format( new Date())) .description( &quot;docker-compose&quot; ) .version( &quot;1.0&quot; ) .termsOfServiceUrl( &quot;https://www.atguigu.com/&quot; ) .build(); &#125; &#125; 新建entity User 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768package com.atguigu.docker.entities; import javax.persistence. Column ; import javax.persistence. GeneratedValue ; import javax.persistence. Id ; import javax.persistence. Table ; import java.util.Date; @Table (name = &quot;t_user&quot; ) public class User &#123; @Id @GeneratedValue (generator = &quot;JDBC&quot; ) private Integer id ; private String username ; private String password ; private Byte sex ; private Byte deleted ; @Column (name = &quot;update_time&quot; ) private Date updateTime ; @Column (name = &quot;create_time&quot; ) private Date createTime ; public Integer getId() &#123; return id ; &#125; public void setId(Integer id) &#123; this . id = id; &#125; public String getUsername() &#123; return username ; &#125; public void setUsername(String username) &#123; this . username = username; &#125; public String getPassword() &#123; return password ; &#125; public void setPassword(String password) &#123; this . password = password; &#125; public Byte getSex() &#123; return sex ; &#125; public void setSex(Byte sex) &#123; this . sex = sex; &#125; public Byte getDeleted() &#123; return deleted ; &#125; public void setDeleted(Byte deleted) &#123; this . deleted = deleted; &#125; public Date getUpdateTime() &#123; return updateTime ; &#125; public void setUpdateTime(Date updateTime) &#123; this . updateTime = updateTime; &#125; public Date getCreateTime() &#123; return createTime ; &#125; public void setCreateTime(Date createTime) &#123; this . createTime = createTime; &#125; &#125; UserDTO 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157package com.atguigu.docker.entities;import io.swagger.annotations. ApiModel ;import io.swagger.annotations. ApiModelProperty ;import lombok. AllArgsConstructor ;import lombok. Data ;import lombok. NoArgsConstructor ;import java.io.Serializable;import java.util.Date;@NoArgsConstructor@AllArgsConstructor@Data@ApiModel (value = &quot; 用户信息 &quot; )public class UserDTO implements Serializable&#123; @ApiModelProperty (value = &quot; 用户 ID&quot; ) private Integer id ; @ApiModelProperty (value = &quot; 用户名 &quot; ) private String username ; @ApiModelProperty (value = &quot; 密码 &quot; ) private String password ; @ApiModelProperty (value = &quot; 性别 0= 女 1= 男 &quot; ) private Byte sex ; @ApiModelProperty (value = &quot; 删除标志，默认 0 不删除， 1 删除 &quot; ) private Byte deleted ; @ApiModelProperty (value = &quot; 更新时间 &quot; ) private Date updateTime ; @ApiModelProperty (value = &quot; 创建时间 &quot; ) private Date createTime ; /** * @return id */ public Integer getId() &#123; return id ; &#125; /** * @param id */ public void setId(Integer id) &#123; this . id = id; &#125; /** * 获取用户名 * * @return username - 用户名 */ public String getUsername() &#123; return username ; &#125; /** * 设置用户名 * * @param username 用户名 */ public void setUsername(String username) &#123; this.username = username; &#125; /** * 获取密码 * * @return password - 密码 */ public String getPassword() &#123; return password ; &#125; /** * 设置密码 * * @param password 密码 */ public void setPassword(String password) &#123; this.password=password; &#125; /** *获取性别 0= 女 1= 男 * * @return sex - 性别 0= 女 1= 男 */ public Byte getSex() &#123; return sex ; &#125; /** * 设置性别 0= 女 1= 男 * * @param sex 性别 0= 女 1= 男 */ public void setSex(Byte sex) &#123; this.sex = sex; &#125; /** * 获取删除标志，默认 0 不删除， 1 删除 * * @return deleted - 删除标志，默认 0 不删除， 1 删除 */ public Byte getDeleted() &#123; return deleted ; &#125; /** * 设置删除标志，默认 0 不删除， 1 删除 * * @param deleted 删除标志，默认 0 不删除， 1 删除 */ public void setDeleted(Byte deleted) &#123; this.deleted = deleted; &#125; /** * 获取更新时间 * * @return update_time - 更新时间 */ public Date getUpdateTime() &#123; return updateTime ; &#125; /** * 设置更新时间 * * @param updateTime 更新时间 */ public void setUpdateTime(Date updateTime) &#123; this . updateTime = updateTime; &#125; /** * 获取创建时间 * * @return create_time - 创建时间 */ public Date getCreateTime() &#123; return createTime ; &#125; /** * 设置创建时间 * * @param createTime 创建时间 */ public void setCreateTime(Date createTime) &#123; this . createTime = createTime; &#125; @Override public String toString() &#123; \\ return &quot;User&#123;&quot; + &quot;id=&quot; + id + &quot;, username=&#x27;&quot; + username + &#x27; \\&#x27; &#x27; + &quot;, password=&#x27;&quot; + password + &#x27; \\&#x27; &#x27; + &quot;, sex=&quot; + sex + &#x27;&#125;&#x27; ; &#125;&#125; 新建mapper 12345678新建接口UserMappersrc\\main\\resource路径下新建mapper文件夹并新增UserMapper.xmlpackage com.atguigu.docker.mapper;import com.atguigu.docker.entities.User;import tk.mybatis.mapper.common.Mapper;public interface UserMapper extends Mapper&lt;User&gt; &#123;&#125; UserMapper.xml 123456789101112131415 &lt;? xml version =&quot;1.0&quot; encoding =&quot;UTF-8&quot; ?&gt; &lt;!DOCTYPE mapper PUBLIC &quot;-//mybatis.org//DTD Mapper 3.0//EN&quot; &quot;http://mybatis.org/dtd/mybatis-3-mapper.dtd&quot; &gt; &lt; mapper namespace =&quot;com.atguigu.docker.mapper.UserMapper&quot; &gt; &lt; resultMap id =&quot;BaseResultMap&quot; type =&quot;com.atguigu.docker.entities.User&quot; &gt; &lt;!-- WARNING - @mbg.generated --&gt; &lt; id column =&quot;id&quot; jdbcType =&quot;INTEGER&quot; property =&quot;id&quot; /&gt; &lt; result column =&quot;username&quot; jdbcType =&quot;VARCHAR&quot; property =&quot;username&quot; /&gt; &lt; result column =&quot;password&quot; jdbcType =&quot;VARCHAR&quot; property =&quot;password&quot; /&gt; &lt; result column =&quot;sex&quot; jdbcType =&quot;TINYINT&quot; property =&quot;sex&quot; /&gt; &lt; result column =&quot;deleted&quot; jdbcType =&quot;TINYINT&quot; property =&quot;deleted&quot; /&gt; &lt; result column =&quot;update_time&quot; jdbcType =&quot;TIMESTAMP&quot; property =&quot;updateTime&quot; /&gt; &lt; result column =&quot;create_time&quot; jdbcType =&quot;TIMESTAMP&quot; property =&quot;createTime&quot; /&gt; &lt;/ resultMap &gt; &lt;/ mapper &gt; 新建Service 1234```**新建Controller** 1234567891011121314151617181920212223mvn package命令将微服务形成新的jar包并上传到Linux服务器/mydocker目录下**编写Dockerfile**```shell# 基础镜像使用java FROM java:8 # 作者 MAINTAINER zzyy # VOLUME 指定临时文件目录为/tmp，在主机/var/lib/docker目录下创建了一个临时文件并链接到容器的/tmp VOLUME /tmp # 将jar包添加到容器中并更名为zzyy_docker.jar ADD docker_boot-0.0.1-SNAPSHOT.jar zzyy_docker.jar # 运行jar包 RUN bash -c &#x27;touch /zzyy_docker.jar&#x27; ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/zzyy_docker.jar&quot;] #暴露6001端口作为微服务 EXPOSE 6001 构建镜像 1docker build -t zzyy_docker:1.6 . 5.5.2 不用Compose 1234567891011121314151617181920一、单独的mysql容器实例1. 新建mysql容器实例docker run -p 3306:3306 --name mysql57 --privileged=true -v /zzyyuse/mysql/conf:/etc/mysql/conf.d -v /zzyyuse/mysql/logs:/logs -v /zzyyuse/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.72. 进入mysql容器实例并新建库db2021+新建表t_userdocker exec -it mysql57 /bin/bash mysql -uroot -p create database db2021; use db2021; CREATE TABLE `t_user` ( `id` INT(10) UNSIGNED NOT NULL AUTO_INCREMENT, `username` VARCHAR(50) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;用户名&#x27;, `password` VARCHAR(50) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;密码&#x27;, `sex` TINYINT(4) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;性别 0=女 1=男 &#x27;, `deleted` TINYINT(4) UNSIGNED NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;删除标志，默认0不删除，1删除&#x27;, `update_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;更新时间&#x27;, `create_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COMMENT=&#x27;用户表&#x27;; 单独的redis容器实例 1docker run -p 6379:6379 --name redis608 --privileged=true -v /app/redis/redis.conf:/etc/redis/redis.conf -v /app/redis/data:/data -d redis:6.0.8 redis-server /etc/redis/redis.conf 微服务工程 1docker run -d -p 6001:6001 zzyy_docker:1.6 上面三个容器实例依次顺序启动成功 1docker ps 5.5.3 swagger 测试 1http://localhost:你的微服务端口号/swagger-ui.html#/ 5.5.4 上面存在什么问题？1234567先后顺序要求固定，先mysql+redis才能微服务访问成功多个run命令......容器间的启停或宕机，有可能导致IP地址对应的容器实例变化，映射出错，要么生产IP写死(可以但是不推荐)，要么通过服务调用 5.5.5 使用Compose1231. 服务编排，一套带走，安排2. 编写docker-componse.yml文件 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849version: &quot;3&quot; services: microService: image: zzyy_docker:1.6 container_name: ms01 ports: - &quot;6001:6001&quot; volumes: - /app/microService:/data networks: - atguigu_net depends_on: - redis - mysql redis: image: redis:6.0.8 ports: - &quot;6379:6379&quot; volumes: - /app/redis/redis.conf:/etc/redis/redis.conf - /app/redis/data:/data networks: - atguigu_net command: redis-server /etc/redis/redis.conf mysql: image: mysql:5.7 environment: MYSQL_ROOT_PASSWORD: &#x27;123456&#x27; MYSQL_ALLOW_EMPTY_PASSWORD: &#x27;no&#x27; MYSQL_DATABASE: &#x27;db2021&#x27; MYSQL_USER: &#x27;zzyy&#x27; MYSQL_PASSWORD: &#x27;zzyy123&#x27; ports: - &quot;3306:3306&quot; volumes: - /app/mysql/db:/var/lib/mysql - /app/mysql/conf/my.cnf:/etc/my.cnf - /app/mysql/init:/docker-entrypoint-initdb.d networks: - atguigu_net command: --default-authentication-plugin=mysql_native_password #解决外部无法访问 networks: atguigu_net: 123. 第二次修改微服务工程docker_boot写YML 通过服务名访问，IP无关 12345678910111213141516171819202122server.port = 6001# ========================alibaba.druid 相关配置 =====================spring.datasource.type = com.alibaba.druid.pool.DruidDataSourcespring.datasource.driver-class-name=com.mysql.jdbc.Driver#spring.datasource.url=jdbc:mysql://192.168.111.169:3306/db2021?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=falsespring.datasource.url = jdbc:mysql://mysql:3306/db2021?useUnicode=true&amp;characterEncoding=utf-8&amp;useSSL=false spring.datasource.username = rootspring.datasource.password = 123456spring.datasource.druid.test-while-idle = false# ========================redis 相关配置 =====================spring.redis.database = 0#spring.redis.host=192.168.111.169spring.redis.host = redis spring.redis.port = 6379spring.redis.password =spring.redis.lettuce.pool.max-active = 8spring.redis.lettuce.pool.max-wait = -1msspring.redis.lettuce.pool.max-idle = 8spring.redis.lettuce.pool.min-idle = 0# ========================mybatis 相关配置 ===================mybatis.mapper-locations = classpath:mapper/*.xmlmybatis.type-aliases-package = com.atguigu.docker.entities# ========================swagger=====================spring.swagger2.enabled = true 12345678910111213141516171819202122mvn package命令将微服务形成新的jar包并上传到Linux服务器/mydocker目录下编写Dockerfile# 基础镜像使用java FROM java:8 # 作者 MAINTAINER zzyy # VOLUME 指定临时文件目录为/tmp，在主机/var/lib/docker目录下创建了一个临时文件并链接到容器的/tmp VOLUME /tmp # 将jar包添加到容器中并更名为zzyy_docker.jar ADD docker_boot-0.0.1-SNAPSHOT.jar zzyy_docker.jar # 运行jar包 RUN bash -c &#x27;touch /zzyy_docker.jar&#x27; ENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;/zzyy_docker.jar&quot;] #暴露6001端口作为微服务 EXPOSE 6001 构建镜像docker build -t zzyy_docker:1.6 . 14. 执行 docker-compose up 或者 执行 docker-compose up -d 123456789101112131415165. 进入mysql容器实例并新建库db2021+新建表t_userdocker exec -it 容器实例id /bin/bash mysql -uroot -p create database db2021; use db2021; CREATE TABLE `t_user` ( `id` INT(10) UNSIGNED NOT NULL AUTO_INCREMENT, `username` VARCHAR(50) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;用户名&#x27;, `password` VARCHAR(50) NOT NULL DEFAULT &#x27;&#x27; COMMENT &#x27;密码&#x27;, `sex` TINYINT(4) NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;性别 0=女 1=男 &#x27;, `deleted` TINYINT(4) UNSIGNED NOT NULL DEFAULT &#x27;0&#x27; COMMENT &#x27;删除标志，默认0不删除，1删除&#x27;, `update_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP COMMENT &#x27;更新时间&#x27;, `create_time` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP COMMENT &#x27;创建时间&#x27;, PRIMARY KEY (`id`) ) ENGINE=INNODB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4 COMMENT=&#x27;用户表&#x27;; 12345678910111213141516176. 测试通过7. Compose常用命令Compose 常用命令 docker-compose -h # 查看帮助 docker-compose up # 启动所有 docker-compose服务 docker-compose up -d # 启动所有 docker-compose服务 并后台运行 docker-compose down # 停止并删除容器、网络、卷、镜像。 docker-compose exec yml里面的服务id # 进入容器实例内部 docker-compose exec docker-compose.yml文件中写的服务id /bin/bash docker-compose ps # 展示当前docker-compose编排过的运行的所有容器 docker-compose top # 展示当前docker-compose编排过的容器进程 docker-compose logs yml里面的服务id # 查看容器输出日志 dokcer-compose config # 检查配置 dokcer-compose config -q # 检查配置，有问题才有输出 docker-compose restart # 重启服务 docker-compose start # 启动服务 docker-compose stop # 停止服务 128. 关停docker -compose stop 六、Docker轻量级可视化工具Portainer6.1 是什么1Portainer 是一款轻量级的应用，它提供了图形化界面，用于方便地管理Docker环境，包括单机环境和集群环境。 6.2 安装 一、官网 https://www.portainer.io/ https://docs.portainer.io/v/ce-2.9/start/install/server/docker/linux 二、步骤 docker命令安装 1&gt;docker run -d -p 8000:8000 -p 9000:9000 --name portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer 第一次登录需创建admin，访问地址：xxx.xxx.xxx.xxx:9000 12&gt;用户名，直接用默认admin &gt;密码记得8位，随便你写 设置admin用户和密码后首次登陆 选择local选项卡后本地docker详细信息展示 上一步的图形展示，能想得起对应命令吗？ 登陆并演示介绍常用操作case 七、Docker容器监控之CAdvisor+InfluxDB+Granfana7.1 原生命令123456docker stats命令的结果 问题通过docker stats命令可以很方便的看到当前宿主机上所有容器的CPU,内存以及网络流量等数据， 一般小公司够用了。。。。 但是docker stats统计结果只能是当前宿主机的全部容器，数据资料是实时的，没有地方存储、没有健康指标过线预警等功能 7.2 是什么 容器监控3剑客 一句话 1&gt;CAdvisor监控收集+InfluxDB存储数据+Granfana展示图表 CAdvisor InfluxDB Granfana 7.3 compose容器编排，一套带走 一、新建目录 二、新建3件套组合的 docker-compose.yml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051&gt;version: &#x27;3.1&#x27; &gt;volumes: &gt;grafana_data: &#123;&#125; &gt;services: &gt;influxdb: &gt;image: tutum/influxdb:0.9 &gt;restart: always &gt;environment: - PRE_CREATE_DB=cadvisor ports: - &quot;8083:8083&quot; - &quot;8086:8086&quot; volumes: - ./data/influxdb:/data &gt;cadvisor: image: google/cadvisor links: - influxdb:influxsrv command: -storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=influxsrv:8086 restart: always ports: - &quot;8080:8080&quot; volumes: - /:/rootfs:ro - /var/run:/var/run:rw - /sys:/sys:ro - /var/lib/docker/:/var/lib/docker:ro &gt;grafana: user: &quot;104&quot; image: grafana/grafana user: &quot;104&quot; restart: always links: - influxdb:influxsrv ports: - &quot;3000:3000&quot; volumes: - grafana_data:/var/lib/grafana environment: - HTTP_USER=admin - HTTP_PASS=admin - INFLUXDB_HOST=influxsrv - INFLUXDB_PORT=8086 - INFLUXDB_NAME=cadvisor - INFLUXDB_USER=root - INFLUXDB_PASS=root 三、启动docker-compose文件 1&gt;docker-compose up 四、查看三个服务容器是否启动 1&gt;docker ps 五、测试 1234567&gt;1. 浏览cAdvisor收集服务，http://ip:8080/&gt;第一次访问慢，请稍等&gt;cadvisor也有基础的图形展现功能，这里主要用它来作数据采集&gt;2. 浏览influxdb存储服务，http://ip:8083/ 123456789&gt;3. 浏览grafana展现服务，http://ip:3000ip+3000端口的方式访问,默认帐户密码（admin/admin）https://gitee.com/yooome/golang/tree/main/Docker详细教程配置步骤[1] 配置数据源[2] 选择influxdb数据源[3] 配置细节[4] 配置面板panel[5] 到这里cAdvisor+InfluxDB+Grafana容器监控系统就部署完成了 原视频地址 原文章地址","categories":[{"name":"Docker","slug":"Docker","permalink":"https://lwy0518.github.io/categories/Docker/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://lwy0518.github.io/tags/Docker/"}],"author":"lwy"},{"title":"docker学习笔记","slug":"docker学习笔记","date":"2022-02-13T13:01:04.000Z","updated":"2022-02-13T13:50:07.692Z","comments":true,"path":"2022/02/13/docker学习笔记/","link":"","permalink":"https://lwy0518.github.io/2022/02/13/docker%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"目录","text":"目录 Docker 详细教程一、Docker简介1.1 docker是什么【问题】：问什么会有docker出现 Docker的出现 使得Docker得以打破过去「程序即应用」的观念。透过镜像(images)将作业系统核心除外，运作应用程式所需要的系统环境，由下而上打包，达到应用程式跨平台间的无缝接轨运作。 【docker理念】：解决了运行环境和配置问题的软件容器，方便持续继承并有助于整体发布的容器虚拟化技术。 1.2 容器与虚拟机比较1.2.1 容器发展简史￼￼￼ 1.2.2 传统虚拟机技术虚拟机（virtual machine）就是带环境安装的一种解决方案。 它可以在一种操作系统里面运行另一种操作系统，比如在Windows10系统里面运行Linux系统CentOS7。应用程序对此毫无感知，因为虚拟机看上去跟真实系统一模一样，而对于底层系统来说，虚拟机就是一个普通文件，不需要了就删掉，对其他部分毫无影响。这类虚拟机完美的运行了另一套系统，能够使应用程序，操作系统和硬件三者之间的逻辑不变。 Win10 VMWare Centos7 各种cpu、内存网络额配置+各种软件 虚拟机实例 虚拟机的缺点： 1 资源占用多 2 冗余步骤多 3 启动慢 1.2.3 容器虚拟化技术由于前面虚拟机存在某些缺点，Linux发展出了另一种虚拟化技术： Linux容器(Linux Containers，缩写为 LXC) Linux容器是与系统其他部分隔离开的一系列进程，从另一个镜像运行，并由该镜像提供支持进程所需的全部文件。容器提供的镜像包含了应用的所有依赖项，因而在从开发到测试再到生产的整个过程中，它都具有可移植性和一致性。 Linux 容器不是模拟一个完整的操作系统 而是对进程进行隔离。有了容器，就可以将软件运行所需的所有资源打包到一个隔离的容器中。 容器与虚拟机不同，不需要捆绑一整套操作系统 ，只需要软件工作所需的库资源和设置。系统因此而变得高效轻量并保证部署在任何环境中的软件都能始终如一地运行。 1.2.4 对比 比较了 Docker 和传统虚拟化方式的不同之处： 传统虚拟机技术是虚拟出一套硬件后，在其上运行一个完整操作系统，在该系统上再运行所需应用进程； 容器内的应用进程直接运行于宿主的内核，容器内没有自己的内核 且也没有进行硬件虚拟 。因此容器要比传统虚拟机更为轻便。 每个容器之间互相隔离，每个容器有自己的文件系统 ，容器之间进程不会相互影响，能区分计算资源。 1.3 能干什么1.3.1 技术职级变化coder -&gt; programmer -&gt; software engineer -&gt; DevOps engineer 1.3.2 开发/运维（Devops)新一代开发工程师 一次构建、随处运行 更快速的应用交付和部署 更便捷的升级和扩缩容 更简单的系统运维 更高效的计算资源利用 1.3.3 Docker应用场景 Docker 借鉴了标砖集装箱的概念。标准集装箱将货物运往世界各地，Docker将这个模型运用到自己的设计中，唯一不同的是：集装箱运输货物，而Docker运输软件。 1.4 那些企业在使用 新浪 美团 蘑菇街 1.5 下载地址官网：http://www.docker.com Docker Hub 官网：https://hub.docker.com 二、Docker安装2.1 前提说明2.1.1 CentOS Docker 安装 2.1.2 前提条件目前，CentOS仅发行版本中的内核支持Docker。Docker运行在CentOS 7（64-bit）上，要求系统为64位，Linux系统内核版本为3.8以上，这里选用Centos7.x 2.1.3 查看自己的内核uname 命令用于打印当前系统相关信息（内核版本号，硬件架构，主机名称和操作系统类型等）。 2.2 Docker的基本组成2.2.1 镜像（image）Docker 镜像（Image）就是一个 只读 的模板。镜像可以用来创建 Docker 容器， 一个镜像可以创建很多容器 。 它也相当于是一个root文件系统。比如官方镜像 centos:7 就包含了完整的一套 centos:7 最小系统的 root 文件系统。 相当于容器的“源代码”， docker镜像文件类似于Java的类模板，而docker容器实例类似于java中new出来的实例对象。 2.2.2 容器（container） 从面向对象角度 Docker 利用容器（Container）独立运行的一个或一组应用，应用程序或服务运行在容器里面，容器就类似于一个虚拟化的运行环境， 容器是用镜像创建的运行实例 。就像是Java中的类和实例对象一样，镜像是静态的定义，容器是镜像运行时的实体。容器为镜像提供了一个标准的和隔离的运行环境 ，它可以被启动、开始、停止、删除。每个容器都是相互隔离的、保证安全的平台 从镜像容器角度 可以把容器看做是一个简易版的 *Linux* 环境 （包括root用户权限、进程空间、用户空间和网络空间等）和运行在其中的应用程序。 2.2.3 仓库（repository）仓库（Repository）是 集中存放镜像 文件的场所。 类似于 Maven仓库，存放各种jar包的地方； github仓库，存放各种git项目的地方； Docker公司提供的官方registry被称为Docker Hub，存放各种镜像模板的地方。 仓库分为公开仓库（Public）和私有仓库（Private）两种形式。 最大的公开仓库是 Docker Hub(https://hub.docker.com/) ， 存放了数量庞大的镜像供用户下载。国内的公开仓库包括阿里云 、网易云等 2.2.4 小总结 需要正确的理解仓库/镜像/容器这几个概念: Docker 本身是一个容器运行载体或称之为管理引擎。我们把应用程序和配置依赖打包好形成一个可交付的运行环境，这个打包好的运行环境就是image镜像文件。只有通过这个镜像文件才能生成Docker容器实例(类似Java中new出来一个对象)。 image文件可以看作是容器的模板。Docker 根据 image 文件生成容器的实例。同一个 image 文件，可以生成多个同时运行的容器实例。 镜像文件 image 文件生成的容器实例，本身也是一个文件，称为镜像文件。 容器实例 一个容器运行一种服务，当我们需要的时候，就可以通过docker客户端创建一个对应的运行实例，也就是我们的容器 。 仓库 就是放一堆镜像的地方，我们可以把镜像发布到仓库中，需要的时候再从仓库中拉下来就可以了。 2.3 Docker平台架构图解（入门版） 2.3.1 Docker工作原理Docker是一个Client-Server结构的系统，Docker守护进程运行在主机上， 然后通过Socket连接从客户端访问，守护进程从客户端接受命令并管理运行在主机上的容器 。 容器，是一个运行时环境，就是我们前面说到的集装箱。可以对比mysql演示对比讲解 2.3.2 整体架构及底层通信原理简述Docker是一个C/S模式的架构，后端是一个松耦合架构，众多模块各司其职 2.3.3 Docker运行的基本流程为： 用户是使用Docker Client 与Docker Daemon 建立通信，并发送请求给后者。 Docker Daemon 作为Docker架构中的主体部分，首先提供Docker Server 的功能时期可以接受 Docker Client的请求。 Docker Engine 执行Docker内部的一些列工作，每一项工作都是以一个Job的形式的存在。 Job的运行过程中，当需要容器镜像是，则从Docker Register中下载镜像，并通过镜像管理驱动Graph driver 将下载镜像以Graph的形式存储。 当需要为Docker创建网络环境时，通过网络驱动Network driver创建并配置Docker容器网络环境。 当需要限制Docker容器运行资源或执行用户指令等操作时，则通过Exec driver来完成。 Libcontainer是一项独立的容器管理包，Network driver以及Exec driver都是通过Libcontainer来实现具体容器进行的操作。 2.4、安装步骤2.4.1 CentOS7安装Docker 确定你是CentOS7以上版本 12# 查看CentOS版本命令cat /etc/redhat-release 卸载旧版本 123456789# 卸载旧版本docker命令$ sudo yum remove docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-engine yum安装gcc相关命令 123# yum安装gcc相关命令yum -y install gccyum -y install gcc-c++ 安装需要的软件包 使用存储库安装 12345在新主机上首次安装Docker Engine之前，您需要设置Docker存储库。之后，您可以从存储库安装和更新Docker设置存储库安装 yum-utils 包（提供yum-config-manager 实用程序）并设置稳定的存储库# 官网要求yum install -y yum-utils 设置stable镜像仓库 12# 推荐使用 使用阿里的 docker 镜像仓库，国外的镜像仓库是比较慢的yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 更新yum软件包索引 12# 更新yum软件包索引yum makecache fast 安装DOCKER CE 引擎 12# 命令yum -y install docker-ce docker-ce-cli containerd.io 启动docker 12# 启动命令systemctl start docker 测试 1234# 测试docker version docker run hello-world 卸载 12345# 卸载命令systemctl stop docker yum remove docker-ce docker-ce-cli containerd.iorm -rf /var/lib/dockerrm -rf /var/lib/containerd 2.5、阿里云镜像加速2.5.1 是什么 地址：https://promotion.aliyun.com/ntms/act/kubernetes.html 注册一个属于自己的阿里云账户 获得加速器地址连接： 登陆阿里云开发者平台 点击控制台 选择容器镜像服务 获取加速器地址 粘贴脚本直接执行 123456mkdir -p /etc/docker tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;&#123; &quot;registry-mirrors&quot;: [&quot;https://aa25jngu.mirror.aliyuncs.com&quot;] &#125; EOF 123# 或者分开步骤执行mkdir -p /etc/dockervim /etc/docker/daemon.json 重启服务器 123# 重启服务器systemctl daemon-reloadsystemctl restart docker 2.5.2 永远的HelloWorld启动Docker后台容器（测试运行 hello-world） 12# 命令docker run hello-world 2.5.3 底层原理为什么Docker会比VM虚拟机快: 1234(1)docker有着比虚拟机更少的抽象层 由于docker不需要Hypervisor(虚拟机)实现硬件资源虚拟化,运行在docker容器上的程序直接使用的都是实际物理机的硬件资源。因此在CPU、内存利用率上docker将会在效率上有明显优势。 (2)docker利用的是宿主机的内核,而不需要加载操作系统OS内核 当新建一个容器时,docker不需要和虚拟机一样重新加载一个操作系统内核。进而避免引寻、加载操作系统内核返回等比较费时费资源的过程,当新建一个虚拟机时,虚拟机软件需要加载OS,返回新建过程是分钟级别的。而docker由于直接利用宿主机的操作系统,则省略了返回过程,因此新建一个docker容器只需要几秒钟。 三、Docker常用命令3.1 帮助启动类命令12345678910111213141516# 启动命令systemctl start docker# 停止命令systemctl stop docker# 重启命令systemctl restart docker# 查看docker状态systemctl status docker# 开机启动systemctl enable docker# 查看 docker 概要信息docker info# 查看docker 总体帮助文档docker --help# 查看docker命令帮助文档：docker 具体命令 --help 3.2 镜像命令3.2.1 docker images12# 列出本地主机上的镜像docker images 各个选项说明: REPOSITORY：表示镜像的仓库源 TAG：镜像的标签版本号 IMAGE ID：镜像ID CREATED：镜像创建时间 SIZE：镜像大小 同一仓库源可以有多个 TAG版本，代表这个仓库源的不同个版本，我们使用 REPOSITORY:TAG 来定义不同的镜像。 如果你不指定一个镜像的版本标签，例如你只使用 ubuntu，docker 将默认使用 ubuntu:latest 镜像 3.2.2 OPTIONS 说明-a : 列出本地所有的镜像（含历史映像层） -q：只显示镜像ID 3.2.3 docker search 某个XXX镜像名字1234567# 网站https://hub.docker.com# 命令docker search [OPTIONS]镜像名字# OPTIONS说明# --limit ：只列出N个镜像，默认25个docker search --limit 5 redis 案例： 3.2.4 docker pull 某个XXX镜像名字12345678# 下载镜像 docker pull 镜像名字[:TAG] docker pull 镜像名字 # 没有TAG就是最新版本 等价于 docker pull 镜像名字：latest docker pull ubuntu 3.2.5 docker system df 查看镜像/容器/数据卷所占用的空间 3.2.6 docker rmi 删除镜像12345678# 删除单个docker rmi -f 镜像ID# 删除多个docker rmi -f 镜像名1:TAG 镜像名2:TAG# 删除全部docker rmi -f $(docker images -qa) 3.2.7 谈谈docker虚悬镜像是什么？123仓库名称，标签都是&lt;none&gt;的镜像，俗称虚悬镜像dangling image长什么样子后续Dockerfile章节在介绍 3.3 容器命令 有镜像才能创建容器，这是根本前提（下载一个CentOS或者ubuntu镜像演示） 1.说明 2.docker pull centos3.docker pull ubuntu4.本次演示用ubuntu演示 3.3.1 新建+启动容器 新建+启动容器 命令docker run [OPTIONS] IMAGE [COMMAND] [ARG…] OPTIONS说明OPTIONS说明（常用）：有些是一个减号，有些是两个减号 –name=”容器新名字” 为容器指定一个名称；-d: 后台运行容器并返回容器ID，也即启动守护式容器(后台运行)； -i：以交互模式运行容器，通常与 -t 同时使用；-t：为容器重新分配一个伪输入终端，通常与 -i 同时使用；也即 启动交互式容器(前台有伪终端，等待交互) ； -P: 随机 端口映射，大写P-p: 指定 端口映射，小写p 启动交互式容器（前台命令行） 使用镜像centos:latest以 交互模式 启动一个容器,在容器内执行/bin/bash命令。 docker run -it centos /bin/bash 参数说明： -i: 交互式操作。 -t: 终端。 centos : centos 镜像。 /bin/bash：放在镜像名后的是命令，这里我们希望有个交互式 Shell，因此用的是 /bin/bash。 要退出终端，直接输入 exit: 3.3.2 列出当前所有正在运行的容器1234567# 列出当前所有正在运行的容器docker ps [OPTIONS]# OPTIONS说明-a : 列出当前所有 正在运行 的容器 + 历史上运行过 的 -l :显示最近创建的容器。 -n：显示最近n个创建的容器。 -q :静默模式，只显示容器编号。 3.3.3 退出容器12345# 两种退出方式# 1、run进去容器，exit退出，容器停止exit # 2、run进去容器，ctrl+p+q退出，容器不停止ctrl+p+q 3.3.4 启动已停止运行的容器1234567891011121314# 启动已停止运行的容器docker start 容器ID或者容器名# 重启容器docker restart 容器ID或者容器名# 停止容器docker stop 容器ID或者容器名# 强制停止容器docker kill 容器ID或容器名# 删除已停止的容器docker rm 容器ID# 一次性删除多个容器实例docker rm -rf $(docker ps -a -q)docker ps -a -q | xargs docker rm 3.3.5 重要启动守护式容器（后台服务器）： 12345678910111213141516171819有镜像才能创建容器，这是根本前提（下载一个Redis6.0.8镜像演示）在大部分的场景下，我们希望docker的服务是在后台运行的，我们可以通过 -d 指定容器的后台运行模式。docker run -d 容器名# 使用镜像centos:latest以后台模式启动一个容器 docker run -d centos 问题：然后docker ps -a 进行查看, 会发现容器已经退出 很重要的要说明的一点: Docker容器后台运行,就必须有一个前台进程. 容器运行的命令如果不是那些 一直挂起的命令 （比如运行top，tail），就是会自动退出的。 这个是docker的机制问题,比如你的web容器,我们以nginx为例，正常情况下, 我们配置启动服务只需要启动响应的service即可。例如service nginx start 但是,这样做,nginx为后台进程模式运行,就导致docker前台没有运行的应用, 这样的容器后台启动后,会立即自杀因为他觉得他没事可做了. 所以，最佳的解决方案是, 将你要运行的程序以前台进程的形式运行， 常见就是命令行模式，表示我还有交互操作，别中断，O(∩_∩)O哈哈~ redis前后台启动演示case 1234# 前台交互式启动docker run -it redis:6.0.8# 后台交互式启动docker run -d redis:6.0.8 查看容器日志 12# 查看容器日志docker logs 容器ID 查看容器内运行的进程 12# 查看容器内运行的进程docker top 容器ID 查看容器内部细节 12# 查看容器内部细节docker inspect 容器ID 进入正在运行的容器并以命令行交互 1docker exec -it 容器ID bashShell 重新进入docker attach 容器ID 案例演示，用centos或者unbuntu都可以上述两个区别： attach 直接进入容器启动命令的终端，不会启动新的进程用exit退出，会导致容器的停止。 2. exec 是在容器中打开新的终端，并且可以启动新的进程用exit退出，不会导致容器的停止。 推荐大家使用docker exec 命令，因为退出容器终端，不会导致容器的停止。 使用之前的redis容器实例进入试试 12345docker exec -it 容器ID /bin/bashdocker exec -it 容器ID redis-cli一般用-d后台启动的程序，在用exec进入对应容器实例 从容器内拷贝文件到主机上 容器 -&gt; 主机 docker cp 容器ID:容器内路径 目的主机路径 公式： docker cp 容器 ID: 容器内路径 目的主机路径 导入和导出容器 Export 导出容器的内容留作为一个tar归档文件[对应import命令] import 从tar 包中的内容创建一个新的文件系统在导入为镜像[对应export] 【案例】： docker export 容器ID &gt; 文件.tar cat 文件名.tar | docker import -镜像用户/镜像名:镜像版本号 3.4 小总结 12345678910111213141516171819202122232425262728293031323334353637attach Attach to a running container # 当前 shell 下 attach 连接指定运行镜像 build Build an image from a Dockerfile # 通过 Dockerfile 定制镜像 commit Create a new image from a container changes # 提交当前容器为新的镜像 cp Copy files/folders from the containers filesystem to the host path #从容器中拷贝指定文件或者目录到宿主机中 create Create a new container # 创建一个新的容器，同 run，但不启动容器 diff Inspect changes on a container&#x27;s filesystem # 查看 docker 容器变化 events Get real time events from the server # 从 docker 服务获取容器实时事件 exec Run a command in an existing container # 在已存在的容器上运行命令 export Stream the contents of a container as a tar archive # 导出容器的内容流作为一个 tar 归档文件[对应 import ] history Show the history of an image # 展示一个镜像形成历史 images List images # 列出系统当前镜像 import Create a new filesystem image from the contents of a tarball # 从tar包中的内容创建一个新的文件系统映像[对应export] info Display system-wide information # 显示系统相关信息 inspect Return low-level information on a container # 查看容器详细信息 kill Kill a running container # kill 指定 docker 容器 load Load an image from a tar archive # 从一个 tar 包中加载一个镜像[对应 save] login Register or Login to the docker registry server # 注册或者登陆一个 docker 源服务器 logout Log out from a Docker registry server # 从当前 Docker registry 退出 logs Fetch the logs of a container # 输出当前容器日志信息 port Lookup the public-facing port which is NAT-ed to PRIVATE_PORT # 查看映射端口对应的容器内部源端口 pause Pause all processes within a container # 暂停容器 ps List containers # 列出容器列表 pull Pull an image or a repository from the docker registry server # 从docker镜像源服务器拉取指定镜像或者库镜像 push Push an image or a repository to the docker registry server # 推送指定镜像或者库镜像至docker源服务器 restart Restart a running container # 重启运行的容器 rm Remove one or more containers # 移除一个或者多个容器 rmi Remove one or more images # 移除一个或多个镜像[无容器使用该镜像才可删除，否则需删除相关容器才可继续或 -f 强制删除] run Run a command in a new container # 创建一个新的容器并运行一个命令 save Save an image to a tar archive # 保存一个镜像为一个 tar 包[对应 load] search Search for an image on the Docker Hub # 在 docker hub 中搜索镜像 start Start a stopped containers # 启动容器 stop Stop a running containers # 停止容器 tag Tag an image into a repository # 给源中镜像打标签 top Lookup the running processes of a container # 查看容器中运行的进程信息 unpause Unpause a paused container # 取消暂停容器 version Show the docker version information # 查看 docker 版本号 wait Block until a container stops, then print its exit code # 截取容器停止时的退出状态值 四、Docker镜像4.1 是什么1234567891011【镜像】 是一种轻量级、可执行的独立软件包，它包含运行某个软件所需的所有内容，我们把应用程序和配置依赖打包好形成一个可交付的运行环境(包括代码、运行时需要的库、环境变量和配置文件等)，这个打包好的运行环境就是image镜像文件。 只有通过这个镜像文件才能生成Docker容器实例(类似Java中new出来一个对象)。【分层镜像】以我们的pull为例，在下载的过程中我们可以看到docker的镜像好像是在一层一层的在下载 。【UnionFS（联合文件系统）】UnionFS（联合文件系统）：Union文件系统（UnionFS）是一种分层、轻量级并且高性能的文件系统，它支持 对文件系统的修改作为一次提交来一层层的叠加， 同时可以将不同目录挂载到同一个虚拟文件系统下(unite several directories into a single virtual filesystem)。Union 文件系统是 Docker 镜像的基础。 镜像可以通过分层来进行继承 ，基于基础镜像（没有父镜像），可以制作各种具体的应用镜像。 特性：一次同时加载多个文件系统，但从外面看起来，只能看到一个文件系统，联合加载会把各层文件系统叠加起来，这样最终的文件系统会包含所有底层的文件和目录 Docker镜像加载原理 Docker镜像加载原理： docker的镜像实际上由一层一层的文件系统组成，这种层级的文件系统UnionFS。 bootfs(boot file system)主要包含bootloader和kernel, bootloader主要是引导加载kernel, Linux刚启动时会加载bootfs文件系统， 在Docker镜像的最底层是引导文件系统bootfs。 这一层与我们典型的Linux/Unix系统是一样的，包含boot加载器和内核。当boot加载完成之后整个内核就都在内存中了，此时内存的使用权已由bootfs转交给内核，此时系统也会卸载bootfs。 rootfs (root file system) ，在bootfs之上 。包含的就是典型 Linux 系统中的 /dev, /proc, /bin, /etc 等标准目录和文件。rootfs就是各种不同的操作系统发行版，比如Ubuntu，Centos等等。 平时我们安装进虚拟机的CentOS都是好几个G，为什么docker这里才200M？？ 对于一个精简的OS，rootfs可以很小，只需要包括最基本的命令、工具和程序库就可以了，因为底层直接用Host的kernel，自己只需要提供 rootfs 就行了。由此可见对于不同的linux发行版, bootfs基本是一致的, rootfs会有差别, 因此不同的发行版可以公用bootfs。 为什么Docker镜像要采用这种分层结构呢 镜像分层最大的一个好处就是共享资源，方便复制迁移，就是为了复用。 比如说有多个镜像都从相同的 base 镜像构建而来，那么 Docker Host 只需在磁盘上保存一份 base 镜像；同时内存中也只需加载一份 base 镜像，就可以为所有容器服务了。而且镜像的每一层都可以被共享。 4.2 重点理解 Docker镜像层都是只读的，容器层是可写的，当容器启动时，一个新的可写层被加载到镜像的顶部。这一层通常被称作”容器层”，”容器层”之下的都叫”镜像层”。 所有对容器的改动 - 无论添加、删除、还是修改文件都只会发生在容器层中。只有容器层是可写的，容器层下面的所有镜像层都是只读的。 4.3 Docker镜像commit操作案例 docker commit 提交容器副本使之成为一个新的镜像 docker commit -m=”提交的描述信息” -a=”作者” 容器ID 要创建的目标镜像名:[标签名] 【案例演示】ubuntu安装vim 从Hub上下ubuntu镜像到笨地并成功运行 原始默认Ubuntu镜像是不带着vim命令的 外网连通情况下，安装vim 1234# 先更新我们的包管理工具apt-get update# 然后安装我们需要的vimapt-get install vim docker容器内执行上述两条命令： apt-get update apt-get -y install vim 安装完成后，commit我们自己的新镜像 启动我们的新镜像并和原来的对比 官网是默认下载的Ubuntu没有vim命令 我们自己commit构建的镜像，新增加了vim功能，可以成功使用。 总结 12 Docker中的镜像分层， 支持通过扩展现有镜像，创建新的镜像 。类似Java继承于一个Base基础类，自己再按需扩展。 新镜像是从 base 镜像一层一层叠加生成的。每安装一个软件，就在现有镜像的基础上增加一层 五、本地镜像发布到阿里云5.1 本地镜像发布到阿里云流程 5.2 镜像生成的方法 上一讲已经介绍过 基于当前容器创建一个新的镜像，新功能增强 docker commit [OPTIONS]容器ID [REPOSOTORY[:TAG]] OPTIONS说明： -a :提交的镜像作者； -m :提交时的说明文字； 本次案例centos+ubuntu两个，当堂讲解一个，家庭作业一个，请大家务必动手，亲自实操。 5.3 将本地镜像推送到阿里云 本地镜像素材原型 阿里云开发者平台 地址：https://promotion.aliyun.com/ntms/act/kubernetes.html 将镜像推送到阿里云 将镜像推送到阿里云registry ，管理界面脚本 脚本实例 1234567docker login --username=zzyybuy registry.cn-hangzhou.aliyuncs.com docker tag cea1bb40441c registry.cn-hangzhou.aliyuncs.com/atguiguwh/myubuntu:1.1 docker push registry.cn-hangzhou.aliyuncs.com/atguiguwh/myubuntu:1.1 上面命令是阳哥自己本地的，你自己酌情处理，不要粘贴我的。 5.4 将阿里云上的镜像下载到本地 1docker pull registry.cn-hangzhou.aliyuncs.com/atguiguwh/myubuntu:1.1 六、本地镜像发布到私有库6.1 本地镜像发布到私有库流程 下载镜像Docker Registry docker pull registry 运行私有库Registry，相当于本地有个私有库Docker hub docker run -d -p 5000:5000 -v /zzyyuse/myregistry/:/tmp/registry –privileged=true registry 默认情况，仓库被创建在容器的/var/lib/registry目录下，建议自行用容器卷映射，方便于宿主机联调 案例演示创建一个新镜像，ubuntu安装ifconfig命令 从Hub上下载ubuntu镜像到本地并成功运行 原始Ubuntu镜像是不带着ifconfig命令的 从Hub上下载ubuntu镜像到本地并成功运行 原始Ubuntu镜像是不带着ifconfig命令的 外网连通情况下，安装ifconfig命令通过测试 docker容器内 执行上述两条命令： apt-get update apt-get install net-tools 安装完成后，commit我们自己的新镜像 公式： docker commit -m=” 提交的描述信息 “ -a=” 作者 “ 容器 ID 要创建的目标镜像名 :[ 标签名 ] 命令： 在容器外执行，记得 docker commit -m=” ifconfig cmd add “ -a=” zzyy “ a69d7c825c4f zzyyubuntu:1.2 启动我们的新镜像并和原来的对比 1.官网是默认下载的Ubuntu没有ifconfig命令 2.我们自己commit构建的新镜像，新增加了ifconfig功能，可以成功使用。 curl验证私服库上有什么镜像 curl -XGET http://192.168.111.162:5000/v2/_catalog 可以看到，目前私服库没有任何镜像上传过。。。。。。 将新镜像zzyyubuntu:1.2修改符合私服规范的Tag 12345按照公式： docker tag 镜像:Tag Host:Port/Repository:Tag 自己host主机IP地址，填写同学你们自己的，不要粘贴错误，O(∩_∩)O 使用命令 docker tag 将zzyyubuntu:1.2 这个镜像修改为192.168.111.162:5000/zzyyubuntu:1.2 docker tag zzyyubuntu:1.2 192.168.111.162:5000/zzyyubuntu:1.2 修改配置文件使之支持http 1234别无脑照着复制，registry-mirrors 配置的是国内阿里提供的镜像加速地址，不用加速的话访问官网的会很慢。2个配置中间有个逗号 &#x27;,&#x27;别漏了 ，这个配置是json格式的。 2个配置中间有个逗号 &#x27;,&#x27;别漏了 ，这个配置是json格式的。 2个配置中间有个逗号 &#x27;,&#x27;别漏了 ，这个配置是json格式的。 vim命令新增如下红色内容：vim /etc/docker/daemon.json 1234&#123; &quot;registry-mirrors&quot;: [&quot;https://aa25jngu.mirror.aliyuncs.com&quot;] , &quot;insecure-registries&quot;: [&quot;192.168.111.162:5000&quot;] &#125; 上述理由：docker默认不允许http方式推送镜像，通过配置选项来取消这个限制。====&gt; 修改完后如果不生效，建议重启docker push推送到私服库 1docker push 192.168.111.162:5000/zzyyubuntu:1.2 curl验证私服库上有什么镜像2 curl -XGET http://192.168.111.162:5000/v2/_catalog pull到本地并运行 1docker pull 192.168.111.162:5000/zzyyubuntu:1.2 docker run -it 镜像ID /bin/bash 七、Docker容器数据卷7.1 坑：容器卷记得加入123456789--privileged=true# 原因 Docker挂载主机目录访问 如果出现cannot open directory .: Permission denied 解决办法：在挂载目录后多加一个--privileged=true参数即可 如果是CentOS7安全模块会比之前系统版本加强，不安全的会先禁止，所以目录挂载的情况被默认为不安全的行为， 在SELinux里面挂载目录被禁止掉了额，如果要开启，我们一般使用--privileged=true命令，扩大容器的权限解决挂载目录没有权限的问题，也即 使用该参数，container内的root拥有真正的root权限，否则，container内的root只是外部的一个普通用户权限。 7.2 回顾下上一将的知识点，参数V还记得蓝色框框中的内容嘛 7.3 是什么1234一句话：有点类似我们Redis里面的rdb和aof文件将docker容器内的数据保存进宿主机的磁盘中运行一个带有容器卷存储功能的容器实例docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录 镜像名 7.4 能干什么12345678910将运用与运行的环境打包镜像，run后形成容器实例运行 ，但是我们对数据的要求希望是 持久化的 Docker容器产生的数据，如果不备份，那么当容器实例删除后，容器内的数据自然也就没有了。 为了能保存数据在docker中我们使用卷。 特点： 1：数据卷可在容器之间共享或重用数据 2：卷中的更改可以直接实时生效，爽 3：数据卷中的更改不会包含在镜像的更新中 4：数据卷的生命周期一直持续到没有容器使用它为止 7.5 数据卷案例 7.5.1 宿主vs容器之间映射添加容器卷直接命令添加 123&gt;公式：docker run -it -v /宿主机目录:/容器内目录&gt;ubuntu /bin/bash&gt;docker run -it --name myu3 --privileged=true -v /tmp/myHostData:/tmp/myDockerData ubuntu /bin/bash 查看数据卷是否挂成功 1&gt;docker inspect 容器ID 容器和宿主机之间数据共享 123&gt;1. docker修改，主机同步获得 &gt;2. 主机修改，docker同步获得 &gt;3. docker容器stop，主机修改，docker容器重启看数据是否同步。 7.5.2 读写规则映射添加说明读写(默认) 12docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录:rw 镜像名默认同上案例，默认就是rw 默认同上案例，默认就是rw 只读 容器实例内部被限制，只能读取不能写 123/容器目录:ro 镜像名 就能完成功能，此时容器自己只能读取不能写 ro = read only 此时如果宿主机写入内容，可以同步给容器内，容器可以读取到。 1docker run -it --privileged=true -v /宿主机绝对路径目录:/容器内目录:ro 镜像名 7.5.3 卷的集成和共享容器1完成和宿主机的映射 1&gt;docker run -it --privileged=true -v /mydocker/u:/tmp --name u1 ubuntu 容器2集成容器1的卷规则 1&gt;docker run -it --privileged=true --volumes-from 父类 --name u2 ubuntu 八、Docker常规安装简介8.1 总体步骤12345671. 搜索镜像2. 拉去镜像3. 查看镜像4. 查看镜像5. 启动镜像 服务端口映射6. 停止容器 8.2 安装tomcat 1、docker hub 上面查找tomcat镜像 12# 命令docker search tomcat 2、从docker hub 上拉去tomcat镜像到本地 12# 命令docker pull tomcat 3、docker images 查看是否有拉去到tomcat 12# 命令docker images tomcat 4、使用tomcat镜像创建容器实例（也叫运行镜像） 12345678910111213# 命令docker run -it -p 8080:8080 tomcat-p 小写，主机端口:docker容器端口-P 大写，随机分配端口i:交互t:终端d:后台 5、访问tomcat首页 1234567可能出现404 的情况解决1、可能没有映射端口或者没有关闭防火墙2、把webapps.dist 目录换成webapps 先成功启动tomcat 查看webapps文件夹查看为空 6、免修改版说明 docker pull billygoo/tomcat8-jdk8 Docker run -d -p 8080:8080 –name mytomcat8 billygoo/tomcat8-djk8 8.3 安装mysql 1、docker hub上面查找mysql镜像 12# 命令docker search mysql 2、从docker hub上（阿里云加速器）拉去mysql镜像到本地标签为5.7 12# 命令docker pull mysql:5.7 3、使用mysql5.7 镜像创建容器（也叫运行镜像） 12345678910# 1、命令出处，哪里来的地址：https://hub.docker.com/_/mysql# 2、简单版docker run -p 3306:3306 -e MYSQL_ROOT_PASSWORD=123456 -d mysql:5.7 docker psdocker exec -it 容器ID /bin/bashmysql -uroot -p 1# 4、 建库建表插入数据 1234567891011外部Win10也来连接运行在dokcer上的mysql容器实例服务【问题】插入中文数据试试，为什么报错？ docker 上默认字符集编码隐患docker里面的mysql容器实例查看，内容如下： SHOW VARIABLES LIKE &#x27;character%&#x27; 删除容器后，里面的mysql数据如何办容器实例一删除，你还有什么？删容器到跑路。。。。。？ 【实战版】 123456789101112131415161718192021222324#1、新建mysql容器实例docker run -d -p 3306:3306 --privileged=true -v /zzyyuse/mysql/log:/var/log/mysql -v /zzyyuse/mysql/data:/var/lib/mysql -v /zzyyuse/mysql/conf:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=123456 --name mysql mysql:5.7 #2、新建my.cnf 通过容器卷同步给MySQL容器实例[client]default_character_set=utf8 [mysqld] collation_server = utf8_general_ci character_set_server = utf8 #3、重新启动mysql容器实例在重新进入并查看字符编码docker restart mysqldocker exec -it mysql_bashshow variables like &#x27;character%&#x27;;#4、再新建库新建表再插入中文测试完全正常#5、结论之前的DB 无效 修改字符集操作+重启mysql容器实例 之后的DB 有效，需要新建 结论： docker安装完MySQL并run出容器后，建议请先修改完字符集编码后再新建mysql库-表-插数据 #6、假如将当前容器实例删除，再重新来一次，之前建的db01实例还有吗？trytry 8.4 安装redis 1、从docker hub上（阿里云加速器）拉去redis镜像到本地标签6.0.8 1234&gt;# 拉去镜像&gt;docker pull redis:6.0.8&gt;# 查看镜像&gt;docker images 2、入门命令 12345&gt;# 启动命令&gt;docker run -d -p 6379:6379 redis:6.0.8&gt;# docker ps&gt;# 后台启动&gt;docker exec -it CONTAINER ID /bin/bash 3、命令提醒：容器卷记得加入 –privileged=true 12&gt;Docker挂载主机目录Docker访问出现cannot open directory .: Permission denied &gt;解决办法：在挂载目录后多加一个--privileged=true参数即可 4、在CentOS宿主机下新建目录/app/redis 12&gt;# 新建目录&gt;mkdir -p /app/redis 5、将一个redis.conf文件模板拷贝进 /app/redis目录下 12345&gt;mkdir -p /app/redis&gt;cp /myredis/redis.conf /app/redis/&gt;cp /app/redis 6、/app/redis 目录下修改redis.conf 12345678910111213&gt;# 修改redis.conf文件 &gt;/app/redis目录下修改redis.conf文件 &gt;开启redis验证 可选 &gt;requirepass 123 &gt;允许redis外地连接 必须 &gt;注释掉 # bind 127.0.0.1 &gt;# 注释daemonize no&gt;daemonize no &gt;将daemonize yes注释起来或者 daemonize no设置，因为该配置和docker run中-d参数冲突，会导致容器一直启动失败&gt;# 开启redis数据持久化&gt;appendonly yes 可选 7、使用redis6.0.8 镜像创建容器(也叫运行镜像) 1&gt;docker run -p 6379:6379 --name myr3 --privileged=true -v /app/redis/redis.conf:/etc/redis/redis.conf -v /app/redis/data:/data -d redis:6.0.8 redis-server /etc/redis/redis.conf 8、测试redis-cli连接上 docker exec -it 运行着Rediis服务的容器ID redis-cli 9、请证明docker启动使用了我们自己指定的配置文件 【修改前】 【修改后】 10、测试redis-cli连接上来第2次 原视频地址 原文章地址","categories":[],"tags":[],"author":"lwy"},{"title":"redis学习笔记","slug":"redis学习笔记","date":"2022-02-13T06:51:52.000Z","updated":"2022-02-13T13:49:59.606Z","comments":true,"path":"2022/02/13/redis学习笔记/","link":"","permalink":"https://lwy0518.github.io/2022/02/13/redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"目录","text":"目录 Redis学习笔记一 、Redis简介1.1 什么是RedisRedis 是完全开源免费的，遵守BSD协议，是一个高性能(NOSQL)的key-value数据库，Redis是一个开源的使用ANSI C语言编写、支持网络、可基于内存可持久化的日志型，Key-Value数据库，并提供多种语言的API。 123BSD是&quot;Berkeley Software Distribution&quot;的缩写，意思是“伯克利软件发型版本”。BSD开源协议是一个给予使用者很大自由的协议。可以自由的使用，修改源代码，也可以将修改后的代码作为开源或者专有软件在发布。BSD代码鼓励代码共享，但需要尊重代码作者的著作权。BSD由于允许使用者修改和重新发布代码，也允许使用或在BSD代码上开发商业软件发布和销售，一次是对商业集成很友好的协议。 1.2 NoSQL1NoSQL,泛指非关系型的数据库，NoSQL即Not-only SQL,它可以作为关系型数据库的良好补充。随着互联网web2.0网站的兴起，非关系型的数据库现在成为了一个及其热门的新领域，非关系型数据库产品的发展非常迅速。 传统数据库暴露很多难以克服的问题，如下问题： 123451、High performance - 对数据库高并发读写的需求。2、Huge Storage - 对海量数据的高效存储和访问的需求。3、High Scalability &amp;&amp; High Availability - 对数据库的高可扩展性和高课用性的需求。 1.3 NoSQL的类别键值（Key-Value）存储数据库 12345678 这一类数据库主要会使用到一个哈希表，这个表中有一个特定的键和一个指针指向特定的数据。key/value模型对于IT系统涞水的优势在于简单，已部署。但是如果DBA只对部分值进行查询或者更新的时候，key/value显示的效率低下。相关产品 ： Redis，Tokyo Cabinet典型应用 ： 内存缓存，主要用于处理大量数据的高访问负载。数据模型 ： 一系列键值对优势 ： 快速查询劣势 ： 存储的数据缺少结构化 列存储数据库 123456这部分数据库通常是用来对分布式存储的海量数据。键仍然存在，但是他们的特点是指向了多个列。这些列是由列家族来安排的。相关产品 : HBase 、Riak典型应用 ： 分布式的文件系统数据模型 ： 以列簇式存储，将同一列数据存在一起优势 ： 查找速度快，可扩展性强，更容易进行分布式扩展劣势 ： 功能相对于局限 文档型数据库 123456文档型数据库：该类型的数据库模型是版本化的文档，半结构化的文档一特定的格式存储，比如JSON。文档数据库可以看做键值数据库的升级版，允许之间嵌套键值。而且文档型数据库比键值数据库的查询效率更高。相关产品：MOngoDB典型应用 ： web应用数据模型 ： 一系列键值对优势 ： 数据结构要求不严格劣势 ： 查询性能不高，而且缺乏统一的查询语言 1.4 总结：NoSQL 数据库在一下的这几种情况下比较适用 ： 1、数据模型比较简单； 2、需要灵活更前的IT系统； 3、对数据库性能要求较高； 4、不需要高度的数据一致性； 5、对于给定key，比较容易映射复杂的环境； 1.5 Redis 描述Redis是完全开源免费的，遵守BSD协议，是一个高性能(NoSQL)的（key-value）数据库，Redis是一个开源的使用ANSI C语言编写，支持网络，可基于内存亦可持久化的日志型，Key-Value数据库，并提供多种语言的API。 1.6 Redis的特点 性能极高 - Redis读写的熟读110000次/s，写的速度是81000次/s。 丰富的数据类型 - Redis支持的类型String， Hash 、List 、Set 及 Ordered Set数据类型操作。 原子性 - Redis的所有操作都是原子性的，意思就是要么成功，要么失败。单个操作时原子性的。多个操作也支持事务，即原子性，通过MULTI和EXEC指令包起来。 丰富的特性 - Redis还支持publis/subscribe，通知，key过期等等特性。 高速读写 ，redis使用自己实现的分离器，代码量很短，没有使用lock(MySQL),因此效率非常高。 Redis是一个简单的，高效的，分布式的，基于内存的缓存工具。 架设好服务器后，通过网络连接(类似数据库)，提供Key-Value缓存服务。 简单，是Redis突出的特色。 简单可以保证核心功能的稳定和优异。 1.7 Redis的应用场景可以作为数据库，缓存，热点数据(经常别查询，但是不经常被修改或者删除的数据)和消息中间件等大部分功能。 Redis常用的场景示例如下： 123456789101112131415161、缓存 缓存现在几乎是所有大中型网站都在用的必杀技，合理利用缓存提升网站的访问速度，还能大大降低数据库的访问压力。Redis提供了键过期功能，也提供了灵活的键淘汰策略，所以，现在Redis用在缓存的场合非常多。2、排行榜 Redis提供的有序集合数据类结构能够实现葛洪复杂的排行榜应用。3、计数器 什么是计数器，，视频网站的播放量等等，每次浏览+1，并发量高时如果每次都请求数据库操作无疑是中挑战和压力。Redis提供的incr命令来实现计数器功能，内存操作，性能非常好，非常是用于这些技术场景。4、分布式会话 集群模式下，在应用不多的情况下一般使用容日自带的session复制功能就能够满足，当应用相对复杂的系统中，一般都会搭建Redis等内存数据库为中心的session服务，session不在由容器管理，而是有session服务及内存数据管理。5、分布式锁 在很多互联网公司中都是用来分布式技术，分布式技术带来的技术挑战是对同一个资源的并发访问，如全局ID，减库存，秒杀等场景，并发量不发的场景可以使用数据库的悲观锁，乐观锁来实现，但是在并发高的场合中，利用数据库锁来控制资源的并发访问是不太理想的，大大影响了数据库的性能。可以利用Redis的setnx功能来编写分布式的锁，如果设置返回1，说明获取所成功，否则获取锁失败，实际应用中药考虑的细节要更多。6、社交网络 点赞、踩、关注/被关注，共同好友等是社交网站的基本功能，社交网站的访问量通常老说比较大，而且传统的关系数据库不适合这种类型的数据，Redis提供的哈希，集合等数据结构能很方便的实现这些功能。7、最新列表 Redis列表结构，LPUSH可以在列表头部插入一个内容ID作为关键字，LTRIM可以用来限制列表的数量，这样列表永远为N个ID。无需查询最新的列表，直接根据ID 去到对应的内容也即可。8、消息系统 消息对队列是网站比用中间件，如ActiveMQ，RabbitMQ，Kafaka等流行的消息队列中间件，主要用于业务解耦，流量削峰及异步处理试试性低的业务。Redis提供了发布/订阅及阻塞队列功能，能实现一个简单的消息队列系统。另外，这个不能喝专业的消息中间件相比。 1.8 Redis总结优势 性能极高 - Redis读写的熟读110000次/s，写的速度是81000次/s。 丰富的数据类型 - Redis支持的类型String， Hash 、List 、Set 及 Ordered Set数据类型操作。 原子性 - Redis的所有操作都是原子性的，意思就是要么成功，要么失败。单个操作时原子性的。多个操作也支持事务，即原子性，通过MULTI和EXEC指令包起来。 丰富的特性 - Redis还支持publis/subscribe，通知，key过期等等特性。 高速读写 ，redis使用自己实现的分离器，代码量很短，没有使用lock(MySQL),因此效率非常高。 缺点 持久化。 Redis直接将数据存储到内存中，要将数据保存到磁盘上，Redis可以使用两种方式实现持久化过程。定时快照(snapshot)：每个一端时间将整个数据库写到磁盘上，每次均是写全部数据，代价非常高。第二种方式基于语句追加（aof）：只追踪变化的数据，但是追加的log可能过大，同时所有的操作均重新执行一遍，回复速度慢。 耗内存 、占用内存过高。 二、Redis安装2.1 Redis官网官方网站 ： https://redis.io/ 官方下载 ： https://redis.io/download 可以根据需要下载不同版本 2.2 Redis 安装Redis是C语言开发，安装Redis需要先将官网下载的源码进行编译。编译依赖gcc环境，如果没有gcc环境，需要安装gcc 2.3 安装gccgcc的安装很简单，首先要确保root登录，其次就是Linux要能连外网 1yum -y install gcc automake autoconf libtool make 注意： 运行yum是出现/var/run/yum.pid已被锁定，PID为xxxx的另外一个程序正在运行的问题解决。 1rm -f /var/run/yum.pid 2.4 安装Redis下载redis二进制安装包 1wget http://download.redis.io/release/redis-6.0.5.tar.gz 解压/apps目录下 12345tar zxvf redis-6.0.5.tar.gz -C /apps#Linux 中剪切命令mv redis-6.0.5.tar.gz 安装包#Linux中复制命令: cp Files pathcp redis-6.0.5.tar.gz /root/apps 进入redis中使用make命令进行编译 1[root@centos redis-5.0.8]# make MALLOC=libc 12345678910 LINK redis-cli CC redis-benchmark.o LINK redis-benchmark INSTALL redis-check-rdb INSTALL redis-check-aofHint: It&#x27;s a good idea to run &#x27;make test&#x27; ;)make[1]: 离开目录“/root/apps/redis-5.0.8/src”[root@centos redis-5.0.8]# ll 安装成功如上 2.5 安装到指定的位置1make PREFIX=/root/apps/redis install （安装编译后的文件）安装到指定目录； 注意：PREFIX必须为大写，同时会自动为我们创建redis目录，并将结果安装此目录； 三 、Redis启动3.1 启动Redis服务端，进入到Redis的安装目录1cd /usr/local/redis 3.2 执行命令1./bin/redis-server 3.3 Redis的客户端进行启动1./bin/redis-cli 3.4 启动Redis客户端命令语法：1redis-cli -h IP地址 -p 端口 //默认IP本机 端口号6379 3.5 检测是否服务端启动启动redis客户端，打开终端并输入命令redis-cli。该命令连接本地的redis服务。 1234127.0.0.1:6379&gt; 127.0.0.1:6379&gt; pingPONG127.0.0.1:6379&gt; 在以上实例中我们连接到本地的redis服务并执行PING 命令，该命令用于检测redis服务是否启动 3.6 检查redis的进程123456#执行 ps -ef | grep -i redis 来查看进程ps -ef | grep -i redisroot 10050 5728 0 23:03 pts/0 00:00:03 ./bin/redis-server *:6379root 10077 10056 0 23:10 pts/1 00:00:00 ./bin/redis-cliroot 10100 10081 0 23:22 pts/2 00:00:00 grep --color=auto -i redis[root@centos ~]# 四、Redis配置详细Redis默认定义了很多默认配置。但在实际开发中，一般我们都会通过手动配置完成。回到安装目录下找到解压文件中的redis.conf。 Redis的配置文件位于Redis安装目录下，文件名称为redis.conf 4.1 配置Redis命令：解压目录下的redis.conf配置文件复制到安装文件的目录下 123456#把编译的redis.conf文件放 ，安装的redis文件目录下[root@centos redis-5.0.8]# pwd/root/apps/redis-5.0.8[root@centos redis-5.0.8]# cp redis.conf /root/apps/redis[root@centos redis-5.0.8]# cd ..[root@centos apps]# ll 4.2 Redis.conf12345678910111213141516171819202122232425261、Redis默认不是以守护进程的方式运行，可以通过该配置项修改，使用yes启用守护进程 daemonize no2、当Redis以守护进程方式运行时，Redis默认会把pid写入/var/run/redis.pid文件，可以通过pidfile指定 pidfile /var/run/redis.pid3、指定Redis监听端口，默认端口为6379；’ port 63794、绑定的主机地址 bind 127.0.0.15、当客户端限制多长时间后关闭连接，如果指定为0，表示关闭该功能 timeout 3006、指定日志记录几倍，Redis总共支持四个级别：debug，verbose，notice，warning，默认为verbose loglevel verbos7、日志记录方式，默认为标准输出，如果配置Redis为守护进程方式运行，而这里又配置日志记录方式标准输出，则日志将会发送给/dev/null logfile stdout8、设置数据库的数量，默认数据库为0，可以使用SELECT&lt;dbid&gt;命令在连接上指定数据库id databases 169、指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合 save&lt;seconds&gt;&lt;changes&gt; Redis默认配置文件中提供了三个条件 save 900 1 save 300 10 save 60 10000 分别表示900秒(15分钟)内有1个更改，300秒(5分钟)内有10个更改以及60秒内有10000个更改。10、指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF(压缩算法)压缩，如果为了节省CPU时间，可以关闭该选项，但会导致数据库文件变的巨大 rdbcompression yes 中间10个 123456789101112131415161718192021222311、指定本地数据库文件名，默认为dump.rdb dbfilename dump.rdb12、指定本地数据库存放目录 dir ./13、设置当本机为slav服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步slaveof&lt;masterip&gt; &lt;masterport&gt;14、当master服务设置了密码保护时，slav服务连接master的密码 masterauth&lt;master-password&gt;15、设置Redis连接密码，如果配置了连接密码，客户端在连接Redis是需要通过AUTH &lt;password&gt;命令提供密码，默认关闭 requirepass foobared16、设置同一时间最大客户端连接数，默认是无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，如果设置maxclients 0，表示不作限制。当客户端连接数到达限制是，Redis会关闭新的连接并向客户端返回max number of clients reached错误信息 maxclients 12817、指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key，档次方法处理后，仍然达最大内存设置，将无法再进行写入操作，但仍然可以静心读取操作。Rdis新的vm机制，会把key存放内存，Value会存放在swap区 maxmemory &lt;bytes&gt;18、指定是否每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一端时间内的数据丢失。因为 redis 本省同步数据文件是按上面save条件来同步的，所有的数据会在一端时间内只存在于内存中。默认为no appendonly no19、指定更新日志文件名，默认为appendonly.aof appendfulename appendonly.aof20、指定更新日志条件，共有3个可选值： no: 表示等操作系统进行数据缓存同步到磁盘(快) always: 表示每次更新操作后活动调用fsync()将数据写到磁盘(慢，安全) everysec: 表示每秒同步一个(折中，默认值) appendfsync everysec 1234567maxmemory-policy noeviction # 内存达到上限之后的处理策略1、volatile-lru ： 只对设置了过期时间的key进行LRU（默认值）2、allkeys-lru ： 产出lru算法的key3、volatile-random ： 随机删除即将过期key4、allkey -random : 随机删除5、volatile-ttl : 删除即将过期的6、noeviction ： 永不过期，返回错误 结尾10个 12345678910111213141516171819202121、指定是否启用虚拟内存机制，默认为no，简单的介绍一下，vm机制将数据分页存放，有Redis将访问量较少的页即冷数据swap到磁盘上，访问多的页面由磁盘自动换出到内存中(在后面的文章会仔细分析Redis的vm机制) vm-enabled no22、虚拟内存文件路径，默认值为/tmp/redis.swap，不可多个Redis实例共享 vm-swap-file /tmp/redis.swap23、将所有大于vm-max-memory的数据存入虚拟内存，无论vm-max-memory设置多小，所有索引数据都是内存存储的(Redis的索引数据 就是keys)，也就是说，当vm-max-memory设置为0的时候，其实是所有value都存在于磁盘。默认值为0 vm-page-size 3224、Redis swap文件分成了很多的page，一个对象可以保存咱几多个page上面，但一个page上不能被多个对象共享，vm-page-size是要根据存储的 数据大小来设定的，作者建议如果村粗很多小对象，page大小最好设置为32或者64bytes；如果存储很大大对象，则可以使用更大的page，如果不确定，就是用默认值 vm-page-size 3225、设置swap文件中的page数量，由于页表(一种表示页面空闲或是欧诺个的bitmap)是放在内存中的，在磁盘上每8个pages将消耗1byte的内存 vm-pages 13421772826、设置访问swap文件的线程数，最好不要超过机器的核数，如果设置为0，那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为 4 vm-max-threads 427、设置在向客户端应答时，是否把较小的包含并未一个包发送，默认为开启 glueoutputbuf yes28、指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法 hash-max-zipmap-entries 64 hash-max-zipmap-value 51229、指定是否激活重置哈希。默认为开启(后面在介绍Redis的哈希算法时具体介绍) activerehasing yes30、指定包含其他的配置文件，可以在同一主机上多个redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件 include /path/to/local.conf 4.2、内存中的维护策略redis作为优秀的中间缓存件，时常会存储大量的数据，即使采取了集群部署来动态扩容，也应该即使的整理内存，维持系统性能。 4.2.1 在redis中有两种解决方案 为数据设置超时时间 1234567//设置过期时间expire key time(以秒为单位)--这是最常用的方式setex(String key, int seconds, String value) --字符串独有的方式1、除了字符串自己独有设置过期时间的方法外，其他方法都需要依靠expire方法来设置时间2、如果没有设置时间，那缓存就是永不过期3、如果设置了过期时间，之后又想让缓存永不过期没使用persist key 采用LRU算法动态将不用的数据删除 123内存管理的一种页面置换算法，对于在内存中但又不用的数据块(内存块)叫做LRU，操作系统会根据哪些数据属于LRU而将其移除内存而腾出空间来加载另外的数据。 1.volatile-lru：设定超时时间的数据中，删除最不常使用的数据 2.allkeys-lru：查询所有的key只能怪最近最不常使用的数据进行删除，这是应用最广泛的策略。 3.volatile-random：在已经设定了超时的数据中随机删除。 4.allkeys-random：查询所有的key，之后随机删除。 5.volatile-ttl：查询全部设定超时时间的数据，之后排序，将马上要过期的数据进行删除操作。 6.noeviction：如果设置为该属性，则不会进行删除操作，如果内存溢出则报错返回。 7.volatile-lfu：从所有配置了过去时间的键中驱逐使用频率最少的键 8.allkeys-lfu：从所有键中驱逐使用频率最少的键 4.2.2 自定义配置redis进入对应的安装目录： 1/root/apps/redis 修改redis.conf配置文件 vim redis.conf（进入命令模式 通过/内容 查找相应字符串） 123daemonize no 修改为 daemonize yes 守护进程启动bind 127.0.0.1 注释掉 允许除本机 外的机器访问redis服务requirepass 设置密码 设定数据库密码 (保证服务安全/有些情况下不设定密码是无法进行远程连接访问的) Redis采用的是单进程多线程的模式。当redis.conf中选项daemonize设置成为yes时，代表开启守护进程模式。在该模式下，redis会在后台运行，并将进程pid号写入值redis.conf选项pidfile设置的文件中，此时redis将一直运行，除非手动kill该进程。但当daemonize选项设置为no时，当前界面将进入redis的命令行界面，exit强制退出或者关闭连接工具（putty,xshell等）都会呆滞redis进程退出。 服务端开发的大部分应用都是采用后台运行的模式 requirepass设置密码。因为redis速度相当快，所以一台比较好的服务器下，一个外部用户在一秒内可以进行15w密码尝试，这意味你需要设定非常强大的密码来防止暴力破解。 可以通过redis的配置文件设置密码参数，这样客户端连接大redis服务就需要密码验证，这样可以让你的redis服务更加安全。 五 、Redis启动5.1 Redis以守护进程服务端进行启动123456// 使用该命令进行启动：【./bin/redis-server ./redis.conf 】[root@centos redis]# ./bin/redis-server ./redis.conf 11450:C 05 Jul 2020 12:23:34.257 # oO0OoO0OoO0Oo Redis is starting oO0OoO0OoO0Oo11450:C 05 Jul 2020 12:23:34.257 # Redis version=5.0.8, bits=64, commit=00000000, modified=0, pid=11450, just started11450:C 05 Jul 2020 12:23:34.257 # Configuration loaded[root@centos redis]# 5.2 Redis客户端进行启动：12345//Redis 客户端启动命令：./redis-cli[root@centos bin]# ./redis-cli 127.0.0.1:6379&gt; keys *1) &quot;name&quot;127.0.0.1:6379&gt; 六、Redis关闭6.1、第一种关闭方式：(断电、非正常关闭，容易数据丢失) 查询不到redis进程id 1PID ps -ef | grep -i redis kill 查询的id进行强制关闭 1kill -9 PID 6.2、第二种关闭方式(正常关闭，数据保存) 关闭redis服务，通过客户端进行shutdown 如果redis设置了密码，需要先在客户端通过密码登录，在进行shutdown即可关闭服务端 12345// 在客户端使用【shutdown】命令关闭Redis服务端127.0.0.1:6379&gt; SHUTDOWNnot connected&gt; yCould not connect to Redis at 127.0.0.1:6379: Connection refusednot connected&gt; shutdown 七、远程连接7.1 Redis远程连接比较流行的软件：RedisDesktoManager默认不允许远程连接，需要修改一下信息才可以进行修改， 12bind 127.0.0.1 注释掉 允许除本机以外的机器访问Redis服务requirepass 设置密码 设定数据库密码(有些情况系不设定密码是无法进行远程连接访问的) 7.2 Redis使用密码登录123456// Redis客户端使用密码进行登录 【./bin/redis-cli -a redis】[root@centos redis]# ./bin/redis-cli -a redisWarning: Using a password with &#x27;-a&#x27; or &#x27;-u&#x27; option on the command line interface may not be safe.127.0.0.1:6379&gt; keys *1) &quot;name&quot;127.0.0.1:6379&gt; Centos 防火墙端口开放8080端口（如下命令只针对Centos7以上） 查看已经开放的端口： 1firewall-cmd --list-ports 开启端口： 1firewall-cmd --zone=public --add-port-6379/tcp --permanent 重启防火墙： 1234firewall-cmd --reload #重启Firewall systemctl stop firewalld.service #停止firewall systemctl disable firewalld.service #禁止firewall 开机启动 八 、Docker安装Redis8.1 搜索redis1docker search redis 8.2 下载镜像1docker pull redis：4.0.1 8.3 创建并运行容器1docker run -d --name redis6379 -p 6379:6379 redis:4.0.1 --requirepass &quot;redis&quot; 8.4 测试进入Redis进入客户端使用redis镜像执行redis-cli命令连接到刚启动的容器 九、Redis常用命令Redis 命令用于在redis服务上执行操作。要在redis服务上执行命令需要一个redis客户端。 Redis 客户端在我们之前下载的Redis的安装包中。 **Redis支持的物种数据类型 ：string(字符串)，hash(哈希)，list(列表)，set(集合)及zset（sorted set : 有序集合）等 9.1 常用命令key管理1234567891011121314151617181、 keys * :返回满足的所有键，可以模糊匹配，比如 keys abc* ：表示以 abc 开头的 key2、 exists key ： 是否存在指定的key ，存在返回1.不存在返回03、 expire key second ：设置某个key的过期时间 时间为妙4、 del key : 删除某个key5、 ttl key ：查看剩余时间，当key不存在是，返回-2；存在但没有设置剩余生存时间时，返回 -1，否 则，以秒为单位，返回key 的剩余生存时间。6、 persist key ：取消过去时间7、 PEXPIRE key millisseconds 修改key 的过期时间为毫秒8、 select ： 选择数据库 数据库为0-15（默认一共16个数据库）9、 设计成多个数据库实际上是为了数据库安全和备份10、 move key dbindex ： 将当前数据中的key转移到其他数据库11、 randomkey ： 随机返回一个key12、 rename key key2 : 种命名key13、 echo ： 打印命令14、 dbsize : 查看数据库的key数量15、 info : 查看数据库信息16、 config get * 实时存储收到的请求，返回相关的配置17、 flushdb ： 清除当前数据库18、 flushall ： 清空所有数据库 9.2 DEL key1该命令用于在key存在时删除key。 9.3 EXISTS key1检查给定key是否存在。 9.4 EXPIRE key seconds1为给定key设置过期时间(以秒计) 9.5 PEXPIRE key milliseconds1设置key的过期时间以毫秒计 9.6 TTL key1以秒为单位，返回给定key的剩余生存时间(TTL, time to live) 9.7 PTTL key1以秒为单位，返回 key 的剩余生存时间 9.8 KEYS pattern12345查找所有服务给定模式(pattern)的key。keys 通配符 获取所有与pattern匹配的key，返回所有与该匹配 通配符 ： * 代表所有 ? 表示代表一个字符 9.9 RENAME key newkey1修改key的名称 9.10 MOVE key db1将当前数据库的 key 移动到给定的数据库db当中 9.11 TYPE key1返回 key 所存储的值的类型 9.12 应用场景EXPIPER key seconds 12341、限时的优惠活动信息2、网站数据缓存(对于一些需要定时更新的数据，例如:积分排行榜)3、手机验证码4、限制网站访客访问频率(例如：1分钟最多访问10次) 9.13 key的命名建议redis单个key允许存入512M大小 1、key 不要太长，尽量不要超过1024字节，这不仅消耗内存，而且会降低查找的效率 2、key 也不要太短，太短的话，key的可读性会降低 3、在一个项目中，key最好使用提议的命名模式，例如user:12:password 4、key名称区分大小写 十 、Redis数据类型10.1 String 类型String类型是Redis最基本的数据类型，一个键最大能存储512MB。 String 数据结构最贱但的key-value类型，value其不仅是string，也可以是数字，是包含很多种类型的特殊类型， String类型是二进制安全的。意思是redis的string可以包含任何数据。 比如序列化的对象进行存储，比如一张图片进行二进制存储，比如一个简单的字符串，数值等等。 String命令123456789101112131415161718192021222324252627282930313233343536373839404142431、复制语法： SET KEY_NAME VALUE : (说明：多次设置name会覆盖)(Redis SET 命令用于设置给定 key 的值。如果 key 已经存储值，SET 就要写旧值，且无视类型)。2、命令： SETNX key1 value：(not exist) 如果key1不存在，则设置 并返回1。如果key1存在，则不设置并返回0;(解决分布式锁 方案之一，只有在key 不存在时设置 key 的值。setnx (SET if not exits)命令在指定的key不存在时，为key设置指定的值)。 SETEX key1 10 lx :(expired)设置key1的值为lx,过期时间为10秒，10秒后key1清除（key也清除）SETEX key1 10 lx :(expired) 设置key1的值为lx，过期时间为10秒，10秒后key1清除(key 也清除)SETRANG STRING range value : 替换字符串3、取值语法： GET KEY_NAME : Redis GET 命令用于获取指定 key 的值。如果 key 不存在，返回 nil。如果key存储的值不是字符串类型，返回一个错误。 GETRANGE key start end : 用于获取存储在指定key中字符串的子字符串。字符串的截取范围由 start 和 end 两个偏移量来决定(包括 start 和 end 在内) GETBIT key offset ：对 key 所存储的字符串值，获取指定偏移量上的为(bit)；GETTEST语法 ： GETSET KEY_NAME VALUE : GETSET 命令用于设置指定 key 的值，并返回key的旧值。当key不存在是，返回 nil STRLEN key :返回 key 所存储的字符串值的长度4、删除语法：DEL KEY_NAME : 删除指定的key，如果存在，返回数字类型。5、批量写：MSET K1 V1 K2 V2 ... (一次性写入多个值)6、批量读：MGET K1 K2 K37、GETSET NAME VALUE : 一次性设置和读取(返回旧值，写上新值)8、自增/自减： INCR KEY_Name : Incr 命令将key中存储的数组值增1。如果 key 不存在，那么key的值会先被初始化为0，然后在执行INCR操作 自增: INCRBY KEY_Name : 增量值Incrby 命令将key中存储的数字加上指定的增量值 自减: DECR KEY_Name 或 DECYBY KEY_NAME 减值：DECR 命令将key中存储的数字减少1：(注意这些key对应的必须是数字类型字符串，否则会出错。)字符串拼接：APPEND KEY_NAME VALUE:Append 命令用于为指定的key追加至末尾，如果不存在，为其赋值字符串长度 ：STRLEN key ########################## setex (set with expire) #设置过期时间 setnx (set if not exist) #不存在设置 在分布式锁中会常常使用！ 10.2 应用场景 1、String通常用于保存单个字符串或JSON字符串数据 2、因String是二进制安全的，所以你完全可以把一个图片文件的内容作为字符串来存储 3、计数器(常规key-value缓存应用。常规计数：微博数，粉丝数) 12INCR 等指令本身就具有原子操作的特定，所以我们完全可以利用redis的INCR，INCRBY,DECR,DECRBY等指令来实现原子计数的效果。假如，在某种场景下有3个客户端同时读取了mynum的值(值为2)，然后对其同时进行了加1的操作，那么，最后mynum的值一定是5。不少网站都利用redis的这个特性来实现业务上的统计计数需求。 10.3 Hash类型Hash类型是String类型的field和value的映射表，或者说是一个String集合。hash特别适合用于存储对象，相比较而言，将一个对象类型存储在Hash类型要存储在String类型里占用更少的内存空间，并对整个对象的存取。可以看成具有KEY和VALUE的MAP容器，该类型非常适合于存储值对象的信息。 如：uname，upass，age等。该类型的数据仅占用很少的磁盘空间(相比于JSON). Redis 中每一个hash 可以存储 2的32次方 -1 键值对(40 多亿) 10.4 Hash命令常用命令 1234567891011121314151617181920212223一、赋值语法： 1、 HSET KEY FIELD VALUE ： 为指定的KEY,设定FILD/VALUE 2、 HMSET KEY FIELD VALUE [FIELD1，VALUE]... : 同时将多个 field-value(域-值)对设置到哈希表key中。二、取值语法： HGET KEY FIELD :获取存储在HASH中的值，根据FIELD得到VALUE HMGET KEY FIELD [FIELD1] :获取key所有给定字段的值 HGETALL KEY :返回HASH表中所有的字段和值 HKEYS KEY : 获取所有哈希表中的字段 HLEN KEY : 获取哈希表中字段的数量三、删除语法： HDEL KEY FIELD[FIELD2] :删除一个或多个HASH表字段 四、其它语法： HSETNX KEY FIELD VALUE : 只有在字段field 不存在时，设置哈希表字段的值 HINCRBY KEY FIELD INCREMENT :为哈希key中的指定字段的整数值加上增量 increment。 HINCRBYFLOAT KEY FIELD INCREMENT :为哈希表key 中的指定字段的浮点数值加上增量 increment HEXISTS KEY FIELD : 查看哈希表中key中，指定的字段是否存在 10.5 应用场景Hash的应用场景 ：(存储一个用户信息对象数据) 常用于存储一个对象 为什么不用string存储一个对象 hash值最接近关系数据库结构的数据类型，可以将数据库一条记录或程序中一个对象转换成hashmap存放在redis中。 用户ID为查找的key，存储的value用户对象包含姓名，年龄，生日等信息，如果用普通的key/value结构来存储，主要有一下2中村粗方式： 第一种方式将用户ID作为查找key，把其他信息封装成为一个对象以序列化的方式存储，这种方式的却但是，增加了序列化/反序列化的开销，并且在需要修改其中一项信息时，需要把整个对象取回，并且修改操作需要对并发进行保护，引入CAS等复杂问题。 第二种方法是这个用户信息对象有多少成员就存成多少个key-value对儿，用用户ID+对应属性的名称作为唯一标识来取的对应属性的值，虽然省去了序列化开销和并发问题，但是用户ID重复存储，如果存在大量这样的数据，内存浪费还是非常可观的。 10.6 总结：Redis提供的Hash很好的解决了这个问题，Redis的Hash实际内部存储的Value为一个HashMap， 10.6 List类型简介： 11、 List 类型是一个链表结构的集合，其主要功能有push、pop、获取元素等。更详细的说，List类型是一个双端链表的节后，我们可以通过相关的操作进行集合的头部或者尾部添加和删除元素，List的设计是非常简单精巧，即可以最为栈，有可以最为队列，满足绝大多数的需求。 常用命令 123451、赋值语法： LPUSH KEY VALUE1 [VALUE2] :将一个或多个值插入到列表头部（从左侧添加） RPUSH KEY VALUE1 [VALUE2] ：在列表中添加一个或多个值（从有侧添加） LPUSHX KEY VAKUE :将一个值插入到已存在的列表头部。如果列表不在，操作无效 RPUSHX KEY VALUE :一个值插入已经在的列表尾部（最右边）。如果列表不在，操作无效 12342、取值语法： LLEN KEY :获取列表长度 LINDEX KEY INDEX :通过索引获取列表中的元素 LRANGE KEY START STOP :获取列表指定范围内的元素 描述：返回列表中指定区间的元素，区间以偏移量START和END指定。 其中0表示列表的第一个元素，1表示列表的第二个元素，以此类推。。。 也可以使用负数下标，以-1表示列表的最后一个元素，-2表示列表的倒数第二个元素，一次类推。。。 start：页大小（页数-1） stop：（页大小页数）-1 123453、删除语法： LPOP KEY :移除并获取列表的第一个元素（从左侧删除） RPOP KEY :移除列表的最后一个元素，返回值为移除的元素（从右侧删除） BLPOP key1 [key2]timeout :移除并获取列表的第一个元素，如果列表没有元素会阻塞列表知道等待超时或发现可弹出元素为止。 1234、修改语法： LSET KEY INDEX VALUE :通过索引设置列表元素的值 LINSERT KEY BEFORE|AFTER WORIL VALUE :在列表的元素前或者后 插入元素 描述：将值 value 插入到列表key当中，位于值world之前或之后。 高级命令 123456高级语法： RPOPLPUSH source destiation : 移除列表的最后一个元素，并将该元素添加到另外一个列表并返回 示例描述： RPOPLPUSH a1 a2 : a1的最后元素移到a2的左侧 RPOPLPUSH a1 a1 : 循环列表，将最后元素移到最左侧 BRPOPLPUSH source destination timeout :从列表中弹出一个值，将弹出的元素插入到另外一个列表中并返回它；如果列表没有元素会阻塞列表知道等待超时或发现可弹出的元素为止。 List代码案例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334package com.tyx.service.impl;import com.tyx.po.User;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.stereotype.Service;import org.springframework.util.CollectionUtils;import java.io.Serializable;import java.util.Collections;import java.util.List;import java.util.concurrent.TimeUnit;/** * @author papi * @data 2020/7/15 */@Service@Slf4jpublic class UserServiceImpl &#123; @Autowired private RedisTemplate&lt;String, Object&gt; redisTemplate; /** * Redis有什么命令，Jedis有什么方法 * Lettuce-----》RedisTemplate进一步的封装 *RedisTemplate 方法和命令是肯定不一样的 * Redis 和 String类型 * 需求输入一个key * 先判断该key是否存在如果不存在则在mysql中进行查询，写入到redis中。并返回值。 */ public String getRedisValueByKey(String key)&#123; if (redisTemplate.hasKey(key)) &#123; //表示存在值，进行获取 log.info(&quot;-------&gt; redis中查询的数据&quot;); Object o = redisTemplate.opsForValue().get(key); return (String) o; &#125;else &#123; //不存在去mysql中查并且赋值给reids String val = &quot;redis中不存在的key&quot;; log.info(&quot;------&gt;mysql中查询出来的：&quot;+val); redisTemplate.opsForValue().set(key,val); log.info(&quot;------&gt;mysql中查出的数据存入redis中&quot;); return val; &#125; &#125; /** * 测试String类型 * 需求：用户输入一个redis数据。该key的有效期为28小时 */ public void expireStr(String key, String val)&#123; redisTemplate.opsForValue().set(key,val); redisTemplate.expire(key,2,TimeUnit.HOURS); &#125; /** * 根据ID查询用户对象信息 * 先判断redis中是否存在该key * 如果不存在，查询数据库中mysql中的值，并将结果添加到redis中。 * 如果存在，直接将结果在redis查询，并返回。 */ public User getHashKey(String id)&#123; if (redisTemplate.opsForHash().hasKey(&quot;user&quot;,id))&#123; log.info(&quot;-----&gt;查询redis数据库&quot;); return (User) redisTemplate.opsForHash().get(&quot;user&quot;,id); &#125;else &#123; log.info(&quot;-----&gt;查询mysql数据库&quot;); User user = new User(); user.setId(id); user.setAge(18); user.setName(&quot;速速&quot;); /* @param h 用户的实体 @param hk 用户主键id @param hv 整个对象 */ redisTemplate.opsForHash().put(&quot;user&quot;,id,user); return user; &#125; &#125; /** * 将list放入缓存中 * @param key 值 * @param list 键 * @return true 成功 false 失败 */ public boolean lpushAll(String key, List&lt;Object&gt; list)&#123; try &#123; redisTemplate.opsForList().leftPush(key, list); return true; &#125;catch (Exception e)&#123; e.printStackTrace(); return false; &#125; &#125; /** * 将list放入缓存中 * @param key 值 * @param list 键 * @param time 时间（秒） * @return true 成功 false 失败 */ public boolean lpushAll(String key, List&lt;Object&gt; list, long time)&#123; try &#123; redisTemplate.opsForList().leftPushAll(key, list); if (time &gt;0)&#123; redisTemplate.expire(key,time,TimeUnit.SECONDS); &#125; return true; &#125;catch (Exception e)&#123; e.printStackTrace(); return false; &#125; &#125; /** * 在变量左边添加元素。 * @param key * @param obj * @return */ public boolean lpush(String key, Object obj)&#123; try &#123; redisTemplate.opsForList().leftPush(key,obj); return true; &#125;catch (Exception e)&#123; e.printStackTrace(); return false; &#125; &#125; /** * 在变量左边添加元素。 * @param key 键 * @param prvot 中间参数 * @param object 要放的值 * @return 。 */ public boolean lpush(String key, Object prvot,Object object)&#123; try &#123; redisTemplate.opsForList().leftPush(key,prvot,object); return true; &#125;catch (Exception e)&#123; e.printStackTrace(); return false; &#125; &#125; /** * 在变量左边添加元素。 * @param key 键 * @param prvot 中间参数 * @param object 要放的值 * @return 。 */ public boolean rpush(String key, Object prvot,Object object)&#123; try &#123; redisTemplate.opsForList().rightPush(key,prvot,object); return true; &#125;catch (Exception e)&#123; e.printStackTrace(); return false; &#125; &#125; /** * 在变量左边添加元素。 * @param key 键 * @param object 要放的值 * @return 。 */ public boolean rpush(String key, Object object)&#123; try &#123; redisTemplate.opsForList().rightPush(key,object); return true; &#125;catch (Exception e)&#123; e.printStackTrace(); return false; &#125; &#125; /** * 在变量左边添加元素。 * @param key 键 * @param object 要放的值 * @return 。 */ public boolean lpushIfPresent(String key, Object object)&#123; try &#123; redisTemplate.opsForList().leftPushIfPresent(key,object); return true; &#125;catch (Exception e)&#123; e.printStackTrace(); return false; &#125; &#125; /** * 移除集合中的左边第一个集合 * @param key 键 * @return 返回右边第一个值 */ public Object lpop(String key) &#123; return redisTemplate.opsForList().leftPop(key); &#125; /** * 移除集合中的右边的元素，一般用在队列取值 * @param key 键 * @return 返回右边的元素 */ public Object rpop(String key) &#123; return redisTemplate.opsForList().leftPop(key); &#125; /** * 获取指定区间的值 * @param key 键 * @param start 开始位置 * @param end 结束位置 * @return 返回值 */ public List&lt;Object&gt; lrange(String key, long start, long end) &#123; return redisTemplate.opsForList().range(key,start,end); &#125; /** * 返回当前位置上的值。 * @param key 键 * @param index 当前位置 * @return */ public Object lindex(String key, long index) &#123; return redisTemplate.opsForList().index(key,index); &#125; /** * * @param key 键 * @param count 统计 * @param object 移除的对象 * @return */ public long remove(String key,long count, Object object)&#123; return redisTemplate.opsForList().remove(key,count,object); &#125; /** * 获取集合长度 * @param key 键 * @return 长度 */ public long llen(String key)&#123; return redisTemplate.opsForList().size(key); &#125; /** * 在集合的指定位置插入元素，如果指定的位置已有元素，则覆盖，没有则新增，超过集合的下标+n则会报错； * @param key 键 * @param index 位置 * @param value 值 */ public void set(String key,Long index, Object value) &#123; &#125; /** * 截取集合元素，保留成都内地数据 * @param key 键 * @param start 开始位置 * @param end 结束位置 */ public void trim(String key,Long start, Long end) &#123; redisTemplate.opsForList().trim(key,start,end); &#125; /** *出去集合右边的值，同时集合的左边添加一个值 * @param key 键 * @param str 入栈的值 * @return 返回对象 */ public Object rightPopAndleftPush(String key,String str) &#123; return redisTemplate.opsForList().rightPopAndLeftPush(key,str); &#125; /** *出去集合右边的值，同时集合的左边添加一个值,如果超过等待的时间仍然没有元素则退出 * @param key 键 * @param str 左边新增的值 * @param time 超时时间 * @return 返回移除右边的值 */ public Object rightPopAndleftPush(String key,String str,long time) &#123; return redisTemplate.opsForList().rightPopAndLeftPush(key,str,time,TimeUnit.MINUTES); &#125; /** * 删除. * @param keys */ public void del(String ... keys)&#123; if (keys!=null || keys.length &gt; 0)&#123; if (keys.length == 1) &#123; redisTemplate.delete(keys[0]); &#125;else &#123; redisTemplate.delete(CollectionUtils.arrayToList(keys)); &#125; &#125; &#125; /** * 设置过期时间。 * @param key 键。 * @param seconds 过期时间。 */ public void expire(String key,long seconds)&#123; redisTemplate.expire(key,seconds,TimeUnit.SECONDS); &#125;&#125; 10.7 List 的应用场景项目应用于：1、对数据量大的集合数据删除；2、任务队列 1、对数据量大的集合数据删减 列表数据显示，关注列表，粉丝列表，留言评论等.....分页，热点新闻等 利用LRANG还可以很方便的实现分页的功能，在博客系统中，每片博文的评论也可以存入一个单独的list中。 2、任务队列 (list 通常用来实现一个消息队列，而且可以却表先后顺序，不必像MySQL那样还需要通过ORDER BY来进行排序) 12345任务队列介绍(生产者和消费者模式：) 在处理web客户端发送的命令请求是，某些操作的执行时间可能会比我们预期的更长一些，通过将待执行任务的相关信息放入队列里面，并在之后队列进行处理，用户可以推迟执行那些需要一段时间才能完成的操作，这种将工作交个任务处理器来执行的做法被称为任务队列（task queue）。 RPOPLPUSH source destination 移除列表的最后一个元素，并将该元素添加到另一个列表并返回 3、List应用场景案例1 12 10.8 Set类型简介 Redis的Set是String类型的无需集合。集合成员是唯一的，这就意味着集合中不能出现重复的数据。Redis中集合是通过哈希表实现的，set是通过hashtable实现的 集合中最大的成员数为2^32 -1,类似于JAVA中的Hashtable集合。 命令 1234567891011121314151617181920211、复制语法： SADD KEY member1 [member2]:向集合添加一个或多个成员 2、取值语法： SCARD KEY :获取集合的成员数 SMEMBERS KEY ：返回集合中的所有成员 SISMEMBER KEY MEMBER :判断member元素是否是集合key的成员(开发中：验证是否存在判断) SRANDMEMBER KEY [COUNT] :返回集合中一个或对个随机数 3、删除语法： SREM key member1 [member2] : 移除集合中一个或多个成员 SPOP key [count] : 移除并返回集合中的一个随机元素 SMOVE source destination member :将member 元素从Source集合移动到destination集合中4、差集语言： SDIFF key1 [key2] :返回给定所有集合的差集 SDIFFSTORE destination key1 [key2] :返回给定所有集合的茶几并存储在destination中5、交集语言： SUNION key1 [key2] : 返回所有给定集合的并集 SUNIONSTORE destination key1 [key2] :所有给定集合的并集存储在 destinatiion集合中 10.9 ZSet类型有序集合(sorted set) 简介 1、Redis有序集合和集合一样也是string类型元素的集合，且不允许重复的成员。 2、不同的是每个元素都会关联一个double类型的分数。redis正是通过分数来为集合中的成员进行从小到大的排序。 3、有序集合的成员是唯一的，但分数（score）却可以重复。 4、集合是通过哈希表实现的。集合中最大的成员数为2^32 -1。Redis的ZSet是有序，且不重复。 （很多时候，我们都将redis中的有序结合叫做zsets，这是因为在redis中，有序集合相关的操作指令都是以z开头的） 命令 1234567891011121314151617181920212223242526272829301、复制语法： ZADD KEY score1 member1 【score2 member2】 ：向有序集合添加一个或多个成员，或者更新已经存在成员的分数2、取值语法： ZCARD key ：获取有序结合的成员数 ZCOUNT key min max :计算在有序结合中指定区间分数的成员数 127.0.0.1:6379&gt; ZADD kim 1 tian (integer) 0 127.0.0.1:6379&gt; zadd kim 2 yuan 3 xing (integer) 2 127.0.0.1:6379&gt; zcount kim 1 2 (integer) 2 127.0.0.1:6379&gt; ZRANK key member :返回有序集合中指定成员的所有 ZRANGE KEY START STOP [WITHSCORES]:通过索引区间返回有序集合成指定区间内的成员(低到高) ZRANGEBYSCORE KEY MIN MAX [WITHSCORES] [LIMIT] :通过分数返回有序集合指定区间内的成员 ZREVRANGE KEY START STOP [WITHSCORES] :返回有序集中是定区间内的成员，通过索引，分数从高到底 ZREVERANGEBYSCORE KEY MAX MIN [WITHSCORES] :返回有序集中指定分数区间的成员，分数从高到低排序删除语法： DEL KEY : 移除集合 ZREM key member [member...] 移除有序集合中的一个或多个成员 ZREMRANGEBYSCORE KEY MIN MAX :移除有序集合中给定的分数区间的所有成员。 ZREMRANGEBYSCORE KEY MIN MAX :移除有序集合中给定的分数区间的所有成员。 ZINCRBY KEY INCREMENT MEMBER :增加member元素的分数increment，返回值是更改后的分数 10.10 HyperLogLog常用命令 1234PFADD key element [element ...] : 添加指定元素到HyperLoglog中PFCOUNT KEY [key ...] :返回给定 HyperLogLog的基数估算值PFMERGE destkey sourcekey [sourcekey ...] :将过个HyperLogLog 合并为一个HyperLoglog 应用场景 基数不大，数据量不大就用不上，会有点大材小用浪费空间 有局限性，就是指能统计基数数量，而没办法去知道具体的内容是什么 123456统计注册 IP 数统计每日访问 IP 数统计页面实时 UV 数统计在线用户数统计用户每天搜索不同词条的个数统计真实文章阅读数 10.11 geospatial 地理位置GEORANDIUSBYMEMBER 找出指定元素周围的其他元素 十一、SpringBoot整合Jedis11.1 简介 我们在使用springboot搭建微服务的时候，在很多时候还是需要redis的高速缓存来缓存一些数据，存储一些高品率访问的数据，如果直接使用redis的话由比较麻烦，在这里，我们使用jedis来实现redis缓存达到高效缓存的目的。 11.2 引入Jedis依赖1234567&lt;!-- https://mvnrepository.com/artifact/redis.clients/jedis --&gt;&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt;&lt;/dependency&gt; 因为SpringBoot内默认引用了jedis版本。 所以我们直接引入jedis依赖无需配置jedis的版本号了。 11.3 application.yml例如 在application.yml中配置如下信息： 十二、SpringBoot2.x中redis使用（lettuce）12java代码操作Redis，需要使用Jedis，也就是redis支持java的第三方类库注意：Jedis2.7以上的版本才支持集群操作 12.1 maven配置新建SpringBoot2.0.3的WEB工程，在MAVEN的pom.xml文件中加入如下依赖 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.3.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.tyx&lt;/groupId&gt; &lt;artifactId&gt;lettuce-demo&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;lettuce-demo&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 默认是lettuce客户端 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- redis依赖common-pool 这个依赖一定要添加 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;2.8.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt; &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 12.2 视频中的代码POM文件 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;parent&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt; &lt;version&gt;2.3.1.RELEASE&lt;/version&gt; &lt;relativePath/&gt; &lt;!-- lookup parent from repository --&gt; &lt;/parent&gt; &lt;groupId&gt;com.tyx&lt;/groupId&gt; &lt;artifactId&gt;lettuce-demo&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;name&gt;lettuce-demo&lt;/name&gt; &lt;description&gt;Demo project for Spring Boot&lt;/description&gt; &lt;properties&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- 默认是lettuce客户端 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt; &lt;/dependency&gt; &lt;!-- redis依赖common-pool 这个依赖一定要添加 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;2.8.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-devtools&lt;/artifactId&gt; &lt;scope&gt;runtime&lt;/scope&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;optional&gt;true&lt;/optional&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-test&lt;/artifactId&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;org.junit.vintage&lt;/groupId&gt; &lt;artifactId&gt;junit-vintage-engine&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-maven-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; RedisConfig 12345678910111213141516171819202122232425262728package com.tyx.config;import org.springframework.cache.annotation.CachingConfigurerSupport;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.data.redis.serializer.GenericJackson2JsonRedisSerializer;import org.springframework.data.redis.serializer.StringRedisSerializer;import org.springframework.stereotype.Component;import java.io.Serializable;/** * @author papi * @data 2020/7/15 */@Configurationpublic class RedisConfig &#123; @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate(LettuceConnectionFactory factory)&#123; RedisTemplate&lt;String, Object&gt; redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setKeySerializer(new StringRedisSerializer()); redisTemplate.setValueSerializer(new GenericJackson2JsonRedisSerializer()); redisTemplate.setConnectionFactory(factory); return redisTemplate; &#125;&#125; User 12345678910111213141516171819202122package com.tyx.po;import lombok.Data;import java.io.Serializable;/** * @author papi * @data 2020/7/15 *//** * Java常用编码规范 * Java规范 */@Datapublic class User implements Serializable &#123; private String name; private int age; private String id;&#125; UserServiceImpl 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879package com.tyx.service.impl;import com.tyx.po.User;import lombok.extern.slf4j.Slf4j;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.stereotype.Service;import java.io.Serializable;import java.util.concurrent.TimeUnit;/** * @author papi * @data 2020/7/15 */@Service@Slf4jpublic class UserServiceImpl &#123; @Autowired private RedisTemplate&lt;String, Object&gt; redisTemplate; /** * Redis有什么命令，Jedis有什么方法 * Lettuce-----》RedisTemplate进一步的封装 *RedisTemplate 方法和命令是肯定不一样的 * Redis 和 String类型 * 需求输入一个key * 先判断该key是否存在如果不存在则在mysql中进行查询，写入到redis中。并返回值。 */ public String getRedisValueByKey(String key)&#123; if (redisTemplate.hasKey(key)) &#123; //表示存在值，进行获取 log.info(&quot;-------&gt; redis中查询的数据&quot;); Object o = redisTemplate.opsForValue().get(key); return (String) o; &#125;else &#123; //不存在去mysql中查并且赋值给reids String val = &quot;redis中不存在的key&quot;; log.info(&quot;------&gt;mysql中查询出来的：&quot;+val); redisTemplate.opsForValue().set(key,val); log.info(&quot;------&gt;mysql中查出的数据存入redis中&quot;); return val; &#125; &#125; /** * 测试String类型 * 需求：用户输入一个redis数据。该key的有效期为28小时 */ public void expireStr(String key, String val)&#123; redisTemplate.opsForValue().set(key,val); redisTemplate.expire(key,2,TimeUnit.HOURS); &#125; /** * 根据ID查询用户对象信息 * 先判断redis中是否存在该key * 如果不存在，查询数据库中mysql中的值，并将结果添加到redis中。 * 如果存在，直接将结果在redis查询，并返回。 */ public User getHashKey(String id)&#123; if (redisTemplate.opsForHash().hasKey(&quot;user&quot;,id))&#123; log.info(&quot;-----&gt;查询redis数据库&quot;); return (User) redisTemplate.opsForHash().get(&quot;user&quot;,id); &#125;else &#123; log.info(&quot;-----&gt;查询mysql数据库&quot;); User user = new User(); user.setId(id); user.setAge(18); user.setName(&quot;速速&quot;); /* @param h 用户的实体 @param hk 用户主键id @param hv 整个对象 */ redisTemplate.opsForHash().put(&quot;user&quot;,id,user); return user; &#125; &#125;&#125; 测试类 1234567891011121314151617181920212223242526272829package com.tyx;import com.tyx.po.User;import com.tyx.service.impl.UserServiceImpl;import lombok.AllArgsConstructor;import org.junit.jupiter.api.Test;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.test.context.SpringBootTest;@SpringBootTestclass LettuceDemoApplicationTests &#123; @Autowired private UserServiceImpl userService; @Test void contextLoads() &#123; &#125; @Test void T1 ()&#123; String tyx = userService.getRedisValueByKey(&quot;tyx&quot;); System.out.println(&quot;返回redis中的值为：&quot; + tyx); &#125; @Test void T2()&#123; User hashKey = userService.getHashKey(&quot;1003&quot;); System.out.println(hashKey); &#125;&#125; 12.3 查看Redis客户端信息原因：把任何数据保存到redis中时，都需要进行序列化，默认使用JdkSerializationRedisSerializer进行数据序列化。所有的key和value还有hashkey和hashvalue的原始字符前，都加了一串字符。 十三、Redis的发布订阅13.1 redis发布订阅简介Redis 发布订阅(pub/sub)是一种消息通信模式：发送者（pub）发送消息，订阅者（sub)接受消息。 Redis客户端可以订阅任意数量的频道 123Redis 发布订阅(pub/sub)是一种消息通信模式：发送者（pub）发送消息，订阅者（sub)接受消息。Redis 客户端可以订阅任意数量的频道。下图展示了频道channel1，以及订阅这个频道的三个客户端---client2，client5和client1之间的关系。 12345//订阅端SUBSCRIBE redischannelReading messages ...(press ctrl-c quit)//发送端PUBLIC redischannel &quot;redis channel&quot; 十四、Redis多数据库14.1 Redis下，数据库是由一个整数索引标识，而不是一个数据库名称。默认情况下，一个客户端连接到数据库0。redis配置问阿金中下面的参数来控制数据库总数： database 16 //（从0开始 1,2,3…15） select 数据库 //数据库的切换 移动数据(将当前key移动另一库) 1move key 名称 数据库 14.2 数据库清空：12flushdb ：清除当前数据库的所有keyflushall :清除整个redis的数据库所有key 十五、Redis事务Redis事务可以一次执行多个命令，（按顺序地串行化执行，执行中不会被其他命令插入，不许加塞） 简介 Redis事务可以一次指定多个命令（允许在一个单独的步骤中执行一组命令），并且带有一下两个中要的保证： 123批量操作在发送EXEC命令前被放入队列缓存。收到EXEC命令后进入事务执行，事务中任意命令执行失败，其余命令依然被执行。在事务执行过程中，其他客户端提交的命令请求不会插入到事务执行命令列中。 Redis会将一个事务中的所有命令序列化，然后按顺序执行 执行中不会被其它命令插入，不许出现加赛行为 常用命令 1234567891011DISCARD : 取消事务，放弃执行事务块内的所有命令。MULTI : 标记一个事务块的开始。EXEC : 执行所有事务块内的命令。UNWATCH: 取消watch命令对所有key的监视。WATCH KEY [KEY ...] :监视一个(或多个)key，如果在事务执行之前这个(或这些)key被其他命令所改动，那么事务将被打断。 一个事务从开始到执行会经历以下三个阶段： 1、开始事务。 2、命令入队。 3、执行事务。 15.1示例 1 MULTI EXEC转账功能，A向B转账50元 一个事务的例子，它先以MULTI开始一个事务，然后将多个命令入队到事务中，最后由EXEC命令触发事务 输入Multi命令开始，输入的命令都会一次进入命令队列中，但不会执行 知道输入Exce后，Redis会将之前的命令队列中的命令一次执行。 15.2 示例 DISCARD放弃队列运行 输入MULTI命令，输入的命令都会依次进入命令队列中，但不会执行。 直到输入Exec后，Redis会将之前的命令队列中的命令依次执行。 命令队列的过程中可以使用命令DISCARD来放弃队列运行。 15.3 示例3事务的错误处理事务的错误处理： 如果执行的某个命令报出了错误，则只有报错的命令不会被执行，而其他的命令都会执行，不会回滚。 15.4 示例4 事务的错误处理事务的错误处理： 队列中的某个命令出现了 报告错误，执行是整个的所有队列都会被取消。 由于之前的错误，事务执行失败 15.5 示例5 事务的watch12WATCH key [key ...]:监视一个(或多个)key，如果在事务执行前这个(或这些)key被其他命令所改动，那么事务将被打断。 需求：某一账户在一事务内进行操作，在提交事务前，另一个进程对该账户进行操作。 15.6 应用场景一组命令必须同时都执行，或者都不执行。 我们想要保证一组命令在执行的过程之中不被其他命令插入。 案例： 秒杀 15.7 Redis事务的总结Redis事务本质：一组命令的集合！一个事务中的所有命令都会被序列化，在事务执行过程中，会按照顺序执行！一次性，顺序性，排他性！执行一些列的命令！ Redis事务没有隔离级别的概念！ 所有的命令在事务中，并没有直接被执行！只有发起执行命令的时候才会执行！Exec Redis单条命令保存原子性，但是事务不保证原子性！ Redis 事务其实是支持原子性的！即使 Redis 不支持事务回滚机制，但是它会检查每一个事务中的命令是否错误。 十六、Redis持久化16.1 什么是Redis 持久化？持久化就是把内存的数据写到磁盘中去，防止府服务宕机内存数据丢失。 Redis提供了两种持久化方式：RDB(默认)和AOF 简介 数据存放于： 内存：高效，断电（关机）内存数据会丢失 硬盘：读写速度慢于内存，断电数据不会丢失 Redis持久化存储支持两种方式：RDB和AOF。RDB一定时间取存储文件，AOF默认每秒去存储历史命令， Redis是支持持久化的内存数据库，也就是说redis需要经常将内存中的数据同步到硬盘来保证持久化。 16.2 RDBRDB是Redis DataBase缩写 Redis是内存数据库，如果不将内存中的数据库状态保存到磁盘中，那么一旦服务器进程退出，服务器中的数据库的状态也会消失。造成数据的丢失，所以redis提供了持久化的功能。 在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是所说的snapshot快照，它恢复是将卡UN关照文件爱你直接读到内存里。 Redis会单独创建（fock）一个子进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束了，在用这个临时文件替换上次持久化好的文件。整个过程中，主进程是不进行任何IO操作的。这就确保了极高的性能。如果需要进行大规模的数据的恢复，且对于数据恢复的完整性不死非常敏感，那RDB方式要比AOF 方式更加的高效。RDB的缺点是最后一次持久化的数据可能丢失。 功能核心函数rdbSave（生成RDB文件）和rdbLoad（从文件加载内存）两个函数 rdbSave：生成RDB文件 rdbLoad：从文件夹杂内存 RDB : 是redis默认的持久化机制 快照是默认的持久化方式。这种方式就是将内存中数据以快照的方式写入到二进制文件中，默认的文件名为dump.rdb。 优点： 快照保存数据极快，还原数据极快 适用于灾难备份 缺点： 小内存机器不适合使用，RDB机制符合要求就会照快照 快照条件： 12345671、服务器正常关闭：./bin/redis-cli shutdown2、key满足一定条件，会进行快照 vim redis.config 搜索save /savesave 900 1 //每秒900秒（15分钟）至少1个key发生变化，产生快照save 300 10 //每300秒 （5分钟）至少10个key发生变化，产生快照save 60 10000 //每60秒（1分钟）至少10000个key发生变化，产生快照 16.3 AOF由于快照方式是在一定间隔时间做一次的，所以如果redis意外down掉的话，就会丢失最后一个快照后的所有修改。如果应用要求不能丢失任何修改的话，可以采用aof持久化方式。 Append-only file：aof比rdb有更好的持久化性，是由于在使用aof持久化方式是，redis会将每一个收到的命令都通过write函数追加到文件中（默认是appendonly.aof)。当redis重启是会通过重新执行文件中保存的写命令来在内存冲重建整个数据库的内容。 每当执行服务器（定时）任务或者函数时flushAppendOnlyFile函数都会被调用，这个函数执行以下两个工作aof写入保存： WRITE：根据条件，将aof_buf中的缓存写入到AOF文件。 SAVE：根据条件，调用fsync或fdatasync函数，将AOF文件保存到磁盘中。 有三种方式如下（默认是：每秒fsync一次） appendonly yes //启用aof持久化方式 # appendfsync always //收到写命令就立即写入磁盘，最慢，但是保证完全的持久化 appendfysnceverysec //每秒钟写入磁盘一次，在性能和持久化方面做了很好的折中 # appendfysnc no //完全依赖os，性能孔，持久化没保证 产生的问题： aof的方式也同时带来了另一个问题。持久化文件会变的越来越大。例如我们调用incr test命令 100次，问价中必须保存全部的1000条命令，其实有99条都是多余的。 十七、Redis缓存与数据库一致性17.1 实时同步对强一直要求比较高的，应采用实时同步方案，即查询缓存查询不到在从DB查询，保存到缓存；更新缓存时，先更新数据库，在将缓存的设置过期（建议不要去更新缓存内容，直接设置缓存过期）。 @Cacheable：查询时使用，注意Long类型需要转换为String类型，否则会抛异常 @CachePut：跟新是使用，使用此注解，一定会从DB上查询数据 @CacheEvict：删除时使用； @Caching ：组合用法 17.2 异步队列对于并发程度高的，可采用异步队列的方式同步，可采用kafka等消息中间件处理消息生产和消费。 17.3 使用阿里的同步工具canal17.4 采用UDF自定义函数的方式面对mysql的API进行编程，利用触发器进行缓存同步，但UDF主要是C/C++语言实现，学习成本高。 十八、总结18.1 缓存穿透缓存穿透是指查询一个一定不存在的数据，由于缓存时不命中时需要从数据库查询，查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到数据库去查询，造成缓存穿透。 解决办法：持久层查询不到就缓存空结果，查询时先判断缓存中是否exists(key)，如果有直接返回空，没有则查询后返回， 注意insert时需要清除查询的key，否则即便DB中有值也查询不到（当然可以设置空缓存的过去时间） 概念 缓存穿透的概念很简单，用户想要查询一个数据没法安redis内存数据库没有，也就是缓存没有命中，于是向持久层数据量查询。发现也没有，于是本次查询失败。当用户很多的时候，缓存都没有命中，于是都去请求持久层数据库，这会给持久层数据库造成很大的压力，这时候就相当于出现了缓存穿透。 解决方案 18.1.1 布隆过滤器布隆过滤器是一种数据结构，对所有可能查询的参数以hash形式存储，在控制层先进行校验，不符合则丢弃，从而避免了对底层存储系统的查询压力； 但是这种方法存在两个问题： 1、如果空值能够被缓存起来，这就意味着缓存需要更过的空间村粗更过的键，因为这当中可能回有很多的空值的键； 2、即使对空值设置了过期时间，还是会存在缓存层和存储层的数据会有一段时间窗口的不一致，这对于需要保持一致性的业务会有影响； 18.1.2 缓存击穿 概念 这里需要主要的是缓存击穿的区别，缓存击穿，是指一个key非常热点，在不停的扛着大并发，大并发集中对这一个点进行访问，当这个key在失效的瞬间，持续的大并发就穿破缓存，直接请求数据库，就行在一个屏幕上凿开一个洞 当某个key在过期的瞬间，有大量的请求并发访问，这类数据一般是热点数据，由于缓存过期，会同时访问数据库来查询最新数据，并且回写缓存，会导致数据库瞬间压力过大。 解决方案 设置热点数据永不过期 从缓存层面来看，没有设置过期时间，所有不会出现热点key过期后产生的问题。 加锁互斥 分布式锁：使用分布式锁，保证对于每个key同时只有一个线程去查询后盾服务，其他线程没有获得分布式锁的权限，因此只需要等待即可，这种方式将高并发的压力转移到了分布式锁，因此对分布式锁的考验很大。 18.2 雪崩雪崩：缓存大量失效的时候，引发大量查询数据库。 解决办法： 用锁/分布式锁或者队列串行访问 缓存失效时间均匀分布 123如果缓存集中在一端时间内失效，发生大量的缓存穿透，所有的查询都落在数据库上，造成了缓存雪崩。这个没有完美解决办法，但是可以分析用户的行为，尽量让失效时间点均匀分布。大所属系统设计者考虑用加锁或者队列的方式保证缓存的单线程写，从而避免失效时大量的并发请求落到底层存储系统上。 加锁排队。限流—限流算法 在缓存失效后，通过加锁或者队列来控制读数据库写缓存的线程数量。比如对某个key只允许一个线程查询数据和写缓存，其他线程等待。 简单地来说，就是在缓存失效的时候（判断拿出来的值为空），不是立即去load db，而是先使用缓存工具的某些带成功操作返回值的操作（比如Redisde SETNX或者Memcache的ADD）去set一个mutex key，当操作返回成功是，在进行koad db 的操作应设缓存；否则，就重试整个get缓存的方法。 SETNX ,是【SET IF NOT EXISTS]的缩写，也就是只有不存在的时候才设置，可以利用它来实现锁的效果。 数据预热 可以通过缓存reload机制，预选去更新缓存，再即将发生大并发访问前手动触发加载缓存不同的key，设置不同的过期时间，让缓存失效的时间点尽量均匀。 18.3 热点key热点key：某个key访问非常频繁。当key失效的时候有大量线程来构建缓存，导致负载增加，系统崩溃。 解决办法： 使用锁，单机用synchronized ， lock等，分布式使用分布式锁 缓存过期时间不设置，而是设置在key对应的value里。如果检测到存的时间超过过期时间则 异步跟新缓存。 在value设置一个比过去时间t0小的过期时间值t1,当t1过期的时候，延长t1并做更新缓存操作。 设置标签缓存，标签缓存设置过期时间，标签缓存过期后，需异步地跟新实际缓存。 案例 1假设并发有10000个请求，想达到对个请求从数据库中获取，其他9999个请求冲redis中获取这种效果 双重检测锁测压： 十九、可能的问题一般来说，要将redis运用于工程项目中，只是用一台Redis是万万不能的，原因如下： 1、从结构上，单个Redis服务器会发生单点故障，并且一台服务器需要处理所有的请求负载，压力较大；（容错性） 2、从容量上，单个redis服务器内存容量有限，就算一台redis服务器内存容量为256G，也不能将所有内容用作Redis存储内存，一般来说，单台Redis最大使用内存不应该超过20G。 问题： 内存容量有限 处理能力有限 无法高可用。 二十、主从复制简介 电子商务网站上的商品，一般都是一次上传，无数次的浏览的，说专业点的也就是“多读少些”。 20.1主从复制：一个Redis服务可以有多个该服务的复制品，这个Redis服务成为Master，其他的复制成为Slaves 如图中所示：我们将一台Redis服务器作为主库（master），其他的三台作为（salve），主库负责写数据，每次有数据跟新都更新的数据同步到它的所有的从库，而从库只负责读数据。这样一来，就有了两个好处： 1、读写分离：不仅可以提高服务器的负载能力，并且可以根据读请求的规模自由增加或者减少从库的数据。 2、数据被复制成了好几份，就算一台机器出现故障，也可以使用其它机器的数据快速的恢复。 需要注意的是：Redis主从复制模式中，一台主库可以用用多个从库，一个从库只能属于一个主库。 20.2 Redis主从复制配置在Redis中，要实现主从复制架构非常的简单，只需要在从数据库的配置文件中加上如下命令即可： 1、主数据库不需要任务配置，创建爱哪一个从数据库： redis.config（配置文件信息） 12-- port 6380 : 从服务的端口号--slaveof 127.0.0.1 6379 ：指定主服务器 2、启动从数据库： 1./bin/redis-server ./redis.conf --port 6380 --slaveof 127.0.0.1 6379 3、登录到从服务客户端 1./bin/redis-cli -p 6380 -a redis 4、哨兵模式 简介 1Redis-Sentinel(哨兵模式)是高可用解决方案，当redis在做master-slave的高可用方案时，假如master宕机了，redis本身（以及其很多客户端）都没有实现自动进行主备切换，而redis-sentinel本身是独立运行的进程，可以部署在其他的与redis集群可通讯的机器中监控redis集群。 12有了主从复制的实现之后，我们如果想从服务器进行监控，那么在redis2.6以后提供了有个“哨兵”机制，并在2.8版本以后功能稳定起来。哨兵：故名司仪，就是监控Redis系统的运行状况。 哨兵模式的特点 12341、不时地监控redis是否按照预期良好地运行；2、如果发现某个redis 节点运行出现状况，能够通知另外一个进程（例如它的客户端）；3、能够进行自动切换。当一个master节点不可用时，能够选举出master的多个slave(如果有超过一个slave的话)中的一个来作为新的master，其他的slave节点会将它所追随的master地址改为被提升为master的salve的新地址。4、哨兵为客户端提供服务发现，客户端连接哨兵，哨兵提供当前master的地址然后提供服务，如果出现切换，也就是master挂了，哨兵会提供客户端一个新地址。 20.3 基本概述高可用 “高可用性（High Availability）”通常用来描述一个系统经过专门的设计，从而减少停工时间，而保证器服务的高可用性。（一直都能用） 高可用：6个99.9999% 全年停机不超过32秒。 高并发 高并发 （High Concurrentcy）是互联网分布式系统架构设计中必须考虑的因素之一，它通常是指，通过设计在保证系统能够同时并行处理的很多请求。 高并发相关商用的一些指标有如下： 响应时间（Response Time） 吞吐量（Throughput） 每秒查询率QPS（Query Pre Second），并发用户数等。 响应时间：系统对请求做出响应的时间，例如系统处理一个HTTP请求需要200ms，这个200ms就是系统的响应时间。 吞吐量：单位时间内处理的请求数量。 QPS ：每秒响应请求数。在互联网领域，这个指标和吞吐量区分的没有那么明显。 并发用户：同时承载正常使用系统功能的用户数量。例如一个即使通讯系统，同时在线量一定程度上代表 20.4 主从复制的主要作用包括1、数据冗余：主从复制实现了数据热备份，是持久化之外的一种数据冗余的方式。 2、故障恢复：当主节点出现 问题时，可以由从节点提供服务，实现快速的故障恢复；世界上是以一种服务的冗余。 3、负载均衡：在主从复制的基础上，配合读写分离，可以由节点提供写的服务，由从节点提供提供读服务（即写redis数据时应用及连接主节点，读redis数据时应该用从节点），人丹服务器的负载；尤其在写少读多的场景下，通过多个从节点分担读负载，可以大大提高redis服务器的并发量。 4、高可用的基石：除了上述作用以外，主从复制还是哨兵和集群能够实施的基础，因此说主从复制是redis高可用的基础。 20.5 主从复制的原理Slave 启动成功连接到master后会发送一个sync命令 Master接收到名，启动后台的存盘进程，同时收集所有接受到的用于修改数据集命令，在后台进程执行完毕后，master将传送的数据文件到slave，并完成一次完全同步 全量复制：而slave服务在接受到数据库文件数据后，将其存盘并加载内存中。 增量文件：Master继续将新的所有收集的修改命令一次传给slave，完成同步 但是只要是重新连接master，一次完全同步（全量复制）将别自动执行。 二十一、Redis Cluster集群简介 21.1 集群模式是实际使用最多的模式。Redis Cluster是社区版推出的Redis分布式集群解决方案，主要解决Redis分布式方面的需求，比如，当遇到单机内存，并发和流量等瓶颈的时候，Redis Cluster能起到很好的的负载均衡的目的。 为什么使用redis-cluster？ 123为了在大流量访问下提供稳定的业务，集群化时存储的必然形态未来的发展趋势可定是云计算和大数据的紧密结合只有分布式架构能满足需求 21.2 集群描述Redis集群搭建方案： 123(1)、Twitter开发的twemproxy(2)、豌豆荚开发的codis(3)、redis观法的redis-cluster Redis集群搭建的方式有很多种，但从redis 3.0 之后变动表呢支持redis-cluster集群，志超需要3（master）+3（Slave）才能简历集群。Redis——Cluster采用无中心结构，没个节点保存数据和整个集群状态，每个节点都和其他所有 节点连接。其redis-cluster架构图如下所示： Redis Cluster集群几点最小配置6个节点以上（3主3从），其中主节点提供读写操作，从节点作为备用节点，不提供请求，只作为故障转移使用。 Redis Cluster集群特点 1、所有的redis节点彼此互联（PING-PONG)，内部使用二进制协议优化传输速度和带宽。 2、节点的fail是通过集群中超过半数的节点检测失效时才生效。 3、客户端与redis节点直连，不需要中间proxy层。客户端不需要连接集群所有节点，连接集群中任何一个可用节点即可。 4、redis-cluster把所有的物理节点映射到[0-16383]slot上（不一定是平均分配），cluster负责维护 5、redis集群预先分好16384个哈希槽，当需要在redis集群中放置一个key-value时，redis先对key使用crc16算法算出一个结果，然后把结果对16384求余数，这样对每个key都会对应一个编号在0-16383之间的哈希槽，redis会根据节点数量大致均等的将哈希槽映射到不同的节点。 21.3 Redis Cluster集群搭建集群搭建参考官网：https://redis.io/topic/cluster-tutorial redis集群需要至少三个master节点，我们这里搭建三个master节点，并且给每个master在搭建一个slave节点，总共6个节点，这里用一台机器（可以多台机器部署，修改一下ip地址就可以了）部署6个redis实例，三主三从。搭建集群的步骤如下： 创建Redis节点安装目录 1mkdir /root/apps/redis_cluster ：指定目录下 创建 redis_cluster 在redis_cluster目录，创建7000-7005 6个文件夹下 1mkdir 70000 70001 70002 70003 70004 70005 并将redis-conf分别拷贝到70000-70005文件夹下 1cp /opt/redis-5.0.8/redis.conf ./70000 修改Redis配置文件 12345678910111213141516171819/root/apps/redis_cluster/70000# 关闭保护模式 用于公网访问 protected-mode no port 70000# 开启集群模式 cluster-enabled yes cluster-config-file nodes-70000.config cluster-node-timeout 5000# 后台启动 daemonize yes pidfile /var/run/redis_70000.pid logfile &quot;70000.log&quot;# dir /redis/data# 此处绑定ip,可以是阿里内网ip和本地ip也可以直接注释掉该项# bind 127.0.0.1# 用于连接主节点密码 masterauth redis#设置redis密码 各个几点请保持密码一致 requirepass redis 依次复制并修改6个redis.conf 12cp ./70000/redis.conf ./70001 :依次进行复制vim ./70001/redis.conf : 执行 %s/old/new/g 全部替换 ：wq 保存并退出 即可 6、依次启动6个节点 将安装的redis目录下的src复制到cluster下，方便启动服务端 12cd /opt/redis-5.0.8 :进入redis安装目录cp -r ./src/ /usr/local/redis_cluster/ :将src文件复制到redis——cluster目录中 123456./src/redis-server ./7000/redis.conf./src/redis-server ./7001/redis.conf./src/redis-server ./7002/redis.conf./src/redis-server ./7003/redis.conf./src/redis-server ./7004/redis.conf./src/redis-server ./7005/redis.conf 启动后，可以用PS查看进程： 1ps -ef | grep -i redis 创建集群 Redis 5版本后 通过redis-cli 客户端命令来创建集群。 1./src/redis-cli --cluster create -a redis 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 127.0.0.1:7003 127.0.0.1:7004 127.0.0.1:7005 --cluster-replicas 1 1234Performing hash slots allocation on 6 nodesTrying to optimize slaves allocation for anti-affinity 1[OK] All 16384 slots covered. Redis Cluster集群验证 在某台机器上（或）连接集群的7001端口的几点： 1redis-cli -h 127.0.0.1 -c -p 7000 -a redis :加参数 -c 可以连接到集群 redis cluster在设计的时候，就考虑了去中心化，去中间件，也就是说集群中的每个节点都是平等的关系，都是对等的，每个几点都保存各自的数据和整个集群的状态。每个节点都和其他所有节点连接，而且这些连接保持活跃，这样就保证了我们只需要连接集群中的任意一个节点，就可以获取到其他节点的数据。 基本命令 info replication 通过Cluster Nodes 命令和Cluster Info命令来看看集群的效果 1234567891011121314127.0.0.1:7000&gt; info replication# Replicationrole:masterconnected_slaves:1slave0:ip=127.0.0.1,port=7004,state=online,offset=1026,lag=1master_replid:2c2851db4bea0ea2f9d93d60a065e868112c47d7master_replid2:0000000000000000000000000000000000000000master_repl_offset:1026second_repl_offset:-1repl_backlog_active:1repl_backlog_size:1048576repl_backlog_first_byte_offset:1repl_backlog_histlen:1026127.0.0.1:7000&gt; 输入命令cluster nodes 1234567127.0.0.1:7000&gt; cluster nodesc58e7d40e34251897af3bee6bc6edd7a500f9fa6 127.0.0.1:7005@17005 slave 3a7ac96a39a41b395d5459eeac1f17d1f6fd96d7 0 1596391181092 6 connected71e898b018d1775214a431bd22b3408b055dbb62 127.0.0.1:7003@17003 slave 5a191a63e8c2f5dbe945451bca0552426d6e9260 0 1596391182096 4 connected60335b82c5348215c0dbbbac5b65c769f6668e4e 127.0.0.1:7000@17000 myself,master - 0 1596391181000 1 connected 0-54605a191a63e8c2f5dbe945451bca0552426d6e9260 127.0.0.1:7002@17002 master - 0 1596391183102 3 connected 10923-163839cae9ebf31a03c09a82281dcb629aac418e22831 127.0.0.1:7004@17004 slave 60335b82c5348215c0dbbbac5b65c769f6668e4e 0 1596391182000 5 connected3a7ac96a39a41b395d5459eeac1f17d1f6fd96d7 127.0.0.1:7001@17001 master - 0 1596391182598 2 connected 5461-10922 每个Redis的节点都有一个ID值，此ID将被此特定redis实例永久使用，以便实例在集群上下文中具有唯一的名称。每个节点都都会记住使用此ID的每个其他节点，而不是通过IP或端口号。IP地址和端口可能会发生变化，但唯一的节点标识符在节点的整个生命周期内都不会改变。我们简单称这个标识符为节点ID。 21.4 Redis总结简介： redis cluster 为了保证数据的高可用性，加入了主从模式，一个节点对应一个或多个从节点，主节点提供数据存取，从节点则是从主节点拉去数据备份，当这个主节点挂掉后，就会有这个从节点选取一个来充当主节点，从而保证集群不会挂掉。 集群有ABC三个主节点，如果这3个几点都没有加入从节点，如果B挂掉了，我们就无法访问整个集群了。A和C的slot也无法访问。 所以我们集群建立的时候，一定腰围每个主节点都添加一个从节点，比如像这样，集群包含主节点A,B,C 以及从节点A1,B1,C1，那么及时B挂掉系统也可以继续正确工作。 B1节点代替了B节点，所有Redis集群将会选择B1节点作为新的主节点，集群将会继续正确的提供服务。当B重新开启后，它就变成B1的从节点。 不过需要注意，如果几点B和B1同时挂掉，Redis集群就无法继续正确的提供服务了。 21.5 关闭集群在/root/apps/redis_cluster 目录下编写脚本文件 ： vim shutdowm.sh 内容如下： 1234567/root/apps/redis_cluster/src/redis-cli -c -h 127.0.0.1 -p 7000 -a redis shutdown/root/apps/redis_cluster/src/redis-cli -c -h 127.0.0.1 -p 7001 -a redis shutdown/root/apps/redis_cluster/src/redis-cli -c -h 127.0.0.1 -p 7002 -a redis shutdown/root/apps/redis_cluster/src/redis-cli -c -h 127.0.0.1 -p 7003 -a redis shutdown/root/apps/redis_cluster/src/redis-cli -c -h 127.0.0.1 -p 7004 -a redis shutdown/root/apps/redis_cluster/src/redis-cli -c -h 127.0.0.1 -p 7005 -a redis shutdown 12345chmod u+x shutdown.sh :然后执行将shutdown.sh变成可执行文件./shutdown.sh :在当前目录下启动查看： ps aux | grep redis官方：/usr/local/redis_cluster/redis-cli -a xxx -c -h 192.168.5.100 -p 8001提示：-a ：访问服务端密码 ， -c 表示集群模式 ， -h指定ip地址 ，-p指定端口号 123456/root/apps/redis_cluster/src/redis-service ./70000/redis.conf/root/apps/redis_cluster/src/redis-service ./70001/redis.conf/root/apps/redis_cluster/src/redis-service ./70002/redis.conf/root/apps/redis_cluster/src/redis-service ./70003/redis.conf/root/apps/redis_cluster/src/redis-service ./70004/redis.conf/root/apps/redis_cluster/src/redis-service ./70005/redis.conf 12chmod u+x redisinstall.sh :然后执行将redisinstall.sh变成可执行文件./redisinstall.sh 二十二、Redis中的拓展22.1 Redis为什么单线程还这么快1、误区1：高性能的服务器一定是多线程的？ 2、误区2：多线程（CUP上下文会切换！）一定比单线程效率高！ 先去CPU&gt;内存&gt;硬盘的速度要有所了解！ 核心：redis是将所有的数据全部放在内存中的，所以说使用单线程去炒作效率就是最高的。 多线称（CUP上下文切换：耗时！！！），对于内存来说，如果没有上下文切换效率就是最高的。对此读写就是在CUP上所以Redis 的速度是非常 快的。 二十三、重新配置RedisTemplate1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677package com.yux.redisdemo.redis; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration; import org.springframework.data.redis.connection.RedisConnectionFactory; import org.springframework.data.redis.core.RedisTemplate; import org.springframework.data.redis.serializer.Jackson2JsonRedisSerializer; import org.springframework.data.redis.serializer.StringRedisSerializer; import com.fasterxml.jackson.annotation.JsonAutoDetect; import com.fasterxml.jackson.annotation.PropertyAccessor; import com.fasterxml.jackson.databind.ObjectMapper; /**15 * redis配置类16 * @author YUX17 * @date 2018年6月6日18 *19 */ @Configurationpublic class RedisConfig &#123; @Bean @SuppressWarnings(&quot;all&quot;) public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory factory) &#123; RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;String, Object&gt;(); template.setConnectionFactory(factory); Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper om = new ObjectMapper(); om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(om); StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); // key采用String的序列化方式 template.setKeySerializer(stringRedisSerializer); // hash的key也采用String的序列化方式 template.setHashKeySerializer(stringRedisSerializer); // value序列化方式采用jackson template.setValueSerializer(jackson2JsonRedisSerializer); // hash的value序列化方式采用jackson template.setHashValueSerializer(jackson2JsonRedisSerializer); template.afterPropertiesSet(); return template; &#125; &#125; 二十四、重新编写RedisUtils123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382383384385386387388389390391392393394395396397398399400401402403404405406407408409410411412413414415416417418419420421422423424425426427428429430431432433434435436437438439440441442443444445446447448449450451452453454455456457458459460461462463464465466467468469470471472473474475476477478479480481482483484485486487488489490491492493494495496497498499500501502503504505506507508509510511512513514515516517518519520521522523524525526527528529530531532533534535536537538539540541542543544545546547548549550551552553554555556557558559560561562563564565566567568569570571572573574575576577578579580581582583584585586587588589590591592593594595596597598599600601602603604605606607608609610611612613614615616617618619620621622623624625626627628629630631632633634635636637638639640641642643644645646647648649650651652653654655656657658659660661662663664665666667668669670671672673674675676677678679680681682683684685686687688689690691692693694695696697698699700701702703704705706707708709710711712713714715716717718719720721722723724725726727728729730731732733734735736737738739740741742743744745746747748749750751752753754package com.yux.redisdemo.redis;import java.util.List; import java.util.Map;import java.util.Set;import java.util.concurrent.TimeUnit; import org.springframework.beans.factory.annotation.Autowired;import org.springframework.data.redis.core.RedisTemplate; import org.springframework.stereotype.Component; import org.springframework.util.CollectionUtils;/**14 * Redis工具类15 * @author YUX16 * @date 2018年6月7日17 */ @Componentpublic final class RedisUtil &#123; @Autowired private RedisTemplate&lt;String, Object&gt; redisTemplate; // =============================common============================ /** * 26 * 指定缓存失效时间 * 27 * * @param key 键 * 28 * @param time 时间(秒) * 29 * @return 30 */ public boolean expire(String key, long time) &#123; try &#123; if (time &gt; 0) &#123; redisTemplate.expire(key, time, TimeUnit.SECONDS); &#125; return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 44 * 根据key 获取过期时间 * 45 * * @param key 键 不能为null * 46 * @return 时间(秒) 返回0代表为永久有效 * 47 */ public long getExpire(String key) &#123; return redisTemplate.getExpire(key, TimeUnit.SECONDS); &#125; /** * 53 * 判断key是否存在 * 54 * * @param key 键 * 55 * @return true 存在 false不存在 * 56 */ public boolean hasKey(String key) &#123; try &#123; return redisTemplate.hasKey(key); &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 67 * 删除缓存 * 68 * * @param key 可以传一个值 或多个 * 69 */ @SuppressWarnings(&quot;unchecked&quot;) public void del(String... key) &#123; if (key != null &amp;&amp; key.length &gt; 0) &#123; if (key.length == 1) &#123; redisTemplate.delete(key[0]); &#125; else &#123; redisTemplate.delete(CollectionUtils.arrayToList(key)); &#125; &#125; &#125; // ============================String============================= /** * 83 * 普通缓存获取 * 84 * * @param key 键 * 85 * @return 值 * 86 */ public Object get(String key) &#123; return key == null ? null : redisTemplate.opsForValue().get(key); &#125; /** * 92 * 普通缓存放入 * 93 * * @param key 键 * 94 * @param value 值 * 95 * @return true成功 false失败 * 96 */ public boolean set(String key, Object value) &#123; try &#123; redisTemplate.opsForValue().set(key, value); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 109 * 普通缓存放入并设置时间 * 110 * * @param key 键 * 111 * @param value 值 * 112 * @param time 时间(秒) time要大于0 如果time小于等于0 将设置无限期 * 113 * @return true成功 false 失败 * 114 */ public boolean set(String key, Object value, long time) &#123; try &#123; if (time &gt; 0) &#123; redisTemplate.opsForValue().set(key, value, time, TimeUnit.SECONDS); &#125; else &#123; set(key, value); &#125; return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 130 * 递增 * 131 * * @param key 键 * 132 * @param delta 要增加几(大于0) * 133 * @return 134 */ public long incr(String key, long delta) &#123; if (delta &lt; 0) &#123; throw new RuntimeException(&quot;递增因子必须大于0&quot;); &#125; return redisTemplate.opsForValue().increment(key, delta); &#125; /** * 143 * 递减 * 144 * * @param key 键 * 145 * @param delta 要减少几(小于0) * 146 * @return 147 */ public long decr(String key, long delta) &#123; if (delta &lt; 0) &#123; throw new RuntimeException(&quot;递减因子必须大于0&quot;); &#125; return redisTemplate.opsForValue().increment(key, -delta); &#125; // ================================Map================================= /** * 157 * HashGet * 158 * * @param key 键 不能为null * 159 * @param item 项 不能为null * 160 * @return 值 * 161 */ public Object hget(String key, String item) &#123; return redisTemplate.opsForHash().get(key, item); &#125; /** * 167 * 获取hashKey对应的所有键值 * 168 * * @param key 键 * 169 * @return 对应的多个键值 * 170 */ public Map&lt;Object, Object&gt; hmget(String key) &#123; return redisTemplate.opsForHash().entries(key); &#125; /** * 176 * HashSet * 177 * * @param key 键 * 178 * @param map 对应多个键值 * 179 * @return true 成功 false 失败 * 180 */ public boolean hmset(String key, Map&lt;String, Object&gt; map) &#123; try &#123; redisTemplate.opsForHash().putAll(key, map); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 192 * HashSet 并设置时间 * 193 * * @param key 键 * 194 * @param map 对应多个键值 * 195 * @param time 时间(秒) * 196 * @return true成功 false失败 * 197 */ public boolean hmset(String key, Map&lt;String, Object&gt; map, long time) &#123; try &#123; redisTemplate.opsForHash().putAll(key, map); if (time &gt; 0) &#123; expire(key, time); &#125; return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 212 * 向一张hash表中放入数据,如果不存在将创建 * 213 * * @param key 键 * 214 * @param item 项 * 215 * @param value 值 * 216 * @return true 成功 false失败 * 217 */ public boolean hset(String key, String item, Object value) &#123; try &#123; redisTemplate.opsForHash().put(key, item, value); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 229 * 向一张hash表中放入数据,如果不存在将创建 * 230 * * @param key 键 * 231 * @param item 项 * 232 * @param value 值 * 233 * @param time 时间(秒) 注意:如果已存在的hash表有时间,这里将会替换原有的时间 * 234 * @return true 成功 false失败 * 235 */ public boolean hset(String key, String item, Object value, long time) &#123; try &#123; redisTemplate.opsForHash().put(key, item, value); if (time &gt; 0) &#123; expire(key, time); &#125; return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false &#125; &#125; /** * 250 * 删除hash表中的值 * 251 * * @param key 键 不能为null * 252 * @param item 项 可以使多个 不能为null * 253 */ public void hdel(String key, Object... item) &#123; redisTemplate.opsForHash().delete(key, item); &#125; /** * 259 * 判断hash表中是否有该项的值 * 260 * * @param key 键 不能为null * 261 * @param item 项 不能为null * 262 * @return true 存在 false不存在 * 263 */ public boolean hHasKey(String key, String item) &#123; return redisTemplate.opsForHash().hasKey(key, item); &#125; /** * 269 * hash递增 如果不存在,就会创建一个 并把新增后的值返回 * 270 * * @param key 键 * 271 * @param item 项 * 272 * @param by 要增加几(大于0) * 273 * @return 274 */ public double hincr(String key, String item, double by) &#123; return redisTemplate.opsForHash().increment(key, item, by); &#125; /** * 280 * hash递减 * 281 * * @param key 键 * 282 * @param item 项 * 283 * @param by 要减少记(小于0) * 284 * @return 285 */ public double hdecr(String key, String item, double by) &#123; return redisTemplate.opsForHash().increment(key, item, -by); &#125; // ============================set============================= /** * 292 * 根据key获取Set中的所有值 * 293 * * @param key 键 * 294 * @return 295 */ public Set&lt;Object&gt; sGet(String key) &#123; try &#123; return redisTemplate.opsForSet().members(key); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * 306 * 根据value从一个set中查询,是否存在 * 307 * * @param key 键 * 308 * @param value 值 * 309 * @return true 存在 false不存在 * 310 */ public boolean sHasKey(String key, Object value) &#123; try &#123; return redisTemplate.opsForSet().isMember(key, value); &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 321 * 将数据放入set缓存 * 322 * * @param key 键 * 323 * @param values 值 可以是多个 * 324 * @return 成功个数 * 325 */ public long sSet(String key, Object... values) &#123; try &#123; return redisTemplate.opsForSet().add(key, values); &#125; catch (Exception e) &#123; e.printStackTrace(); return 0; &#125; &#125; /** * 336 * 将set数据放入缓存 * 337 * * @param key 键 * 338 * @param time 时间(秒) * 339 * @param values 值 可以是多个 * 340 * @return 成功个数 * 341 */ public long sSetAndTime(String key, long time, Object... values) &#123; try &#123; Long count = redisTemplate.opsForSet().add(key, values); if (time &gt; 0) expire(key, time); return count; &#125; catch (Exception e) &#123; e.printStackTrace(); return 0; &#125; &#125; /** * 355 * 获取set缓存的长度 * 356 * * @param key 键 * 357 * @return 358 */ public long sGetSetSize(String key) &#123; try &#123; return redisTemplate.opsForSet().size(key); &#125; catch (Exception e) &#123; e.printStackTrace(); return 0; &#125; &#125; /** * 369 * 移除值为value的 * 370 * * @param key 键 * 371 * @param values 值 可以是多个 * 372 * @return 移除的个数 * 373 */ public long setRemove(String key, Object... values) &#123; try &#123; Long count = redisTemplate.opsForSet().remove(key, values); return count; &#125; catch (Exception e) &#123; e.printStackTrace(); return 0; &#125; &#125; // ===============================list================================= /** * 386 * 获取list缓存的内容 * 387 * * @param key 键 * 388 * @param start 开始 * 389 * @param end 结束 0 到 -1代表所有值 * 390 * @return 391 */ public List&lt;Object&gt; lGet(String key, long start, long end) try &#123; return redisTemplate.opsForList().range(key, start, end); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * 402 * 获取list缓存的长度 * 403 * * @param key 键 * 404 * @return 405 */ public long lGetListSize(String key) &#123; try &#123; return redisTemplate.opsForList().size(key); &#125; catch (Exception e) &#123; e.printStackTrace(); return 0; &#125; &#125; /** * 416 * 通过索引 获取list中的值 * 417 * * @param key 键 * 418 * @param index 索引 index&gt;=0时， 0 表头，1 第二个元素，依次类推；index&lt;0时，-1，表尾，-2倒数第二个元素，依次类推 * 419 * @return 420 */ public Object lGetIndex(String key, long index) &#123; try &#123; return redisTemplate.opsForList().index(key, index); &#125; catch (Exception e) &#123; e.printStackTrace(); return null; &#125; &#125; /** * 431 * 将list放入缓存 * 432 * * @param key 键 * 433 * @param value 值 * 434 * @param time 时间(秒) * 435 * @return 436 */ public boolean lSet(String key, Object value) &#123; try &#123; redisTemplate.opsForList().rightPush(key, value); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 将list放入缓存 * * @param key 键 * @param value 值 * @param time 时间(秒) * @return */ public boolean lSet(String key, Object value, long time) &#123; try &#123; redisTemplate.opsForList().rightPush(key, value); if (time &gt; 0) expire(key, time); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 467 * 将list放入缓存 * 468 * * @param key 键 * 469 * @param value 值 * 470 * @param time 时间(秒) * 471 * @return 472 */ public boolean lSet(String key, List&lt;Object&gt; value) &#123; try &#123; redisTemplate.opsForList().rightPushAll(key, value); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 484 * 将list放入缓存 * 485 * &lt;p&gt; * 486 * * @param key 键 * 487 * @param value 值 * 488 * @param time 时间(秒) * 489 * @return 490 */ public boolean lSet(String key, List&lt;Object&gt; value, long time) &#123; try &#123; redisTemplate.opsForList().rightPushAll(key, value); if (time &gt; 0) expire(key, time); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 504 * 根据索引修改list中的某条数据 * 505 * * @param key 键 * 506 * @param index 索引 * 507 * @param value 值 * 508 * @return 509 */ public boolean lUpdateIndex(String key, long index, Object value) &#123; try &#123; redisTemplate.opsForList().set(key, index, value); return true; &#125; catch (Exception e) &#123; e.printStackTrace(); return false; &#125; &#125; /** * 521 * 移除N个值为value * 522 * * @param key 键 * 523 * @param count 移除多少个 * 524 * @param value 值 * 525 * @return 移除的个数 * 526 */ public long lRemove(String key, long count, Object value) &#123; try &#123; Long remove = redisTemplate.opsForList().remove(key, count, value); return remove; &#125; catch (Exception e) &#123; e.printStackTrace(); return 0; &#125; &#125;&#125; 二十五、哨兵模式25.1 自动选举master的模式主从切换的技术方式是：当主机宕机之后，需要手动把一台服务器切换为主机服务器，这就需要人工干预，费时费力，还会造成一段时间内服务器不可用，这是一种不推荐的方式，更多的时候，我们在考虑使用哨兵模式。redis中2.8之后提供了哨兵模式来解决这个问题。 哨兵模式是一种特殊的模式，首先redis提供哨兵命令，哨兵是一个独立的进程，作为进程，它会独立运行。其原理是哨兵同过发送命令，等待redis 服务器响应，从而监控运行的多个redis实例。 这里的哨兵有两个作用： 通过发送命令，让redis服务器返回监控其运行状态，包括主服务器和从服务器。 当哨兵检测到master宕机，会自动将slave切换成为master，然后通过发布订阅模式通知其他的服务器，修改配置文件，让她们切换主机。 然而一个哨兵进程对Redis服务器进行监控可能会出现问题，为此，我们可以使用多个哨兵进行监控。各个哨兵之间还会进行监控，这样就形成了多哨兵模式。 假设主服务宕机，哨兵1先检测到这个结果，系统并不会马上进行failover过程，仅仅是哨兵1主管的认为服务器不可用，这个现象称为主观下线。当后面的哨兵也检测到主服务器不可用，并且数量达到一定值时，那么哨兵之间就会进行一侧投票，投票的结果由一个哨兵发起，进行failover[故障转移]操作。切换成功后，就会通过发布订阅模式，让哥哥哨兵把自己监控的服务器实现切换主机，这个过程称为可观下线。 我们目前的装填是一主二从！ 1、配置哨兵配置文件sentinel.conf 12# sentinel monitor 被监控的名称host port 1sentinel monitor myredis 127.0.0.1 6379 1 候命的这个数字1，代表主机挂了，slave投票看让谁阶梯称为主机，票数最多的，就会称为主机！ 1redis-sentinel bin/sentinel.conf 优点： 哨兵集群，基于主从复制模式，所有的主从配置优点，它全有 主从可以切换，故障可以转移，系统的可用性就会更好 哨兵模式就是主从模式的升级买手动到自动，更加健壮！ 缺点 ： redis不好在线扩容，集群容量一旦达到上限，在向扩容就十分麻烦！ 实现哨兵模式的配置其实是很麻烦的，里面有很多选择！ 原视频地址 原文章地址","categories":[{"name":"Redis","slug":"Redis","permalink":"https://lwy0518.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://lwy0518.github.io/tags/Redis/"},{"name":"数据库","slug":"数据库","permalink":"https://lwy0518.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"author":"lwy"},{"title":"kubernetes相关知识","slug":"kubernetes相关知识","date":"2022-02-12T13:34:24.000Z","updated":"2022-02-13T13:49:40.714Z","comments":true,"path":"2022/02/12/kubernetes相关知识/","link":"","permalink":"https://lwy0518.github.io/2022/02/12/kubernetes%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/","excerpt":"目录","text":"目录 Kubernetes详细教程1. Kubernetes介绍1.1 应用部署方式演变在部署应用程序的方式上，主要经历了三个时代： 传统部署：互联网早期，会直接将应用程序部署在物理机上 优点：简单，不需要其它技术的参与 缺点：不能为应用程序定义资源使用边界，很难合理地分配计算资源，而且程序之间容易产生影响 虚拟化部署：可以在一台物理机上运行多个虚拟机，每个虚拟机都是独立的一个环境 优点：程序环境不会相互产生影响，提供了一定程度的安全性 缺点：增加了操作系统，浪费了部分资源 容器化部署：与虚拟化类似，但是共享了操作系统 优点： 可以保证每个容器拥有自己的文件系统、CPU、内存、进程空间等 运行应用程序所需要的资源都被容器包装，并和底层基础架构解耦 容器化的应用程序可以跨云服务商、跨Linux操作系统发行版进行部署 容器化部署方式给带来很多的便利，但是也会出现一些问题，比如说： 一个容器故障停机了，怎么样让另外一个容器立刻启动去替补停机的容器 当并发访问量变大的时候，怎么样做到横向扩展容器数量 这些容器管理的问题统称为容器编排问题，为了解决这些容器编排问题，就产生了一些容器编排的软件： Swarm：Docker自己的容器编排工具 Mesos：Apache的一个资源统一管控的工具，需要和Marathon结合使用 Kubernetes：Google开源的的容器编排工具 1.2 kubernetes简介 kubernetes，是一个全新的基于容器技术的分布式架构领先方案，是谷歌严格保密十几年的秘密武器—-Borg系统的一个开源版本，于2014年9月发布第一个版本，2015年7月发布第一个正式版本。 kubernetes的本质是一组服务器集群，它可以在集群的每个节点上运行特定的程序，来对节点中的容器进行管理。目的是实现资源管理的自动化，主要提供了如下的主要功能： 自我修复：一旦某一个容器崩溃，能够在1秒中左右迅速启动新的容器 弹性伸缩：可以根据需要，自动对集群中正在运行的容器数量进行调整 服务发现：服务可以通过自动发现的形式找到它所依赖的服务 负载均衡：如果一个服务起动了多个容器，能够自动实现请求的负载均衡 版本回退：如果发现新发布的程序版本有问题，可以立即回退到原来的版本 存储编排：可以根据容器自身的需求自动创建存储卷 1.3 kubernetes组件一个kubernetes集群主要是由**控制节点(master)、工作节点(node)**构成，每个节点上都会安装不同的组件。 master：集群的控制平面，负责集群的决策 ( 管理 ) ApiServer : 资源操作的唯一入口，接收用户输入的命令，提供认证、授权、API注册和发现等机制 Scheduler : 负责集群资源调度，按照预定的调度策略将Pod调度到相应的node节点上 ControllerManager : 负责维护集群的状态，比如程序部署安排、故障检测、自动扩展、滚动更新等 Etcd ：负责存储集群中各种资源对象的信息 node：集群的数据平面，负责为容器提供运行环境 ( 干活 ) Kubelet : 负责维护容器的生命周期，即通过控制docker，来创建、更新、销毁容器 KubeProxy : 负责提供集群内部的服务发现和负载均衡 Docker : 负责节点上容器的各种操作 下面，以部署一个nginx服务来说明kubernetes系统各个组件调用关系： 首先要明确，一旦kubernetes环境启动之后，master和node都会将自身的信息存储到etcd数据库中 一个nginx服务的安装请求会首先被发送到master节点的apiServer组件 apiServer组件会调用scheduler组件来决定到底应该把这个服务安装到哪个node节点上 在此时，它会从etcd中读取各个node节点的信息，然后按照一定的算法进行选择，并将结果告知apiServer apiServer调用controller-manager去调度Node节点安装nginx服务 kubelet接收到指令后，会通知docker，然后由docker来启动一个nginx的pod pod是kubernetes的最小操作单元，容器必须跑在pod中至此， 一个nginx服务就运行了，如果需要访问nginx，就需要通过kube-proxy来对pod产生访问的代理 这样，外界用户就可以访问集群中的nginx服务了 1.4 kubernetes概念Master：集群控制节点，每个集群需要至少一个master节点负责集群的管控 Node：工作负载节点，由master分配容器到这些node工作节点上，然后node节点上的docker负责容器的运行 Pod：kubernetes的最小控制单元，容器都是运行在pod中的，一个pod中可以有1个或者多个容器 Controller：控制器，通过它来实现对pod的管理，比如启动pod、停止pod、伸缩pod的数量等等 Service：pod对外服务的统一入口，下面可以维护者同一类的多个pod Label：标签，用于对pod进行分类，同一类pod会拥有相同的标签 NameSpace：命名空间，用来隔离pod的运行环境 2. kubernetes集群环境搭建2.1 前置知识点目前生产部署Kubernetes 集群主要有两种方式： kubeadm Kubeadm 是一个K8s 部署工具，提供kubeadm init 和kubeadm join，用于快速部署Kubernetes 集群。 官方地址：https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/ 二进制包 从github 下载发行版的二进制包，手动部署每个组件，组成Kubernetes 集群。 Kubeadm 降低部署门槛，但屏蔽了很多细节，遇到问题很难排查。如果想更容易可控，推荐使用二进制包部署Kubernetes 集群，虽然手动部署麻烦点，期间可以学习很多工作原理，也利于后期维护。 2.2 kubeadm 部署方式介绍kubeadm 是官方社区推出的一个用于快速部署kubernetes 集群的工具，这个工具能通过两条指令完成一个kubernetes 集群的部署： 创建一个Master 节点kubeadm init 将Node 节点加入到当前集群中$ kubeadm join &lt;Master 节点的IP 和端口&gt; 2.3 安装要求在开始之前，部署Kubernetes 集群机器需要满足以下几个条件： 一台或多台机器，操作系统CentOS7.x-86_x64 硬件配置：2GB 或更多RAM，2 个CPU 或更多CPU，硬盘30GB 或更多 集群中所有机器之间网络互通 可以访问外网，需要拉取镜像 禁止swap 分区 2.4 最终目标 在所有节点上安装Docker 和kubeadm 部署Kubernetes Master 部署容器网络插件 部署Kubernetes Node，将节点加入Kubernetes 集群中 部署Dashboard Web 页面，可视化查看Kubernetes 资源 2.5 准备环境 角色 IP地址 组件 master01 192.168.5.3 docker，kubectl，kubeadm，kubelet node01 192.168.5.4 docker，kubectl，kubeadm，kubelet node02 192.168.5.5 docker，kubectl，kubeadm，kubelet 2.6 环境初始化2.6.1 检查操作系统的版本123# 此方式下安装kubernetes集群要求Centos版本要在7.5或之上[root@master ~]# cat /etc/redhat-releaseCentos Linux 7.5.1804 (Core) 2.6.2 主机名解析为了方便集群节点间的直接调用，在这个配置一下主机名解析，企业中推荐使用内部DNS服务器 1234# 主机名成解析 编辑三台服务器的/etc/hosts文件，添加下面内容192.168.90.100 master192.168.90.106 node1192.168.90.107 node2 2.6.3 时间同步kubernetes要求集群中的节点时间必须精确一直，这里使用chronyd服务从网络同步时间 企业中建议配置内部的会见同步服务器 1234# 启动chronyd服务[root@master ~]# systemctl start chronyd[root@master ~]# systemctl enable chronyd[root@master ~]# date 2.6.4 禁用iptable和firewalld服务kubernetes和docker 在运行的中会产生大量的iptables规则，为了不让系统规则跟它们混淆，直接关闭系统的规则 123456# 1 关闭firewalld服务[root@master ~]# systemctl stop firewalld[root@master ~]# systemctl disable firewalld# 2 关闭iptables服务[root@master ~]# systemctl stop iptables[root@master ~]# systemctl disable iptables 2.6.5 禁用selinuxselinux是linux系统下的一个安全服务，如果不关闭它，在安装集群中会产生各种各样的奇葩问题 123# 编辑 /etc/selinux/config 文件，修改SELINUX的值为disable# 注意修改完毕之后需要重启linux服务SELINUX=disabled 2.6.6 禁用swap分区swap分区指的是虚拟内存分区，它的作用是物理内存使用完，之后将磁盘空间虚拟成内存来使用，启用swap设备会对系统的性能产生非常负面的影响，因此kubernetes要求每个节点都要禁用swap设备，但是如果因为某些原因确实不能关闭swap分区，就需要在集群安装过程中通过明确的参数进行配置说明 12345# 编辑分区配置文件/etc/fstab，注释掉swap分区一行# 注意修改完毕之后需要重启linux服务vim /etc/fstab注释掉 /dev/mapper/centos-swap swap# /dev/mapper/centos-swap swap 2.6.7 修改linux的内核参数123456789101112# 修改linux的内核采纳数，添加网桥过滤和地址转发功能# 编辑/etc/sysctl.d/kubernetes.conf文件，添加如下配置：net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1# 重新加载配置[root@master ~]# sysctl -p# 加载网桥过滤模块[root@master ~]# modprobe br_netfilter# 查看网桥过滤模块是否加载成功[root@master ~]# lsmod | grep br_netfilter 2.6.8 配置ipvs功能在Kubernetes中Service有两种带来模型，一种是基于iptables的，一种是基于ipvs的两者比较的话，ipvs的性能明显要高一些，但是如果要使用它，需要手动载入ipvs模块 1234567891011121314151617# 1.安装ipset和ipvsadm[root@master ~]# yum install ipset ipvsadmin -y# 2.添加需要加载的模块写入脚本文件[root@master ~]# cat &lt;&lt;EOF&gt; /etc/sysconfig/modules/ipvs.modules#!/bin/bashmodprobe -- ip_vsmodprobe -- ip_vs_rrmodprobe -- ip_vs_wrrmodprobe -- ip_vs_shmodprobe -- nf_conntrack_ipv4EOF# 3.为脚本添加执行权限[root@master ~]# chmod +x /etc/sysconfig/modules/ipvs.modules# 4.执行脚本文件[root@master ~]# /bin/bash /etc/sysconfig/modules/ipvs.modules# 5.查看对应的模块是否加载成功[root@master ~]# lsmod | grep -e ip_vs -e nf_conntrack_ipv4 2.6.9 安装docker1234567891011121314151617181920212223# 1、切换镜像源[root@master ~]# wget https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo -O /etc/yum.repos.d/docker-ce.repo# 2、查看当前镜像源中支持的docker版本[root@master ~]# yum list docker-ce --showduplicates# 3、安装特定版本的docker-ce# 必须制定--setopt=obsoletes=0，否则yum会自动安装更高版本[root@master ~]# yum install --setopt=obsoletes=0 docker-ce-18.06.3.ce-3.el7 -y# 4、添加一个配置文件#Docker 在默认情况下使用Vgroup Driver为cgroupfs，而Kubernetes推荐使用systemd来替代cgroupfs[root@master ~]# mkdir /etc/docker[root@master ~]# cat &lt;&lt;EOF&gt; /etc/docker/daemon.json&#123; &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;registry-mirrors&quot;: [&quot;https://kn0t2bca.mirror.aliyuncs.com&quot;]&#125;EOF# 5、启动dokcer[root@master ~]# systemctl restart docker[root@master ~]# systemctl enable docker 2.6.10 安装Kubernetes组件123456789101112131415161718192021# 1、由于kubernetes的镜像在国外，速度比较慢，这里切换成国内的镜像源# 2、编辑/etc/yum.repos.d/kubernetes.repo,添加下面的配置[kubernetes]name=Kubernetesbaseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64enabled=1gpgchech=0repo_gpgcheck=0gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg# 3、安装kubeadm、kubelet和kubectl[root@master ~]# yum install --setopt=obsoletes=0 kubeadm-1.17.4-0 kubelet-1.17.4-0 kubectl-1.17.4-0 -y# 4、配置kubelet的cgroup#编辑/etc/sysconfig/kubelet, 添加下面的配置KUBELET_CGROUP_ARGS=&quot;--cgroup-driver=systemd&quot;KUBE_PROXY_MODE=&quot;ipvs&quot;# 5、设置kubelet开机自启[root@master ~]# systemctl enable kubelet 2.6.11 准备集群镜像123456789101112131415161718192021# 在安装kubernetes集群之前，必须要提前准备好集群需要的镜像，所需镜像可以通过下面命令查看[root@master ~]# kubeadm config images list# 下载镜像# 此镜像kubernetes的仓库中，由于网络原因，无法连接，下面提供了一种替换方案images=( kube-apiserver:v1.17.4 kube-controller-manager:v1.17.4 kube-scheduler:v1.17.4 kube-proxy:v1.17.4 pause:3.1 etcd:3.4.3-0 coredns:1.6.5)for imageName in $&#123;images[@]&#125;;do docker pull registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName docker tag registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName k8s.gcr.io/$imageName docker rmi registry.cn-hangzhou.aliyuncs.com/google_containers/$imageName done 2.6.11 集群初始化 下面的操作只需要在master节点上执行即可 1234567891011# 创建集群[root@master ~]# kubeadm init \\ --apiserver-advertise-address=192.168.90.100 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version=v1.17.4 \\ --service-cidr=10.96.0.0/12 \\ --pod-network-cidr=10.244.0.0/16# 创建必要文件[root@master ~]# mkdir -p $HOME/.kube[root@master ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config[root@master ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config 下面的操作只需要在node节点上执行即可 12kubeadm join 192.168.0.100:6443 --token awk15p.t6bamck54w69u4s8 \\ --discovery-token-ca-cert-hash sha256:a94fa09562466d32d29523ab6cff122186f1127599fa4dcd5fa0152694f17117 在master上查看节点信息 12345[root@master ~]# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster NotReady master 6m v1.17.4node1 NotReady &lt;none&gt; 22s v1.17.4node2 NotReady &lt;none&gt; 19s v1.17.4 2.6.13 安装网络插件，只在master节点操作即可1wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 由于外网不好访问，如果出现无法访问的情况，可以直接用下面的 记得文件名是kube-flannel.yml，位置：/root/kube-flannel.yml内容： 1https://github.com/flannel-io/flannel/tree/master/Documentation/kube-flannel.yml ![截屏2021-10-01 下午10.23.00](images/截屏2021-10-01 下午10.23.00.png) 2.6.14 使用kubeadm reset重置集群12345678910111213141516#在master节点之外的节点进行操作kubeadm resetsystemctl stop kubeletsystemctl stop dockerrm -rf /var/lib/cni/rm -rf /var/lib/kubelet/*rm -rf /etc/cni/ifconfig cni0 downifconfig flannel.1 downifconfig docker0 downip link delete cni0ip link delete flannel.1##重启kubeletsystemctl restart kubelet##重启dockersystemctl restart docker 2.6.15 重启kubelet和docker1234# 重启kubeletsystemctl restart kubelet# 重启dockersystemctl restart docker 使用配置文件启动fannel 1kubectl apply -f kube-flannel.yml 等待它安装完毕 发现已经是 集群的状态已经是Ready 2.6.16 kubeadm中的命令12# 生成 新的token[root@master ~]# kubeadm token create --print-join-command 2.7 集群测试2.7.1 创建一个nginx服务1kubectl create deployment nginx --image=nginx:1.14-alpine 2.7.2 暴露端口1kubectl expose deploy nginx --port=80 --target-port=80 --type=NodePort 2.7.3 查看服务1kubectl get pod,svc 2.7.4 查看pod 浏览器测试结果： 3. 资源管理3.1 资源管理介绍在kubernetes中，所有的内容都抽象为资源，用户需要通过操作资源来管理kubernetes。 kubernetes的本质上就是一个集群系统，用户可以在集群中部署各种服务，所谓的部署服务，其实就是在kubernetes集群中运行一个个的容器，并将指定的程序跑在容器中。 kubernetes的最小管理单元是pod而不是容器，所以只能将容器放在Pod中，而kubernetes一般也不会直接管理Pod，而是通过Pod控制器来管理Pod的。 Pod可以提供服务之后，就要考虑如何访问Pod中服务，kubernetes提供了Service资源实现这个功能。 当然，如果Pod中程序的数据需要持久化，kubernetes还提供了各种存储系统。 学习kubernetes的核心，就是学习如何对集群上的Pod、Pod控制器、Service、存储等各种资源进行操作 3.2 YAML语言介绍YAML是一个类似 XML、JSON 的标记性语言。它强调以数据为中心，并不是以标识语言为重点。因而YAML本身的定义比较简单，号称”一种人性化的数据格式语言”。 1234&lt;heima&gt; &lt;age&gt;15&lt;/age&gt; &lt;address&gt;Beijing&lt;/address&gt;&lt;/heima&gt; 123heima: age: 15 address: Beijing YAML的语法比较简单，主要有下面几个： 大小写敏感 使用缩进表示层级关系 缩进不允许使用tab，只允许空格( 低版本限制 ) 缩进的空格数不重要，只要相同层级的元素左对齐即可 ‘#’表示注释 YAML支持以下几种数据类型： 纯量：单个的、不可再分的值 对象：键值对的集合，又称为映射（mapping）/ 哈希（hash） / 字典（dictionary） 数组：一组按次序排列的值，又称为序列（sequence） / 列表（list） 1234567891011121314151617# 纯量, 就是指的一个简单的值，字符串、布尔值、整数、浮点数、Null、时间、日期# 1 布尔类型c1: true (或者True)# 2 整型c2: 234# 3 浮点型c3: 3.14# 4 null类型 c4: ~ # 使用~表示null# 5 日期类型c5: 2018-02-17 # 日期必须使用ISO 8601格式，即yyyy-MM-dd# 6 时间类型c6: 2018-02-17T15:02:31+08:00 # 时间使用ISO 8601格式，时间和日期之间使用T连接，最后使用+代表时区# 7 字符串类型c7: heima # 简单写法，直接写值 , 如果字符串中间有特殊字符，必须使用双引号或者单引号包裹 c8: line1 line2 # 字符串过多的情况可以拆成多行，每一行会被转化成一个空格 1234567# 对象# 形式一(推荐):heima: age: 15 address: Beijing# 形式二(了解):heima: &#123;age: 15,address: Beijing&#125; 1234567# 数组# 形式一(推荐):address: - 顺义 - 昌平 # 形式二(了解):address: [顺义,昌平] 小提示： 1 书写yaml切记: 后面要加一个空格 2 如果需要将多段yaml配置放在一个文件中，中间要使用---分隔 3 下面是一个yaml转json的网站，可以通过它验证yaml是否书写正确 https://www.json2yaml.com/convert-yaml-to-json 3.3 资源管理方式 命令式对象管理：直接使用命令去操作kubernetes资源 1kubectl run nginx-pod --image=nginx:1.17.1 --port=80 命令式对象配置：通过命令配置和配置文件去操作kubernetes资源 1kubectl create/patch -f nginx-pod.yaml 声明式对象配置：通过apply命令和配置文件去操作kubernetes资源 1kubectl apply -f nginx-pod.yaml 类型 操作对象 适用环境 优点 缺点 命令式对象管理 对象 测试 简单 只能操作活动对象，无法审计、跟踪 命令式对象配置 文件 开发 可以审计、跟踪 项目大时，配置文件多，操作麻烦 声明式对象配置 目录 开发 支持目录操作 意外情况下难以调试 3.3.1 命令式对象管理kubectl命令 kubectl是kubernetes集群的命令行工具，通过它能够对集群本身进行管理，并能够在集群上进行容器化应用的安装部署。kubectl命令的语法如下： 1kubectl [command] [type] [name] [flags] comand：指定要对资源执行的操作，例如create、get、delete type：指定资源类型，比如deployment、pod、service name：指定资源的名称，名称大小写敏感 flags：指定额外的可选参数 12345678# 查看所有podkubectl get pod # 查看某个podkubectl get pod pod_name# 查看某个pod,以yaml格式展示结果kubectl get pod pod_name -o yaml 资源类型 kubernetes中所有的内容都抽象为资源，可以通过下面的命令进行查看: 1kubectl api-resources 经常使用的资源有下面这些： 资源分类 资源名称 缩写 资源作用 集群级别资源 nodes no 集群组成部分 namespaces ns 隔离Pod pod资源 pods po 装载容器 pod资源控制器 replicationcontrollers rc 控制pod资源 replicasets rs 控制pod资源 deployments deploy 控制pod资源 daemonsets ds 控制pod资源 jobs 控制pod资源 cronjobs cj 控制pod资源 horizontalpodautoscalers hpa 控制pod资源 statefulsets sts 控制pod资源 服务发现资源 services svc 统一pod对外接口 ingress ing 统一pod对外接口 存储资源 volumeattachments 存储 persistentvolumes pv 存储 persistentvolumeclaims pvc 存储 配置资源 configmaps cm 配置 secrets 配置 操作 kubernetes允许对资源进行多种操作，可以通过–help查看详细的操作命令 1kubectl --help 经常使用的操作有下面这些： 命令分类 命令 翻译 命令作用 基本命令 create 创建 创建一个资源 edit 编辑 编辑一个资源 get 获取 获取一个资源 patch 更新 更新一个资源 delete 删除 删除一个资源 explain 解释 展示资源文档 运行和调试 run 运行 在集群中运行一个指定的镜像 expose 暴露 暴露资源为Service describe 描述 显示资源内部信息 logs 日志输出容器在 pod 中的日志 输出容器在 pod 中的日志 attach 缠绕进入运行中的容器 进入运行中的容器 exec 执行容器中的一个命令 执行容器中的一个命令 cp 复制 在Pod内外复制文件 rollout 首次展示 管理资源的发布 scale 规模 扩(缩)容Pod的数量 autoscale 自动调整 自动调整Pod的数量 高级命令 apply rc 通过文件对资源进行配置 label 标签 更新资源上的标签 其他命令 cluster-info 集群信息 显示集群信息 version 版本 显示当前Server和Client的版本 下面以一个namespace / pod的创建和删除简单演示下命令的使用： 123456789101112131415161718192021222324252627282930# 创建一个namespace[root@master ~]# kubectl create namespace devnamespace/dev created# 获取namespace[root@master ~]# kubectl get nsNAME STATUS AGEdefault Active 21hdev Active 21skube-node-lease Active 21hkube-public Active 21hkube-system Active 21h# 在此namespace下创建并运行一个nginx的Pod[root@master ~]# kubectl run pod --image=nginx:latest -n devkubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead.deployment.apps/pod created# 查看新创建的pod[root@master ~]# kubectl get pod -n devNAME READY STATUS RESTARTS AGEpod 1/1 Running 0 21s# 删除指定的pod[root@master ~]# kubectl delete pod pod-864f9875b9-pcw7xpod &quot;pod&quot; deleted# 删除指定的namespace[root@master ~]# kubectl delete ns devnamespace &quot;dev&quot; deleted 3.3.2 命令式对象配置命令式对象配置就是使用命令配合配置文件一起来操作kubernetes资源。 1） 创建一个nginxpod.yaml，内容如下： 12345678910111213141516apiVersion: v1kind: Namespacemetadata: name: dev---apiVersion: v1kind: Podmetadata: name: nginxpod namespace: devspec: containers: - name: nginx-containers image: nginx:latest 2）执行create命令，创建资源： 123[root@master ~]# kubectl create -f nginxpod.yamlnamespace/dev createdpod/nginxpod created 此时发现创建了两个资源对象，分别是namespace和pod 3）执行get命令，查看资源： 123456[root@master ~]# kubectl get -f nginxpod.yamlNAME STATUS AGEnamespace/dev Active 18sNAME READY STATUS RESTARTS AGEpod/nginxpod 1/1 Running 0 17s 这样就显示了两个资源对象的信息 4）执行delete命令，删除资源： 123[root@master ~]# kubectl delete -f nginxpod.yamlnamespace &quot;dev&quot; deletedpod &quot;nginxpod&quot; deleted 此时发现两个资源对象被删除了 12总结: 命令式对象配置的方式操作资源，可以简单的认为：命令 + yaml配置文件（里面是命令需要的各种参数） 3.3.3 声明式对象配置声明式对象配置跟命令式对象配置很相似，但是它只有一个命令apply。 123456789# 首先执行一次kubectl apply -f yaml文件，发现创建了资源[root@master ~]# kubectl apply -f nginxpod.yamlnamespace/dev createdpod/nginxpod created# 再次执行一次kubectl apply -f yaml文件，发现说资源没有变动[root@master ~]# kubectl apply -f nginxpod.yamlnamespace/dev unchangedpod/nginxpod unchanged 12345总结: 其实声明式对象配置就是使用apply描述一个资源最终的状态（在yaml中定义状态） 使用apply操作资源： 如果资源不存在，就创建，相当于 kubectl create 如果资源已存在，就更新，相当于 kubectl patch 扩展：kubectl可以在node节点上运行吗 ? kubectl的运行是需要进行配置的，它的配置文件是$HOME/.kube，如果想要在node节点运行此命令，需要将master上的.kube文件复制到node节点上，即在master节点上执行下面操作： 1scp -r HOME/.kube node1: HOME/ 使用推荐: 三种方式应该怎么用 ? 创建/更新资源 使用声明式对象配置 kubectl apply -f XXX.yaml 删除资源 使用命令式对象配置 kubectl delete -f XXX.yaml 查询资源 使用命令式对象管理 kubectl get(describe) 资源名称 4. 实战入门本章节将介绍如何在kubernetes集群中部署一个nginx服务，并且能够对其进行访问。 4.1 NamespaceNamespace是kubernetes系统中的一种非常重要资源，它的主要作用是用来实现多套环境的资源隔离或者多租户的资源隔离。 默认情况下，kubernetes集群中的所有的Pod都是可以相互访问的。但是在实际中，可能不想让两个Pod之间进行互相的访问，那此时就可以将两个Pod划分到不同的namespace下。kubernetes通过将集群内部的资源分配到不同的Namespace中，可以形成逻辑上的”组”，以方便不同的组的资源进行隔离使用和管理。 可以通过kubernetes的授权机制，将不同的namespace交给不同租户进行管理，这样就实现了多租户的资源隔离。此时还能结合kubernetes的资源配额机制，限定不同租户能占用的资源，例如CPU使用量、内存使用量等等，来实现租户可用资源的管理。 kubernetes在集群启动之后，会默认创建几个namespace 123456[root@master ~]# kubectl get namespaceNAME STATUS AGEdefault Active 45h # 所有未指定Namespace的对象都会被分配在default命名空间kube-node-lease Active 45h # 集群节点之间的心跳维护，v1.13开始引入kube-public Active 45h # 此命名空间下的资源可以被所有人访问（包括未认证用户）kube-system Active 45h # 所有由Kubernetes系统创建的资源都处于这个命名空间 下面来看namespace资源的具体操作： 4.1.1 查看1234567891011121314151617181920212223242526272829303132333435363738394041# 1 查看所有的ns 命令：kubectl get ns[root@master ~]# kubectl get nsNAME STATUS AGEdefault Active 45hkube-node-lease Active 45hkube-public Active 45h kube-system Active 45h # 2 查看指定的ns 命令：kubectl get ns ns名称[root@master ~]# kubectl get ns defaultNAME STATUS AGEdefault Active 45h# 3 指定输出格式 命令：kubectl get ns ns名称 -o 格式参数# kubernetes支持的格式有很多，比较常见的是wide、json、yaml[root@master ~]# kubectl get ns default -o yamlapiVersion: v1kind: Namespacemetadata: creationTimestamp: &quot;2021-05-08T04:44:16Z&quot; name: default resourceVersion: &quot;151&quot; selfLink: /api/v1/namespaces/default uid: 7405f73a-e486-43d4-9db6-145f1409f090spec: finalizers: - kubernetesstatus: phase: Active # 4 查看ns详情 命令：kubectl describe ns ns名称[root@master ~]# kubectl describe ns defaultName: defaultLabels: &lt;none&gt;Annotations: &lt;none&gt;Status: Active # Active 命名空间正在使用中 Terminating 正在删除命名空间# ResourceQuota 针对namespace做的资源限制# LimitRange针对namespace中的每个组件做的资源限制No resource quota.No LimitRange resource. 4.1.2 创建123# 创建namespace[root@master ~]# kubectl create ns devnamespace/dev created 4.1.3 删除123# 删除namespace[root@master ~]# kubectl delete ns devnamespace &quot;dev&quot; deleted 4.1.4 配置方式首先准备一个yaml文件：ns-dev.yaml 1234apiVersion: v1kind: Namespacemetadata: name: dev 然后就可以执行对应的创建和删除命令了： 创建：kubectl create -f ns-dev.yaml 删除：kubectl delete -f ns-dev.yaml 4.2 PodPod是kubernetes集群进行管理的最小单元，程序要运行必须部署在容器中，而容器必须存在于Pod中。 Pod可以认为是容器的封装，一个Pod中可以存在一个或者多个容器。 kubernetes在集群启动之后，集群中的各个组件也都是以Pod方式运行的。可以通过下面命令查看： 123456789101112[root@master ~]# kubectl get pod -n kube-systemNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-6955765f44-68g6v 1/1 Running 0 2d1hkube-system coredns-6955765f44-cs5r8 1/1 Running 0 2d1hkube-system etcd-master 1/1 Running 0 2d1hkube-system kube-apiserver-master 1/1 Running 0 2d1hkube-system kube-controller-manager-master 1/1 Running 0 2d1hkube-system kube-flannel-ds-amd64-47r25 1/1 Running 0 2d1hkube-system kube-flannel-ds-amd64-ls5lh 1/1 Running 0 2d1hkube-system kube-proxy-685tk 1/1 Running 0 2d1hkube-system kube-proxy-87spt 1/1 Running 0 2d1hkube-system kube-scheduler-master 1/1 Running 0 2d1h 4.2.1 创建并运行kubernetes没有提供单独运行Pod的命令，都是通过Pod控制器来实现的 123456# 命令格式： kubectl run (pod控制器名称) [参数] # --image 指定Pod的镜像# --port 指定端口# --namespace 指定namespace[root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --namespace dev deployment.apps/nginx created 4.2.2 查看pod信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657# 查看Pod基本信息[root@master ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEnginx 1/1 Running 0 43s# 查看Pod的详细信息[root@master ~]# kubectl describe pod nginx -n devName: nginxNamespace: devPriority: 0Node: node1/192.168.5.4Start Time: Wed, 08 May 2021 09:29:24 +0800Labels: pod-template-hash=5ff7956ff6 run=nginxAnnotations: &lt;none&gt;Status: RunningIP: 10.244.1.23IPs: IP: 10.244.1.23Controlled By: ReplicaSet/nginxContainers: nginx: Container ID: docker://4c62b8c0648d2512380f4ffa5da2c99d16e05634979973449c98e9b829f6253c Image: nginx:latest Image ID: docker-pullable://nginx@sha256:485b610fefec7ff6c463ced9623314a04ed67e3945b9c08d7e53a47f6d108dc7 Port: 80/TCP Host Port: 0/TCP State: Running Started: Wed, 08 May 2021 09:30:01 +0800 Ready: True Restart Count: 0 Environment: &lt;none&gt; Mounts: /var/run/secrets/kubernetes.io/serviceaccount from default-token-hwvvw (ro)Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled TrueVolumes: default-token-hwvvw: Type: Secret (a volume populated by a Secret) SecretName: default-token-hwvvw Optional: falseQoS Class: BestEffortNode-Selectors: &lt;none&gt;Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300sEvents: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned dev/nginx-5ff7956ff6-fg2db to node1 Normal Pulling 4m11s kubelet, node1 Pulling image &quot;nginx:latest&quot; Normal Pulled 3m36s kubelet, node1 Successfully pulled image &quot;nginx:latest&quot; Normal Created 3m36s kubelet, node1 Created container nginx Normal Started 3m36s kubelet, node1 Started container nginx 4.2.3 访问Pod12345678910111213141516# 获取podIP[root@master ~]# kubectl get pods -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE ... nginx 1/1 Running 0 190s 10.244.1.23 node1 ...#访问POD[root@master ~]# curl http://10.244.1.23:80&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;p&gt;&lt;em&gt;Thank you for using nginx.&lt;/em&gt;&lt;/p&gt;&lt;/body&gt;&lt;/html&gt; 4.2.4 删除指定Pod123456789101112131415161718192021222324# 删除指定Pod[root@master ~]# kubectl delete pod nginx -n devpod &quot;nginx&quot; deleted# 此时，显示删除Pod成功，但是再查询，发现又新产生了一个 [root@master ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEnginx 1/1 Running 0 21s# 这是因为当前Pod是由Pod控制器创建的，控制器会监控Pod状况，一旦发现Pod死亡，会立即重建# 此时要想删除Pod，必须删除Pod控制器# 先来查询一下当前namespace下的Pod控制器[root@master ~]# kubectl get deploy -n devNAME READY UP-TO-DATE AVAILABLE AGEnginx 1/1 1 1 9m7s# 接下来，删除此PodPod控制器[root@master ~]# kubectl delete deploy nginx -n devdeployment.apps &quot;nginx&quot; deleted# 稍等片刻，再查询Pod，发现Pod被删除了[root@master ~]# kubectl get pods -n devNo resources found in dev namespace. 4.2.5 配置操作创建一个pod-nginx.yaml，内容如下： 12345678910111213apiVersion: v1kind: Podmetadata: name: nginx namespace: devspec: containers: - image: nginx:latest name: pod ports: - name: nginx-port containerPort: 80 protocol: TCP 然后就可以执行对应的创建和删除命令了： 创建：kubectl create -f pod-nginx.yaml 删除：kubectl delete -f pod-nginx.yaml 4.3 LabelLabel是kubernetes系统中的一个重要概念。它的作用就是在资源上添加标识，用来对它们进行区分和选择。 Label的特点： 一个Label会以key/value键值对的形式附加到各种对象上，如Node、Pod、Service等等 一个资源对象可以定义任意数量的Label ，同一个Label也可以被添加到任意数量的资源对象上去 Label通常在资源对象定义时确定，当然也可以在对象创建后动态添加或者删除 可以通过Label实现资源的多维度分组，以便灵活、方便地进行资源分配、调度、配置、部署等管理工作。 一些常用的Label 示例如下： 版本标签：”version”:”release”, “version”:”stable”…… 环境标签：”environment”:”dev”，”environment”:”test”，”environment”:”pro” 架构标签：”tier”:”frontend”，”tier”:”backend” 标签定义完毕之后，还要考虑到标签的选择，这就要使用到Label Selector，即： Label用于给某个资源对象定义标识 Label Selector用于查询和筛选拥有某些标签的资源对象 当前有两种Label Selector： 基于等式的Label Selector name = slave: 选择所有包含Label中key=”name”且value=”slave”的对象 env != production: 选择所有包括Label中的key=”env”且value不等于”production”的对象 基于集合的Label Selector name in (master, slave): 选择所有包含Label中的key=”name”且value=”master”或”slave”的对象 name not in (frontend): 选择所有包含Label中的key=”name”且value不等于”frontend”的对象 标签的选择条件可以使用多个，此时将多个Label Selector进行组合，使用逗号”,”进行分隔即可。例如： name=slave，env!=production name not in (frontend)，env!=production 4.3.1 命令方式1234567891011121314151617181920212223# 为pod资源打标签[root@master ~]# kubectl label pod nginx-pod version=1.0 -n devpod/nginx-pod labeled# 为pod资源更新标签[root@master ~]# kubectl label pod nginx-pod version=2.0 -n dev --overwritepod/nginx-pod labeled# 查看标签[root@master ~]# kubectl get pod nginx-pod -n dev --show-labelsNAME READY STATUS RESTARTS AGE LABELSnginx-pod 1/1 Running 0 10m version=2.0# 筛选标签[root@master ~]# kubectl get pod -n dev -l version=2.0 --show-labelsNAME READY STATUS RESTARTS AGE LABELSnginx-pod 1/1 Running 0 17m version=2.0[root@master ~]# kubectl get pod -n dev -l version!=2.0 --show-labelsNo resources found in dev namespace.#删除标签[root@master ~]# kubectl label pod nginx-pod version- -n devpod/nginx-pod labeled 4.3.2 配置方式12345678910111213141516apiVersion: v1kind: Podmetadata: name: nginx namespace: dev labels: version: &quot;3.0&quot; env: &quot;test&quot;spec: containers: - image: nginx:latest name: pod ports: - name: nginx-port containerPort: 80 protocol: TCP 然后就可以执行对应的更新命令了：kubectl apply -f pod-nginx.yaml 4.4 Deployment在kubernetes中，Pod是最小的控制单元，但是kubernetes很少直接控制Pod，一般都是通过Pod控制器来完成的。Pod控制器用于pod的管理，确保pod资源符合预期的状态，当pod的资源出现故障时，会尝试进行重启或重建pod。 在kubernetes中Pod控制器的种类有很多，本章节只介绍一种：Deployment。 4.4.1 命令操作123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 命令格式: kubectl create deployment 名称 [参数] # --image 指定pod的镜像# --port 指定端口# --replicas 指定创建pod数量# --namespace 指定namespace[root@master ~]# kubectl run nginx --image=nginx:latest --port=80 --replicas=3 -n devdeployment.apps/nginx created# 查看创建的Pod[root@master ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEnginx-5ff7956ff6-6k8cb 1/1 Running 0 19snginx-5ff7956ff6-jxfjt 1/1 Running 0 19snginx-5ff7956ff6-v6jqw 1/1 Running 0 19s# 查看deployment的信息[root@master ~]# kubectl get deploy -n devNAME READY UP-TO-DATE AVAILABLE AGEnginx 3/3 3 3 2m42s# UP-TO-DATE：成功升级的副本数量# AVAILABLE：可用副本的数量[root@master ~]# kubectl get deploy -n dev -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES SELECTORnginx 3/3 3 3 2m51s nginx nginx:latest run=nginx# 查看deployment的详细信息[root@master ~]# kubectl describe deploy nginx -n devName: nginxNamespace: devCreationTimestamp: Wed, 08 May 2021 11:14:14 +0800Labels: run=nginxAnnotations: deployment.kubernetes.io/revision: 1Selector: run=nginxReplicas: 3 desired | 3 updated | 3 total | 3 available | 0 unavailableStrategyType: RollingUpdateMinReadySeconds: 0RollingUpdateStrategy: 25% max unavailable, 25% max surgePod Template: Labels: run=nginx Containers: nginx: Image: nginx:latest Port: 80/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt;Conditions: Type Status Reason ---- ------ ------ Available True MinimumReplicasAvailable Progressing True NewReplicaSetAvailableOldReplicaSets: &lt;none&gt;NewReplicaSet: nginx-5ff7956ff6 (3/3 replicas created)Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal ScalingReplicaSet 5m43s deployment-controller Scaled up replicaset nginx-5ff7956ff6 to 3 # 删除 [root@master ~]# kubectl delete deploy nginx -n devdeployment.apps &quot;nginx&quot; deleted 4.4.2 配置操作创建一个deploy-nginx.yaml，内容如下： 123456789101112131415161718192021apiVersion: apps/v1kind: Deploymentmetadata: name: nginx namespace: devspec: replicas: 3 selector: matchLabels: run: nginx template: metadata: labels: run: nginx spec: containers: - image: nginx:latest name: nginx ports: - containerPort: 80 protocol: TCP 然后就可以执行对应的创建和删除命令了： 创建：kubectl create -f deploy-nginx.yaml 删除：kubectl delete -f deploy-nginx.yaml 4.5 Service通过上节课的学习，已经能够利用Deployment来创建一组Pod来提供具有高可用性的服务。 虽然每个Pod都会分配一个单独的Pod IP，然而却存在如下两问题： Pod IP 会随着Pod的重建产生变化 Pod IP 仅仅是集群内可见的虚拟IP，外部无法访问 这样对于访问这个服务带来了难度。因此，kubernetes设计了Service来解决这个问题。 Service可以看作是一组同类Pod对外的访问接口。借助Service，应用可以方便地实现服务发现和负载均衡。 4.5.1 创建集群内部可访问的Service12345678910111213141516171819202122# 暴露Service[root@master ~]# kubectl expose deploy nginx --name=svc-nginx1 --type=ClusterIP --port=80 --target-port=80 -n devservice/svc-nginx1 exposed# 查看service[root@master ~]# kubectl get svc svc-nginx1 -n dev -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORsvc-nginx1 ClusterIP 10.109.179.231 &lt;none&gt; 80/TCP 3m51s run=nginx# 这里产生了一个CLUSTER-IP，这就是service的IP，在Service的生命周期中，这个地址是不会变动的# 可以通过这个IP访问当前service对应的POD[root@master ~]# curl 10.109.179.231:80&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;title&gt;Welcome to nginx!&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Welcome to nginx!&lt;/h1&gt;.......&lt;/body&gt;&lt;/html&gt; 4.5.2 创建集群外部也可访问的Service12345678910111213# 上面创建的Service的type类型为ClusterIP，这个ip地址只用集群内部可访问# 如果需要创建外部也可以访问的Service，需要修改type为NodePort[root@master ~]# kubectl expose deploy nginx --name=svc-nginx2 --type=NodePort --port=80 --target-port=80 -n devservice/svc-nginx2 exposed# 此时查看，会发现出现了NodePort类型的Service，而且有一对Port（80:31928/TC）[root@master ~]# kubectl get svc svc-nginx2 -n dev -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORsvc-nginx2 NodePort 10.100.94.0 &lt;none&gt; 80:31928/TCP 9s run=nginx# 接下来就可以通过集群外的主机访问 节点IP:31928访问服务了# 例如在的电脑主机上通过浏览器访问下面的地址http://192.168.90.100:31928/ 4.5.3 删除Service12[root@master ~]# kubectl delete svc svc-nginx-1 -n dev service &quot;svc-nginx-1&quot; deleted 4.5.4 配置方式创建一个svc-nginx.yaml，内容如下： 1234567891011121314apiVersion: v1kind: Servicemetadata: name: svc-nginx namespace: devspec: clusterIP: 10.109.179.231 #固定svc的内网ip ports: - port: 80 protocol: TCP targetPort: 80 selector: run: nginx type: ClusterIP 然后就可以执行对应的创建和删除命令了： 创建：kubectl create -f svc-nginx.yaml 删除：kubectl delete -f svc-nginx.yaml 小结 至此，已经掌握了Namespace、Pod、Deployment、Service资源的基本操作，有了这些操作，就可以在kubernetes集群中实现一个服务的简单部署和访问了，但是如果想要更好的使用kubernetes，就需要深入学习这几种资源的细节和原理。 5. Pod详解5.1 Pod介绍5.1.1 Pod结构 每个Pod中都可以包含一个或者多个容器，这些容器可以分为两类： 用户程序所在的容器，数量可多可少 Pause容器，这是每个Pod都会有的一个根容器，它的作用有两个： 可以以它为依据，评估整个Pod的健康状态 可以在根容器上设置Ip地址，其它容器都此Ip（Pod IP），以实现Pod内部的网路通信 1这里是Pod内部的通讯，Pod的之间的通讯采用虚拟二层网络技术来实现，我们当前环境用的是Flannel 5.1.2 Pod定义下面是Pod的资源清单： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778apiVersion: v1 #必选，版本号，例如v1kind: Pod #必选，资源类型，例如 Podmetadata: #必选，元数据 name: string #必选，Pod名称 namespace: string #Pod所属的命名空间,默认为&quot;default&quot; labels: #自定义标签列表 - name: string spec: #必选，Pod中容器的详细定义 containers: #必选，Pod中容器列表 - name: string #必选，容器名称 image: string #必选，容器的镜像名称 imagePullPolicy: [ Always|Never|IfNotPresent ] #获取镜像的策略 command: [string] #容器的启动命令列表，如不指定，使用打包时使用的启动命令 args: [string] #容器的启动命令参数列表 workingDir: string #容器的工作目录 volumeMounts: #挂载到容器内部的存储卷配置 - name: string #引用pod定义的共享存储卷的名称，需用volumes[]部分定义的的卷名 mountPath: string #存储卷在容器内mount的绝对路径，应少于512字符 readOnly: boolean #是否为只读模式 ports: #需要暴露的端口库号列表 - name: string #端口的名称 containerPort: int #容器需要监听的端口号 hostPort: int #容器所在主机需要监听的端口号，默认与Container相同 protocol: string #端口协议，支持TCP和UDP，默认TCP env: #容器运行前需设置的环境变量列表 - name: string #环境变量名称 value: string #环境变量的值 resources: #资源限制和请求的设置 limits: #资源限制的设置 cpu: string #Cpu的限制，单位为core数，将用于docker run --cpu-shares参数 memory: string #内存限制，单位可以为Mib/Gib，将用于docker run --memory参数 requests: #资源请求的设置 cpu: string #Cpu请求，容器启动的初始可用数量 memory: string #内存请求,容器启动的初始可用数量 lifecycle: #生命周期钩子 postStart: #容器启动后立即执行此钩子,如果执行失败,会根据重启策略进行重启 preStop: #容器终止前执行此钩子,无论结果如何,容器都会终止 livenessProbe: #对Pod内各容器健康检查的设置，当探测无响应几次后将自动重启该容器 exec: #对Pod容器内检查方式设置为exec方式 command: [string] #exec方式需要制定的命令或脚本 httpGet: #对Pod内个容器健康检查方法设置为HttpGet，需要制定Path、port path: string port: number host: string scheme: string HttpHeaders: - name: string value: string tcpSocket: #对Pod内个容器健康检查方式设置为tcpSocket方式 port: number initialDelaySeconds: 0 #容器启动完成后首次探测的时间，单位为秒 timeoutSeconds: 0 #对容器健康检查探测等待响应的超时时间，单位秒，默认1秒 periodSeconds: 0 #对容器监控检查的定期探测时间设置，单位秒，默认10秒一次 successThreshold: 0 failureThreshold: 0 securityContext: privileged: false restartPolicy: [Always | Never | OnFailure] #Pod的重启策略 nodeName: &lt;string&gt; #设置NodeName表示将该Pod调度到指定到名称的node节点上 nodeSelector: obeject #设置NodeSelector表示将该Pod调度到包含这个label的node上 imagePullSecrets: #Pull镜像时使用的secret名称，以key：secretkey格式指定 - name: string hostNetwork: false #是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络 volumes: #在该pod上定义共享存储卷列表 - name: string #共享存储卷名称 （volumes类型有很多种） emptyDir: &#123;&#125; #类型为emtyDir的存储卷，与Pod同生命周期的一个临时目录。为空值 hostPath: string #类型为hostPath的存储卷，表示挂载Pod所在宿主机的目录 path: string #Pod所在宿主机的目录，将被用于同期中mount的目录 secret: #类型为secret的存储卷，挂载集群与定义的secret对象到容器内部 scretname: string items: - key: string path: string configMap: #类型为configMap的存储卷，挂载预定义的configMap对象到容器内部 name: string items: - key: string path: string 1234567891011121314151617181920212223242526272829303132333435#小提示：# 在这里，可通过一个命令来查看每种资源的可配置项# kubectl explain 资源类型 查看某种资源可以配置的一级属性# kubectl explain 资源类型.属性 查看属性的子属性[root@k8s-master01 ~]# kubectl explain podKIND: PodVERSION: v1FIELDS: apiVersion &lt;string&gt; kind &lt;string&gt; metadata &lt;Object&gt; spec &lt;Object&gt; status &lt;Object&gt;[root@k8s-master01 ~]# kubectl explain pod.metadataKIND: PodVERSION: v1RESOURCE: metadata &lt;Object&gt;FIELDS: annotations &lt;map[string]string&gt; clusterName &lt;string&gt; creationTimestamp &lt;string&gt; deletionGracePeriodSeconds &lt;integer&gt; deletionTimestamp &lt;string&gt; finalizers &lt;[]string&gt; generateName &lt;string&gt; generation &lt;integer&gt; labels &lt;map[string]string&gt; managedFields &lt;[]Object&gt; name &lt;string&gt; namespace &lt;string&gt; ownerReferences &lt;[]Object&gt; resourceVersion &lt;string&gt; selfLink &lt;string&gt; uid &lt;string&gt; 在kubernetes中基本所有资源的一级属性都是一样的，主要包含5部分： apiVersion 版本，由kubernetes内部定义，版本号必须可以用 kubectl api-versions 查询到 kind 类型，由kubernetes内部定义，版本号必须可以用 kubectl api-resources 查询到 metadata 元数据，主要是资源标识和说明，常用的有name、namespace、labels等 spec 描述，这是配置中最重要的一部分，里面是对各种资源配置的详细描述 status 状态信息，里面的内容不需要定义，由kubernetes自动生成 在上面的属性中，spec是接下来研究的重点，继续看下它的常见子属性: containers &lt;[]Object&gt; 容器列表，用于定义容器的详细信息 nodeName 根据nodeName的值将pod调度到指定的Node节点上 nodeSelector &lt;map[]&gt; 根据NodeSelector中定义的信息选择将该Pod调度到包含这些label的Node 上 hostNetwork 是否使用主机网络模式，默认为false，如果设置为true，表示使用宿主机网络 volumes &lt;[]Object&gt; 存储卷，用于定义Pod上面挂在的存储信息 restartPolicy 重启策略，表示Pod在遇到故障的时候的处理策略 5.2 Pod配置本小节主要来研究pod.spec.containers属性，这也是pod配置中最为关键的一项配置。 12345678910111213[root@k8s-master01 ~]# kubectl explain pod.spec.containersKIND: PodVERSION: v1RESOURCE: containers &lt;[]Object&gt; # 数组，代表可以有多个容器FIELDS: name &lt;string&gt; # 容器名称 image &lt;string&gt; # 容器需要的镜像地址 imagePullPolicy &lt;string&gt; # 镜像拉取策略 command &lt;[]string&gt; # 容器的启动命令列表，如不指定，使用打包时使用的启动命令 args &lt;[]string&gt; # 容器的启动命令需要的参数列表 env &lt;[]Object&gt; # 容器环境变量的配置 ports &lt;[]Object&gt; # 容器需要暴露的端口号列表 resources &lt;Object&gt; # 资源限制和资源请求的设置 5.2.1 基本配置创建pod-base.yaml文件，内容如下： 12345678910111213apiVersion: v1kind: Podmetadata: name: pod-base namespace: dev labels: user: heimaspec: containers: - name: nginx image: nginx:1.17.1 - name: busybox image: busybox:1.30 上面定义了一个比较简单Pod的配置，里面有两个容器： nginx：用1.17.1版本的nginx镜像创建，（nginx是一个轻量级web容器） busybox：用1.30版本的busybox镜像创建，（busybox是一个小巧的linux命令集合） 1234567891011121314# 创建Pod[root@k8s-master01 pod]# kubectl apply -f pod-base.yamlpod/pod-base created# 查看Pod状况# READY 1/2 : 表示当前Pod中有2个容器，其中1个准备就绪，1个未就绪# RESTARTS : 重启次数，因为有1个容器故障了，Pod一直在重启试图恢复它[root@k8s-master01 pod]# kubectl get pod -n devNAME READY STATUS RESTARTS AGEpod-base 1/2 Running 4 95s# 可以通过describe查看内部的详情# 此时已经运行起来了一个基本的Pod，虽然它暂时有问题[root@k8s-master01 pod]# kubectl describe pod pod-base -n dev 5.2.2 镜像拉取创建pod-imagepullpolicy.yaml文件，内容如下： 123456789101112apiVersion: v1kind: Podmetadata: name: pod-imagepullpolicy namespace: devspec: containers: - name: nginx image: nginx:1.17.1 imagePullPolicy: Never # 用于设置镜像拉取策略 - name: busybox image: busybox:1.30 imagePullPolicy，用于设置镜像拉取策略，kubernetes支持配置三种拉取策略： Always：总是从远程仓库拉取镜像（一直远程下载） IfNotPresent：本地有则使用本地镜像，本地没有则从远程仓库拉取镜像（本地有就本地 本地没远程下载） Never：只使用本地镜像，从不去远程仓库拉取，本地没有就报错 （一直使用本地） 默认值说明： 如果镜像tag为具体版本号， 默认策略是：IfNotPresent 如果镜像tag为：latest（最终版本） ，默认策略是always 12345678910111213141516171819# 创建Pod[root@k8s-master01 pod]# kubectl create -f pod-imagepullpolicy.yamlpod/pod-imagepullpolicy created# 查看Pod详情# 此时明显可以看到nginx镜像有一步Pulling image &quot;nginx:1.17.1&quot;的过程[root@k8s-master01 pod]# kubectl describe pod pod-imagepullpolicy -n dev......Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled &lt;unknown&gt; default-scheduler Successfully assigned dev/pod-imagePullPolicy to node1 Normal Pulling 32s kubelet, node1 Pulling image &quot;nginx:1.17.1&quot; Normal Pulled 26s kubelet, node1 Successfully pulled image &quot;nginx:1.17.1&quot; Normal Created 26s kubelet, node1 Created container nginx Normal Started 25s kubelet, node1 Started container nginx Normal Pulled 7s (x3 over 25s) kubelet, node1 Container image &quot;busybox:1.30&quot; already present on machine Normal Created 7s (x3 over 25s) kubelet, node1 Created container busybox Normal Started 7s (x3 over 25s) kubelet, node1 Started container busybox 5.2.3 启动命令在前面的案例中，一直有一个问题没有解决，就是的busybox容器一直没有成功运行，那么到底是什么原因导致这个容器的故障呢？ 原来busybox并不是一个程序，而是类似于一个工具类的集合，kubernetes集群启动管理后，它会自动关闭。解决方法就是让其一直在运行，这就用到了command配置。 创建pod-command.yaml文件，内容如下： 123456789101112apiVersion: v1kind: Podmetadata: name: pod-command namespace: devspec: containers: - name: nginx image: nginx:1.17.1 - name: busybox image: busybox:1.30 command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;touch /tmp/hello.txt;while true;do /bin/echo $(date +%T) &gt;&gt; /tmp/hello.txt; sleep 3; done;&quot;] command，用于在pod中的容器初始化完毕之后运行一个命令。 稍微解释下上面命令的意思： “/bin/sh”,”-c”, 使用sh执行命令 touch /tmp/hello.txt; 创建一个/tmp/hello.txt 文件 while true;do /bin/echo $(date +%T) &gt;&gt; /tmp/hello.txt; sleep 3; done; 每隔3秒向文件中写入当前时间 12345678910111213141516171819# 创建Pod[root@k8s-master01 pod]# kubectl create -f pod-command.yamlpod/pod-command created# 查看Pod状态# 此时发现两个pod都正常运行了[root@k8s-master01 pod]# kubectl get pods pod-command -n devNAME READY STATUS RESTARTS AGEpod-command 2/2 Runing 0 2s# 进入pod中的busybox容器，查看文件内容# 补充一个命令: kubectl exec pod名称 -n 命名空间 -it -c 容器名称 /bin/sh 在容器内部执行命令# 使用这个命令就可以进入某个容器的内部，然后进行相关操作了# 比如，可以查看txt文件的内容[root@k8s-master01 pod]# kubectl exec pod-command -n dev -it -c busybox /bin/sh/ # tail -f /tmp/hello.txt14:44:1914:44:2214:44:25 123456特别说明： 通过上面发现command已经可以完成启动命令和传递参数的功能，为什么这里还要提供一个args选项，用于传递参数呢?这其实跟docker有点关系，kubernetes中的command、args两项其实是实现覆盖Dockerfile中ENTRYPOINT的功能。 1 如果command和args均没有写，那么用Dockerfile的配置。 2 如果command写了，但args没有写，那么Dockerfile默认的配置会被忽略，执行输入的command 3 如果command没写，但args写了，那么Dockerfile中配置的ENTRYPOINT的命令会被执行，使用当前args的参数 4 如果command和args都写了，那么Dockerfile的配置被忽略，执行command并追加上args参数 5.2.4 环境变量创建pod-env.yaml文件，内容如下： 123456789101112131415apiVersion: v1kind: Podmetadata: name: pod-env namespace: devspec: containers: - name: busybox image: busybox:1.30 command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;while true;do /bin/echo $(date +%T);sleep 60; done;&quot;] env: # 设置环境变量列表 - name: &quot;username&quot; value: &quot;admin&quot; - name: &quot;password&quot; value: &quot;123456&quot; env，环境变量，用于在pod中的容器设置环境变量。 12345678910# 创建Pod[root@k8s-master01 ~]# kubectl create -f pod-env.yamlpod/pod-env created# 进入容器，输出环境变量[root@k8s-master01 ~]# kubectl exec pod-env -n dev -c busybox -it /bin/sh/ # echo $usernameadmin/ # echo $password123456 这种方式不是很推荐，推荐将这些配置单独存储在配置文件中，这种方式将在后面介绍。 5.2.5 端口设置本小节来介绍容器的端口设置，也就是containers的ports选项。 首先看下ports支持的子选项： 12345678910[root@k8s-master01 ~]# kubectl explain pod.spec.containers.portsKIND: PodVERSION: v1RESOURCE: ports &lt;[]Object&gt;FIELDS: name &lt;string&gt; # 端口名称，如果指定，必须保证name在pod中是唯一的 containerPort&lt;integer&gt; # 容器要监听的端口(0&lt;x&lt;65536) hostPort &lt;integer&gt; # 容器要在主机上公开的端口，如果设置，主机上只能运行容器的一个副本(一般省略) hostIP &lt;string&gt; # 要将外部端口绑定到的主机IP(一般省略) protocol &lt;string&gt; # 端口协议。必须是UDP、TCP或SCTP。默认为“TCP”。 接下来，编写一个测试案例，创建pod-ports.yaml 12345678910111213apiVersion: v1kind: Podmetadata: name: pod-ports namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: # 设置容器暴露的端口列表 - name: nginx-port containerPort: 80 protocol: TCP 123456789101112131415161718# 创建Pod[root@k8s-master01 ~]# kubectl create -f pod-ports.yamlpod/pod-ports created# 查看pod# 在下面可以明显看到配置信息[root@k8s-master01 ~]# kubectl get pod pod-ports -n dev -o yaml......spec: containers: - image: nginx:1.17.1 imagePullPolicy: IfNotPresent name: nginx ports: - containerPort: 80 name: nginx-port protocol: TCP...... 访问容器中的程序需要使用的是Podip:containerPort 5.2.6 资源配额容器中的程序要运行，肯定是要占用一定资源的，比如cpu和内存等，如果不对某个容器的资源做限制，那么它就可能吃掉大量资源，导致其它容器无法运行。针对这种情况，kubernetes提供了对内存和cpu的资源进行配额的机制，这种机制主要通过resources选项实现，他有两个子选项： limits：用于限制运行时容器的最大占用资源，当容器占用资源超过limits时会被终止，并进行重启 requests ：用于设置容器需要的最小资源，如果环境资源不够，容器将无法启动 可以通过上面两个选项设置资源的上下限。 接下来，编写一个测试案例，创建pod-resources.yaml 12345678910111213141516apiVersion: v1kind: Podmetadata: name: pod-resources namespace: devspec: containers: - name: nginx image: nginx:1.17.1 resources: # 资源配额 limits: # 限制资源（上限） cpu: &quot;2&quot; # CPU限制，单位是core数 memory: &quot;10Gi&quot; # 内存限制 requests: # 请求资源（下限） cpu: &quot;1&quot; # CPU限制，单位是core数 memory: &quot;10Mi&quot; # 内存限制 在这对cpu和memory的单位做一个说明： cpu：core数，可以为整数或小数 memory： 内存大小，可以使用Gi、Mi、G、M等形式 1234567891011121314151617181920212223242526272829# 运行Pod[root@k8s-master01 ~]# kubectl create -f pod-resources.yamlpod/pod-resources created# 查看发现pod运行正常[root@k8s-master01 ~]# kubectl get pod pod-resources -n devNAME READY STATUS RESTARTS AGE pod-resources 1/1 Running 0 39s # 接下来，停止Pod[root@k8s-master01 ~]# kubectl delete -f pod-resources.yamlpod &quot;pod-resources&quot; deleted# 编辑pod，修改resources.requests.memory的值为10Gi[root@k8s-master01 ~]# vim pod-resources.yaml# 再次启动pod[root@k8s-master01 ~]# kubectl create -f pod-resources.yamlpod/pod-resources created# 查看Pod状态，发现Pod启动失败[root@k8s-master01 ~]# kubectl get pod pod-resources -n dev -o wideNAME READY STATUS RESTARTS AGE pod-resources 0/1 Pending 0 20s # 查看pod详情会发现，如下提示[root@k8s-master01 ~]# kubectl describe pod pod-resources -n dev......Warning FailedScheduling 35s default-scheduler 0/3 nodes are available: 1 node(s) had taint &#123;node-role.kubernetes.io/master: &#125;, that the pod didn&#x27;t tolerate, 2 Insufficient memory.(内存不足) 5.3 Pod生命周期我们一般将pod对象从创建至终的这段时间范围称为pod的生命周期，它主要包含下面的过程： pod创建过程 运行初始化容器（init container）过程 运行主容器（main container） 容器启动后钩子（post start）、容器终止前钩子（pre stop） 容器的存活性探测（liveness probe）、就绪性探测（readiness probe） pod终止过程 在整个生命周期中，Pod会出现5种状态（相位），分别如下： 挂起（Pending）：apiserver已经创建了pod资源对象，但它尚未被调度完成或者仍处于下载镜像的过程中 运行中（Running）：pod已经被调度至某节点，并且所有容器都已经被kubelet创建完成 成功（Succeeded）：pod中的所有容器都已经成功终止并且不会被重启 失败（Failed）：所有容器都已经终止，但至少有一个容器终止失败，即容器返回了非0值的退出状态 未知（Unknown）：apiserver无法正常获取到pod对象的状态信息，通常由网络通信失败所导致 5.3.1 创建和终止pod的创建过程 用户通过kubectl或其他api客户端提交需要创建的pod信息给apiServer apiServer开始生成pod对象的信息，并将信息存入etcd，然后返回确认信息至客户端 apiServer开始反映etcd中的pod对象的变化，其它组件使用watch机制来跟踪检查apiServer上的变动 scheduler发现有新的pod对象要创建，开始为Pod分配主机并将结果信息更新至apiServer node节点上的kubelet发现有pod调度过来，尝试调用docker启动容器，并将结果回送至apiServer apiServer将接收到的pod状态信息存入etcd中 pod的终止过程 用户向apiServer发送删除pod对象的命令 apiServcer中的pod对象信息会随着时间的推移而更新，在宽限期内（默认30s），pod被视为dead 将pod标记为terminating状态 kubelet在监控到pod对象转为terminating状态的同时启动pod关闭过程 端点控制器监控到pod对象的关闭行为时将其从所有匹配到此端点的service资源的端点列表中移除 如果当前pod对象定义了preStop钩子处理器，则在其标记为terminating后即会以同步的方式启动执行 pod对象中的容器进程收到停止信号 宽限期结束后，若pod中还存在仍在运行的进程，那么pod对象会收到立即终止的信号 kubelet请求apiServer将此pod资源的宽限期设置为0从而完成删除操作，此时pod对于用户已不可见 5.3.2 初始化容器初始化容器是在pod的主容器启动之前要运行的容器，主要是做一些主容器的前置工作，它具有两大特征： 初始化容器必须运行完成直至结束，若某初始化容器运行失败，那么kubernetes需要重启它直到成功完成 初始化容器必须按照定义的顺序执行，当且仅当前一个成功之后，后面的一个才能运行 初始化容器有很多的应用场景，下面列出的是最常见的几个： 提供主容器镜像中不具备的工具程序或自定义代码 初始化容器要先于应用容器串行启动并运行完成，因此可用于延后应用容器的启动直至其依赖的条件得到满足 接下来做一个案例，模拟下面这个需求： 假设要以主容器来运行nginx，但是要求在运行nginx之前先要能够连接上mysql和redis所在服务器 为了简化测试，事先规定好mysql(192.168.90.14)和redis(192.168.90.15)服务器的地址 创建pod-initcontainer.yaml，内容如下： 12345678910111213141516171819apiVersion: v1kind: Podmetadata: name: pod-initcontainer namespace: devspec: containers: - name: main-container image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 initContainers: - name: test-mysql image: busybox:1.30 command: [&#x27;sh&#x27;, &#x27;-c&#x27;, &#x27;until ping 192.168.90.14 -c 1 ; do echo waiting for mysql...; sleep 2; done;&#x27;] - name: test-redis image: busybox:1.30 command: [&#x27;sh&#x27;, &#x27;-c&#x27;, &#x27;until ping 192.168.90.15 -c 1 ; do echo waiting for reids...; sleep 2; done;&#x27;] 12345678910111213141516171819202122232425262728# 创建pod[root@k8s-master01 ~]# kubectl create -f pod-initcontainer.yamlpod/pod-initcontainer created# 查看pod状态# 发现pod卡在启动第一个初始化容器过程中，后面的容器不会运行root@k8s-master01 ~]# kubectl describe pod pod-initcontainer -n dev........Events: Type Reason Age From Message ---- ------ ---- ---- ------- Normal Scheduled 49s default-scheduler Successfully assigned dev/pod-initcontainer to node1 Normal Pulled 48s kubelet, node1 Container image &quot;busybox:1.30&quot; already present on machine Normal Created 48s kubelet, node1 Created container test-mysql Normal Started 48s kubelet, node1 Started container test-mysql# 动态查看pod[root@k8s-master01 ~]# kubectl get pods pod-initcontainer -n dev -wNAME READY STATUS RESTARTS AGEpod-initcontainer 0/1 Init:0/2 0 15spod-initcontainer 0/1 Init:1/2 0 52spod-initcontainer 0/1 Init:1/2 0 53spod-initcontainer 0/1 PodInitializing 0 89spod-initcontainer 1/1 Running 0 90s# 接下来新开一个shell，为当前服务器新增两个ip，观察pod的变化[root@k8s-master01 ~]# ifconfig ens33:1 192.168.90.14 netmask 255.255.255.0 up[root@k8s-master01 ~]# ifconfig ens33:2 192.168.90.15 netmask 255.255.255.0 up 5.3.3 钩子函数钩子函数能够感知自身生命周期中的事件，并在相应的时刻到来时运行用户指定的程序代码。 kubernetes在主容器的启动之后和停止之前提供了两个钩子函数： post start：容器创建之后执行，如果失败了会重启容器 pre stop ：容器终止之前执行，执行完成之后容器将成功终止，在其完成之前会阻塞删除容器的操作 钩子处理器支持使用下面三种方式定义动作： Exec命令：在容器内执行一次命令 12345678…… lifecycle: postStart: exec: command: - cat - /tmp/healthy…… TCPSocket：在当前容器尝试访问指定的socket 123456…… lifecycle: postStart: tcpSocket: port: 8080…… HTTPGet：在当前容器中向某url发起http请求 123456789…… lifecycle: postStart: httpGet: path: / #URI地址 port: 80 #端口号 host: 192.168.5.3 #主机地址 scheme: HTTP #支持的协议，http或者https…… 接下来，以exec方式为例，演示下钩子函数的使用，创建pod-hook-exec.yaml文件，内容如下： 12345678910111213141516171819apiVersion: v1kind: Podmetadata: name: pod-hook-exec namespace: devspec: containers: - name: main-container image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 lifecycle: postStart: exec: # 在容器启动的时候执行一个命令，修改掉nginx的默认首页内容 command: [&quot;/bin/sh&quot;, &quot;-c&quot;, &quot;echo postStart... &gt; /usr/share/nginx/html/index.html&quot;] preStop: exec: # 在容器停止之前停止nginx服务 command: [&quot;/usr/sbin/nginx&quot;,&quot;-s&quot;,&quot;quit&quot;] 123456789101112# 创建pod[root@k8s-master01 ~]# kubectl create -f pod-hook-exec.yamlpod/pod-hook-exec created# 查看pod[root@k8s-master01 ~]# kubectl get pods pod-hook-exec -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE pod-hook-exec 1/1 Running 0 29s 10.244.2.48 node2 # 访问pod[root@k8s-master01 ~]# curl 10.244.2.48postStart... 5.3.4 容器探测容器探测用于检测容器中的应用实例是否正常工作，是保障业务可用性的一种传统机制。如果经过探测，实例的状态不符合预期，那么kubernetes就会把该问题实例” 摘除 “，不承担业务流量。kubernetes提供了两种探针来实现容器探测，分别是： liveness probes：存活性探针，用于检测应用实例当前是否处于正常运行状态，如果不是，k8s会重启容器 readiness probes：就绪性探针，用于检测应用实例当前是否可以接收请求，如果不能，k8s不会转发流量 livenessProbe 决定是否重启容器，readinessProbe 决定是否将请求转发给容器。 上面两种探针目前均支持三种探测方式： Exec命令：在容器内执行一次命令，如果命令执行的退出码为0，则认为程序正常，否则不正常 1234567…… livenessProbe: exec: command: - cat - /tmp/healthy…… TCPSocket：将会尝试访问一个用户容器的端口，如果能够建立这条连接，则认为程序正常，否则不正常 12345…… livenessProbe: tcpSocket: port: 8080…… HTTPGet：调用容器内Web应用的URL，如果返回的状态码在200和399之间，则认为程序正常，否则不正常 12345678…… livenessProbe: httpGet: path: / #URI地址 port: 80 #端口号 host: 127.0.0.1 #主机地址 scheme: HTTP #支持的协议，http或者https…… 下面以liveness probes为例，做几个演示： 方式一：Exec 创建pod-liveness-exec.yaml 123456789101112131415apiVersion: v1kind: Podmetadata: name: pod-liveness-exec namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: exec: command: [&quot;/bin/cat&quot;,&quot;/tmp/hello.txt&quot;] # 执行一个查看文件的命令 创建pod，观察效果 1234567891011121314151617181920# 创建Pod[root@k8s-master01 ~]# kubectl create -f pod-liveness-exec.yamlpod/pod-liveness-exec created# 查看Pod详情[root@k8s-master01 ~]# kubectl describe pods pod-liveness-exec -n dev...... Normal Created 20s (x2 over 50s) kubelet, node1 Created container nginx Normal Started 20s (x2 over 50s) kubelet, node1 Started container nginx Normal Killing 20s kubelet, node1 Container nginx failed liveness probe, will be restarted Warning Unhealthy 0s (x5 over 40s) kubelet, node1 Liveness probe failed: cat: can&#x27;t open &#x27;/tmp/hello11.txt&#x27;: No such file or directory # 观察上面的信息就会发现nginx容器启动之后就进行了健康检查# 检查失败之后，容器被kill掉，然后尝试进行重启（这是重启策略的作用，后面讲解）# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长[root@k8s-master01 ~]# kubectl get pods pod-liveness-exec -n devNAME READY STATUS RESTARTS AGEpod-liveness-exec 0/1 CrashLoopBackOff 2 3m19s# 当然接下来，可以修改成一个存在的文件，比如/tmp/hello.txt，再试，结果就正常了...... 方式二：TCPSocket 创建pod-liveness-tcpsocket.yaml 123456789101112131415apiVersion: v1kind: Podmetadata: name: pod-liveness-tcpsocket namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: tcpSocket: port: 8080 # 尝试访问8080端口 创建pod，观察效果 1234567891011121314151617181920# 创建Pod[root@k8s-master01 ~]# kubectl create -f pod-liveness-tcpsocket.yamlpod/pod-liveness-tcpsocket created# 查看Pod详情[root@k8s-master01 ~]# kubectl describe pods pod-liveness-tcpsocket -n dev...... Normal Scheduled 31s default-scheduler Successfully assigned dev/pod-liveness-tcpsocket to node2 Normal Pulled &lt;invalid&gt; kubelet, node2 Container image &quot;nginx:1.17.1&quot; already present on machine Normal Created &lt;invalid&gt; kubelet, node2 Created container nginx Normal Started &lt;invalid&gt; kubelet, node2 Started container nginx Warning Unhealthy &lt;invalid&gt; (x2 over &lt;invalid&gt;) kubelet, node2 Liveness probe failed: dial tcp 10.244.2.44:8080: connect: connection refused # 观察上面的信息，发现尝试访问8080端口,但是失败了# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长[root@k8s-master01 ~]# kubectl get pods pod-liveness-tcpsocket -n devNAME READY STATUS RESTARTS AGEpod-liveness-tcpsocket 0/1 CrashLoopBackOff 2 3m19s# 当然接下来，可以修改成一个可以访问的端口，比如80，再试，结果就正常了...... 方式三：HTTPGet 创建pod-liveness-httpget.yaml 1234567891011121314151617apiVersion: v1kind: Podmetadata: name: pod-liveness-httpget namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: httpGet: # 其实就是访问http://127.0.0.1:80/hello scheme: HTTP #支持的协议，http或者https port: 80 #端口号 path: /hello #URI地址 创建pod，观察效果 1234567891011121314151617181920# 创建Pod[root@k8s-master01 ~]# kubectl create -f pod-liveness-httpget.yamlpod/pod-liveness-httpget created# 查看Pod详情[root@k8s-master01 ~]# kubectl describe pod pod-liveness-httpget -n dev....... Normal Pulled 6s (x3 over 64s) kubelet, node1 Container image &quot;nginx:1.17.1&quot; already present on machine Normal Created 6s (x3 over 64s) kubelet, node1 Created container nginx Normal Started 6s (x3 over 63s) kubelet, node1 Started container nginx Warning Unhealthy 6s (x6 over 56s) kubelet, node1 Liveness probe failed: HTTP probe failed with statuscode: 404 Normal Killing 6s (x2 over 36s) kubelet, node1 Container nginx failed liveness probe, will be restarted # 观察上面信息，尝试访问路径，但是未找到,出现404错误# 稍等一会之后，再观察pod信息，就可以看到RESTARTS不再是0，而是一直增长[root@k8s-master01 ~]# kubectl get pod pod-liveness-httpget -n devNAME READY STATUS RESTARTS AGEpod-liveness-httpget 1/1 Running 5 3m17s# 当然接下来，可以修改成一个可以访问的路径path，比如/，再试，结果就正常了...... 至此，已经使用liveness Probe演示了三种探测方式，但是查看livenessProbe的子属性，会发现除了这三种方式，还有一些其他的配置，在这里一并解释下： 12345678910[root@k8s-master01 ~]# kubectl explain pod.spec.containers.livenessProbeFIELDS: exec &lt;Object&gt; tcpSocket &lt;Object&gt; httpGet &lt;Object&gt; initialDelaySeconds &lt;integer&gt; # 容器启动后等待多少秒执行第一次探测 timeoutSeconds &lt;integer&gt; # 探测超时时间。默认1秒，最小1秒 periodSeconds &lt;integer&gt; # 执行探测的频率。默认是10秒，最小1秒 failureThreshold &lt;integer&gt; # 连续探测失败多少次才被认定为失败。默认是3。最小值是1 successThreshold &lt;integer&gt; # 连续探测成功多少次才被认定为成功。默认是1 下面稍微配置两个，演示下效果即可： 1234567891011121314151617181920[root@k8s-master01 ~]# more pod-liveness-httpget.yamlapiVersion: v1kind: Podmetadata: name: pod-liveness-httpget namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: httpGet: scheme: HTTP port: 80 path: / initialDelaySeconds: 30 # 容器启动后30s开始探测 timeoutSeconds: 5 # 探测超时时间为5s 5.3.5 重启策略在上一节中，一旦容器探测出现了问题，kubernetes就会对容器所在的Pod进行重启，其实这是由pod的重启策略决定的，pod的重启策略有 3 种，分别如下： Always ：容器失效时，自动重启该容器，这也是默认值。 OnFailure ： 容器终止运行且退出码不为0时重启 Never ： 不论状态为何，都不重启该容器 重启策略适用于pod对象中的所有容器，首次需要重启的容器，将在其需要时立即进行重启，随后再次需要重启的操作将由kubelet延迟一段时间后进行，且反复的重启操作的延迟时长以此为10s、20s、40s、80s、160s和300s，300s是最大延迟时长。 创建pod-restartpolicy.yaml： 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: pod-restartpolicy namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: - name: nginx-port containerPort: 80 livenessProbe: httpGet: scheme: HTTP port: 80 path: /hello restartPolicy: Never # 设置重启策略为Never 运行Pod测试 1234567891011121314# 创建Pod[root@k8s-master01 ~]# kubectl create -f pod-restartpolicy.yamlpod/pod-restartpolicy created# 查看Pod详情，发现nginx容器失败[root@k8s-master01 ~]# kubectl describe pods pod-restartpolicy -n dev...... Warning Unhealthy 15s (x3 over 35s) kubelet, node1 Liveness probe failed: HTTP probe failed with statuscode: 404 Normal Killing 15s kubelet, node1 Container nginx failed liveness probe # 多等一会，再观察pod的重启次数，发现一直是0，并未重启 [root@k8s-master01 ~]# kubectl get pods pod-restartpolicy -n devNAME READY STATUS RESTARTS AGEpod-restartpolicy 0/1 Running 0 5min42s 5.4 Pod调度在默认情况下，一个Pod在哪个Node节点上运行，是由Scheduler组件采用相应的算法计算出来的，这个过程是不受人工控制的。但是在实际使用中，这并不满足的需求，因为很多情况下，我们想控制某些Pod到达某些节点上，那么应该怎么做呢？这就要求了解kubernetes对Pod的调度规则，kubernetes提供了四大类调度方式： 自动调度：运行在哪个节点上完全由Scheduler经过一系列的算法计算得出 定向调度：NodeName、NodeSelector 亲和性调度：NodeAffinity、PodAffinity、PodAntiAffinity 污点（容忍）调度：Taints、Toleration 5.4.1 定向调度定向调度，指的是利用在pod上声明nodeName或者nodeSelector，以此将Pod调度到期望的node节点上。注意，这里的调度是强制的，这就意味着即使要调度的目标Node不存在，也会向上面进行调度，只不过pod运行失败而已。 NodeName NodeName用于强制约束将Pod调度到指定的Name的Node节点上。这种方式，其实是直接跳过Scheduler的调度逻辑，直接将Pod调度到指定名称的节点。 接下来，实验一下：创建一个pod-nodename.yaml文件 12345678910apiVersion: v1kind: Podmetadata: name: pod-nodename namespace: devspec: containers: - name: nginx image: nginx:1.17.1 nodeName: node1 # 指定调度到node1节点上 1234567891011121314151617181920#创建Pod[root@k8s-master01 ~]# kubectl create -f pod-nodename.yamlpod/pod-nodename created#查看Pod调度到NODE属性，确实是调度到了node1节点上[root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE ......pod-nodename 1/1 Running 0 56s 10.244.1.87 node1 ...... # 接下来，删除pod，修改nodeName的值为node3（并没有node3节点）[root@k8s-master01 ~]# kubectl delete -f pod-nodename.yamlpod &quot;pod-nodename&quot; deleted[root@k8s-master01 ~]# vim pod-nodename.yaml[root@k8s-master01 ~]# kubectl create -f pod-nodename.yamlpod/pod-nodename created#再次查看，发现已经向Node3节点调度，但是由于不存在node3节点，所以pod无法正常运行[root@k8s-master01 ~]# kubectl get pods pod-nodename -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE ......pod-nodename 0/1 Pending 0 6s &lt;none&gt; node3 ...... NodeSelector NodeSelector用于将pod调度到添加了指定标签的node节点上。它是通过kubernetes的label-selector机制实现的，也就是说，在pod创建之前，会由scheduler使用MatchNodeSelector调度策略进行label匹配，找出目标node，然后将pod调度到目标节点，该匹配规则是强制约束。 接下来，实验一下： 1 首先分别为node节点添加标签 1234[root@k8s-master01 ~]# kubectl label nodes node1 nodeenv=pronode/node2 labeled[root@k8s-master01 ~]# kubectl label nodes node2 nodeenv=testnode/node2 labeled 2 创建一个pod-nodeselector.yaml文件，并使用它创建Pod 1234567891011apiVersion: v1kind: Podmetadata: name: pod-nodeselector namespace: devspec: containers: - name: nginx image: nginx:1.17.1 nodeSelector: nodeenv: pro # 指定调度到具有nodeenv=pro标签的节点上 12345678910111213141516171819202122232425262728#创建Pod[root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yamlpod/pod-nodeselector created#查看Pod调度到NODE属性，确实是调度到了node1节点上[root@k8s-master01 ~]# kubectl get pods pod-nodeselector -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE ......pod-nodeselector 1/1 Running 0 47s 10.244.1.87 node1 ......# 接下来，删除pod，修改nodeSelector的值为nodeenv: xxxx（不存在打有此标签的节点）[root@k8s-master01 ~]# kubectl delete -f pod-nodeselector.yamlpod &quot;pod-nodeselector&quot; deleted[root@k8s-master01 ~]# vim pod-nodeselector.yaml[root@k8s-master01 ~]# kubectl create -f pod-nodeselector.yamlpod/pod-nodeselector created#再次查看，发现pod无法正常运行,Node的值为none[root@k8s-master01 ~]# kubectl get pods -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE pod-nodeselector 0/1 Pending 0 2m20s &lt;none&gt; &lt;none&gt;# 查看详情,发现node selector匹配失败的提示[root@k8s-master01 ~]# kubectl describe pods pod-nodeselector -n dev.......Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling &lt;unknown&gt; default-scheduler 0/3 nodes are available: 3 node(s) didn&#x27;t match node selector. 5.4.2 亲和性调度上一节，介绍了两种定向调度的方式，使用起来非常方便，但是也有一定的问题，那就是如果没有满足条件的Node，那么Pod将不会被运行，即使在集群中还有可用Node列表也不行，这就限制了它的使用场景。 基于上面的问题，kubernetes还提供了一种亲和性调度（Affinity）。它在NodeSelector的基础之上的进行了扩展，可以通过配置的形式，实现优先选择满足条件的Node进行调度，如果没有，也可以调度到不满足条件的节点上，使调度更加灵活。 Affinity主要分为三类： nodeAffinity(node亲和性）: 以node为目标，解决pod可以调度到哪些node的问题 podAffinity(pod亲和性) : 以pod为目标，解决pod可以和哪些已存在的pod部署在同一个拓扑域中的问题 podAntiAffinity(pod反亲和性) : 以pod为目标，解决pod不能和哪些已存在pod部署在同一个拓扑域中的问题 关于亲和性(反亲和性)使用场景的说明： 亲和性：如果两个应用频繁交互，那就有必要利用亲和性让两个应用的尽可能的靠近，这样可以减少因网络通信而带来的性能损耗。 反亲和性：当应用的采用多副本部署时，有必要采用反亲和性让各个应用实例打散分布在各个node上，这样可以提高服务的高可用性。 NodeAffinity 首先来看一下NodeAffinity的可配置项： 12345678910111213141516pod.spec.affinity.nodeAffinity requiredDuringSchedulingIgnoredDuringExecution Node节点必须满足指定的所有规则才可以，相当于硬限制 nodeSelectorTerms 节点选择列表 matchFields 按节点字段列出的节点选择器要求列表 matchExpressions 按节点标签列出的节点选择器要求列表(推荐) key 键 values 值 operat or 关系符 支持Exists, DoesNotExist, In, NotIn, Gt, Lt preferredDuringSchedulingIgnoredDuringExecution 优先调度到满足指定的规则的Node，相当于软限制 (倾向) preference 一个节点选择器项，与相应的权重相关联 matchFields 按节点字段列出的节点选择器要求列表 matchExpressions 按节点标签列出的节点选择器要求列表(推荐) key 键 values 值 operator 关系符 支持In, NotIn, Exists, DoesNotExist, Gt, Lt weight 倾向权重，在范围1-100。 1234567891011关系符的使用说明:- matchExpressions: - key: nodeenv # 匹配存在标签的key为nodeenv的节点 operator: Exists - key: nodeenv # 匹配标签的key为nodeenv,且value是&quot;xxx&quot;或&quot;yyy&quot;的节点 operator: In values: [&quot;xxx&quot;,&quot;yyy&quot;] - key: nodeenv # 匹配标签的key为nodeenv,且value大于&quot;xxx&quot;的节点 operator: Gt values: &quot;xxx&quot; 接下来首先演示一下requiredDuringSchedulingIgnoredDuringExecution , 创建pod-nodeaffinity-required.yaml 1234567891011121314151617apiVersion: v1kind: Podmetadata: name: pod-nodeaffinity-required namespace: devspec: containers: - name: nginx image: nginx:1.17.1 affinity: #亲和性设置 nodeAffinity: #设置node亲和性 requiredDuringSchedulingIgnoredDuringExecution: # 硬限制 nodeSelectorTerms: - matchExpressions: # 匹配env的值在[&quot;xxx&quot;,&quot;yyy&quot;]中的标签 - key: nodeenv operator: In values: [&quot;xxx&quot;,&quot;yyy&quot;] 12345678910111213141516171819202122232425262728293031# 创建pod[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yamlpod/pod-nodeaffinity-required created# 查看pod状态 （运行失败）[root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE ...... pod-nodeaffinity-required 0/1 Pending 0 16s &lt;none&gt; &lt;none&gt; ......# 查看Pod的详情# 发现调度失败，提示node选择失败[root@k8s-master01 ~]# kubectl describe pod pod-nodeaffinity-required -n dev...... Warning FailedScheduling &lt;unknown&gt; default-scheduler 0/3 nodes are available: 3 node(s) didn&#x27;t match node selector. Warning FailedScheduling &lt;unknown&gt; default-scheduler 0/3 nodes are available: 3 node(s) didn&#x27;t match node selector.#接下来，停止pod[root@k8s-master01 ~]# kubectl delete -f pod-nodeaffinity-required.yamlpod &quot;pod-nodeaffinity-required&quot; deleted# 修改文件，将values: [&quot;xxx&quot;,&quot;yyy&quot;]------&gt; [&quot;pro&quot;,&quot;yyy&quot;][root@k8s-master01 ~]# vim pod-nodeaffinity-required.yaml# 再次启动[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-required.yamlpod/pod-nodeaffinity-required created# 此时查看，发现调度成功，已经将pod调度到了node1上[root@k8s-master01 ~]# kubectl get pods pod-nodeaffinity-required -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE ...... pod-nodeaffinity-required 1/1 Running 0 11s 10.244.1.89 node1 ...... 接下来再演示一下requiredDuringSchedulingIgnoredDuringExecution , 创建pod-nodeaffinity-preferred.yaml 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: pod-nodeaffinity-preferred namespace: devspec: containers: - name: nginx image: nginx:1.17.1 affinity: #亲和性设置 nodeAffinity: #设置node亲和性 preferredDuringSchedulingIgnoredDuringExecution: # 软限制 - weight: 1 preference: matchExpressions: # 匹配env的值在[&quot;xxx&quot;,&quot;yyy&quot;]中的标签(当前环境没有) - key: nodeenv operator: In values: [&quot;xxx&quot;,&quot;yyy&quot;] 12345678# 创建pod[root@k8s-master01 ~]# kubectl create -f pod-nodeaffinity-preferred.yamlpod/pod-nodeaffinity-preferred created# 查看pod状态 （运行成功）[root@k8s-master01 ~]# kubectl get pod pod-nodeaffinity-preferred -n devNAME READY STATUS RESTARTS AGEpod-nodeaffinity-preferred 1/1 Running 0 40s 12345NodeAffinity规则设置的注意事项： 1 如果同时定义了nodeSelector和nodeAffinity，那么必须两个条件都得到满足，Pod才能运行在指定的Node上 2 如果nodeAffinity指定了多个nodeSelectorTerms，那么只需要其中一个能够匹配成功即可 3 如果一个nodeSelectorTerms中有多个matchExpressions ，则一个节点必须满足所有的才能匹配成功 4 如果一个pod所在的Node在Pod运行期间其标签发生了改变，不再符合该Pod的节点亲和性需求，则系统将忽略此变化 PodAffinity PodAffinity主要实现以运行的Pod为参照，实现让新创建的Pod跟参照pod在一个区域的功能。 首先来看一下PodAffinity的可配置项： 123456789101112131415161718192021pod.spec.affinity.podAffinity requiredDuringSchedulingIgnoredDuringExecution 硬限制 namespaces 指定参照pod的namespace topologyKey 指定调度作用域 labelSelector 标签选择器 matchExpressions 按节点标签列出的节点选择器要求列表(推荐) key 键 values 值 operator 关系符 支持In, NotIn, Exists, DoesNotExist. matchLabels 指多个matchExpressions映射的内容 preferredDuringSchedulingIgnoredDuringExecution 软限制 podAffinityTerm 选项 namespaces topologyKey labelSelector matchExpressions key 键 values 值 operator matchLabels weight 倾向权重，在范围1-100 123topologyKey用于指定调度时作用域,例如: 如果指定为kubernetes.io/hostname，那就是以Node节点为区分范围 如果指定为beta.kubernetes.io/os,则以Node节点的操作系统类型来区分 接下来，演示下requiredDuringSchedulingIgnoredDuringExecution, 1）首先创建一个参照Pod，pod-podaffinity-target.yaml： 123456789101112apiVersion: v1kind: Podmetadata: name: pod-podaffinity-target namespace: dev labels: podenv: pro #设置标签spec: containers: - name: nginx image: nginx:1.17.1 nodeName: node1 # 将目标pod名确指定到node1上 12345678# 启动目标pod[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-target.yamlpod/pod-podaffinity-target created# 查看pod状况[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-target -n devNAME READY STATUS RESTARTS AGEpod-podaffinity-target 1/1 Running 0 4s 2）创建pod-podaffinity-required.yaml，内容如下： 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: pod-podaffinity-required namespace: devspec: containers: - name: nginx image: nginx:1.17.1 affinity: #亲和性设置 podAffinity: #设置pod亲和性 requiredDuringSchedulingIgnoredDuringExecution: # 硬限制 - labelSelector: matchExpressions: # 匹配env的值在[&quot;xxx&quot;,&quot;yyy&quot;]中的标签 - key: podenv operator: In values: [&quot;xxx&quot;,&quot;yyy&quot;] topologyKey: kubernetes.io/hostname 上面配置表达的意思是：新Pod必须要与拥有标签nodeenv=xxx或者nodeenv=yyy的pod在同一Node上，显然现在没有这样pod，接下来，运行测试一下。 12345678910111213141516171819202122232425262728293031# 启动pod[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yamlpod/pod-podaffinity-required created# 查看pod状态，发现未运行[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n devNAME READY STATUS RESTARTS AGEpod-podaffinity-required 0/1 Pending 0 9s# 查看详细信息[root@k8s-master01 ~]# kubectl describe pods pod-podaffinity-required -n dev......Events: Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling &lt;unknown&gt; default-scheduler 0/3 nodes are available: 2 node(s) didn&#x27;t match pod affinity rules, 1 node(s) had taints that the pod didn&#x27;t tolerate.# 接下来修改 values: [&quot;xxx&quot;,&quot;yyy&quot;]-----&gt;values:[&quot;pro&quot;,&quot;yyy&quot;]# 意思是：新Pod必须要与拥有标签nodeenv=xxx或者nodeenv=yyy的pod在同一Node上[root@k8s-master01 ~]# vim pod-podaffinity-required.yaml# 然后重新创建pod，查看效果[root@k8s-master01 ~]# kubectl delete -f pod-podaffinity-required.yamlpod &quot;pod-podaffinity-required&quot; de leted[root@k8s-master01 ~]# kubectl create -f pod-podaffinity-required.yamlpod/pod-podaffinity-required created# 发现此时Pod运行正常[root@k8s-master01 ~]# kubectl get pods pod-podaffinity-required -n devNAME READY STATUS RESTARTS AGE LABELSpod-podaffinity-required 1/1 Running 0 6s &lt;none&gt; 关于PodAffinity的 preferredDuringSchedulingIgnoredDuringExecution，这里不再演示。 PodAntiAffinity PodAntiAffinity主要实现以运行的Pod为参照，让新创建的Pod跟参照pod不在一个区域中的功能。 它的配置方式和选项跟PodAffinty是一样的，这里不再做详细解释，直接做一个测试案例。 1）继续使用上个案例中目标pod 1234[root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labelsNAME READY STATUS RESTARTS AGE IP NODE LABELSpod-podaffinity-required 1/1 Running 0 3m29s 10.244.1.38 node1 &lt;none&gt; pod-podaffinity-target 1/1 Running 0 9m25s 10.244.1.37 node1 podenv=pro 2）创建pod-podantiaffinity-required.yaml，内容如下： 123456789101112131415161718apiVersion: v1kind: Podmetadata: name: pod-podantiaffinity-required namespace: devspec: containers: - name: nginx image: nginx:1.17.1 affinity: #亲和性设置 podAntiAffinity: #设置pod亲和性 requiredDuringSchedulingIgnoredDuringExecution: # 硬限制 - labelSelector: matchExpressions: # 匹配podenv的值在[&quot;pro&quot;]中的标签 - key: podenv operator: In values: [&quot;pro&quot;] topologyKey: kubernetes.io/hostname 上面配置表达的意思是：新Pod必须要与拥有标签nodeenv=pro的pod不在同一Node上，运行测试一下。 123456789# 创建pod[root@k8s-master01 ~]# kubectl create -f pod-podantiaffinity-required.yamlpod/pod-podantiaffinity-required created# 查看pod# 发现调度到了node2上[root@k8s-master01 ~]# kubectl get pods pod-podantiaffinity-required -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE .. pod-podantiaffinity-required 1/1 Running 0 30s 10.244.1.96 node2 .. 5.4.3 污点和容忍污点（Taints） 前面的调度方式都是站在Pod的角度上，通过在Pod上添加属性，来确定Pod是否要调度到指定的Node上，其实我们也可以站在Node的角度上，通过在Node上添加污点属性，来决定是否允许Pod调度过来。 Node被设置上污点之后就和Pod之间存在了一种相斥的关系，进而拒绝Pod调度进来，甚至可以将已经存在的Pod驱逐出去。 污点的格式为：key=value:effect, key和value是污点的标签，effect描述污点的作用，支持如下三个选项： PreferNoSchedule：kubernetes将尽量避免把Pod调度到具有该污点的Node上，除非没有其他节点可调度 NoSchedule：kubernetes将不会把Pod调度到具有该污点的Node上，但不会影响当前Node上已存在的Pod NoExecute：kubernetes将不会把Pod调度到具有该污点的Node上，同时也会将Node上已存在的Pod驱离 使用kubectl设置和去除污点的命令示例如下： 12345678# 设置污点kubectl taint nodes node1 key=value:effect# 去除污点kubectl taint nodes node1 key:effect-# 去除所有污点kubectl taint nodes node1 key- 接下来，演示下污点的效果： 准备节点node1（为了演示效果更加明显，暂时停止node2节点） 为node1节点设置一个污点: tag=heima:PreferNoSchedule；然后创建pod1( pod1 可以 ) 修改为node1节点设置一个污点: tag=heima:NoSchedule；然后创建pod2( pod1 正常 pod2 失败 ) 修改为node1节点设置一个污点: tag=heima:NoExecute；然后创建pod3 ( 3个pod都失败 ) 12345678910111213141516171819202122232425262728293031# 为node1设置污点(PreferNoSchedule)[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:PreferNoSchedule# 创建pod1[root@k8s-master01 ~]# kubectl run taint1 --image=nginx:1.17.1 -n dev[root@k8s-master01 ~]# kubectl get pods -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE taint1-7665f7fd85-574h4 1/1 Running 0 2m24s 10.244.1.59 node1 # 为node1设置污点(取消PreferNoSchedule，设置NoSchedule)[root@k8s-master01 ~]# kubectl taint nodes node1 tag:PreferNoSchedule-[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:NoSchedule# 创建pod2[root@k8s-master01 ~]# kubectl run taint2 --image=nginx:1.17.1 -n dev[root@k8s-master01 ~]# kubectl get pods taint2 -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODEtaint1-7665f7fd85-574h4 1/1 Running 0 2m24s 10.244.1.59 node1 taint2-544694789-6zmlf 0/1 Pending 0 21s &lt;none&gt; &lt;none&gt; # 为node1设置污点(取消NoSchedule，设置NoExecute)[root@k8s-master01 ~]# kubectl taint nodes node1 tag:NoSchedule-[root@k8s-master01 ~]# kubectl taint nodes node1 tag=heima:NoExecute# 创建pod3[root@k8s-master01 ~]# kubectl run taint3 --image=nginx:1.17.1 -n dev[root@k8s-master01 ~]# kubectl get pods -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED taint1-7665f7fd85-htkmp 0/1 Pending 0 35s &lt;none&gt; &lt;none&gt; &lt;none&gt; taint2-544694789-bn7wb 0/1 Pending 0 35s &lt;none&gt; &lt;none&gt; &lt;none&gt; taint3-6d78dbd749-tktkq 0/1 Pending 0 6s &lt;none&gt; &lt;none&gt; &lt;none&gt; 12小提示： 使用kubeadm搭建的集群，默认就会给master节点添加一个污点标记,所以pod就不会调度到master节点上. 容忍（Toleration） 上面介绍了污点的作用，我们可以在node上添加污点用于拒绝pod调度上来，但是如果就是想将一个pod调度到一个有污点的node上去，这时候应该怎么做呢？这就要使用到容忍。 污点就是拒绝，容忍就是忽略，Node通过污点拒绝pod调度上去，Pod通过容忍忽略拒绝 下面先通过一个案例看下效果： 上一小节，已经在node1节点上打上了NoExecute的污点，此时pod是调度不上去的 本小节，可以通过给pod添加容忍，然后将其调度上去 创建pod-toleration.yaml,内容如下 1234567891011121314apiVersion: v1kind: Podmetadata: name: pod-toleration namespace: devspec: containers: - name: nginx image: nginx:1.17.1 tolerations: # 添加容忍 - key: &quot;tag&quot; # 要容忍的污点的key operator: &quot;Equal&quot; # 操作符 value: &quot;heima&quot; # 容忍的污点的value effect: &quot;NoExecute&quot; # 添加容忍的规则，这里必须和标记的污点规则相同 123456789# 添加容忍之前的pod[root@k8s-master01 ~]# kubectl get pods -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATED pod-toleration 0/1 Pending 0 3s &lt;none&gt; &lt;none&gt; &lt;none&gt; # 添加容忍之后的pod[root@k8s-master01 ~]# kubectl get pods -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE NOMINATEDpod-toleration 1/1 Running 0 3s 10.244.1.62 node1 &lt;none&gt; 下面看一下容忍的详细配置: 12345678[root@k8s-master01 ~]# kubectl explain pod.spec.tolerations......FIELDS: key # 对应着要容忍的污点的键，空意味着匹配所有的键 value # 对应着要容忍的污点的值 operator # key-value的运算符，支持Equal和Exists（默认） effect # 对应污点的effect，空意味着匹配所有影响 tolerationSeconds # 容忍时间, 当effect为NoExecute时生效，表示pod在Node上的停留时间 6. Pod控制器详解6.1 Pod控制器介绍Pod是kubernetes的最小管理单元，在kubernetes中，按照pod的创建方式可以将其分为两类： 自主式pod：kubernetes直接创建出来的Pod，这种pod删除后就没有了，也不会重建 控制器创建的pod：kubernetes通过控制器创建的pod，这种pod删除了之后还会自动重建 什么是Pod控制器 Pod控制器是管理pod的中间层，使用Pod控制器之后，只需要告诉Pod控制器，想要多少个什么样的Pod就可以了，它会创建出满足条件的Pod并确保每一个Pod资源处于用户期望的目标状态。如果Pod资源在运行中出现故障，它会基于指定策略重新编排Pod。 在kubernetes中，有很多类型的pod控制器，每种都有自己的适合的场景，常见的有下面这些： ReplicationController：比较原始的pod控制器，已经被废弃，由ReplicaSet替代 ReplicaSet：保证副本数量一直维持在期望值，并支持pod数量扩缩容，镜像版本升级 Deployment：通过控制ReplicaSet来控制Pod，并支持滚动升级、回退版本 Horizontal Pod Autoscaler：可以根据集群负载自动水平调整Pod的数量，实现削峰填谷 DaemonSet：在集群中的指定Node上运行且仅运行一个副本，一般用于守护进程类的任务 Job：它创建出来的pod只要完成任务就立即退出，不需要重启或重建，用于执行一次性任务 Cronjob：它创建的Pod负责周期性任务控制，不需要持续后台运行 StatefulSet：管理有状态应用 6.2 ReplicaSet(RS)ReplicaSet的主要作用是保证一定数量的pod正常运行，它会持续监听这些Pod的运行状态，一旦Pod发生故障，就会重启或重建。同时它还支持对pod数量的扩缩容和镜像版本的升降级。 ReplicaSet的资源清单文件： 123456789101112131415161718192021222324apiVersion: apps/v1 # 版本号kind: ReplicaSet # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: rsspec: # 详情描述 replicas: 3 # 副本数量 selector: # 选择器，通过它指定该控制器管理哪些pod matchLabels: # Labels匹配规则 app: nginx-pod matchExpressions: # Expressions匹配规则 - &#123;key: app, operator: In, values: [nginx-pod]&#125; template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 在这里面，需要新了解的配置项就是spec下面几个选项： replicas：指定副本数量，其实就是当前rs创建出来的pod的数量，默认为1 selector：选择器，它的作用是建立pod控制器和pod之间的关联关系，采用的Label Selector机制 在pod模板上定义label，在控制器上定义选择器，就可以表明当前控制器能管理哪些pod了 template：模板，就是当前控制器创建pod所使用的模板板，里面其实就是前一章学过的pod的定义 创建ReplicaSet 创建pc-replicaset.yaml文件，内容如下： 123456789101112131415161718apiVersion: apps/v1kind: ReplicaSet metadata: name: pc-replicaset namespace: devspec: replicas: 3 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 12345678910111213141516171819# 创建rs[root@k8s-master01 ~]# kubectl create -f pc-replicaset.yamlreplicaset.apps/pc-replicaset created# 查看rs# DESIRED:期望副本数量 # CURRENT:当前副本数量 # READY:已经准备好提供服务的副本数量[root@k8s-master01 ~]# kubectl get rs pc-replicaset -n dev -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES SELECTORpc-replicaset 3 3 3 22s nginx nginx:1.17.1 app=nginx-pod# 查看当前控制器创建出来的pod# 这里发现控制器创建出来的pod的名称是在控制器名称后面拼接了-xxxxx随机码[root@k8s-master01 ~]# kubectl get pod -n devNAME READY STATUS RESTARTS AGEpc-replicaset-6vmvt 1/1 Running 0 54spc-replicaset-fmb8f 1/1 Running 0 54spc-replicaset-snrk2 1/1 Running 0 54s 扩缩容 12345678910111213141516171819202122232425262728293031323334# 编辑rs的副本数量，修改spec:replicas: 6即可[root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n devreplicaset.apps/pc-replicaset edited# 查看pod[root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-replicaset-6vmvt 1/1 Running 0 114mpc-replicaset-cftnp 1/1 Running 0 10spc-replicaset-fjlm6 1/1 Running 0 10spc-replicaset-fmb8f 1/1 Running 0 114mpc-replicaset-s2whj 1/1 Running 0 10spc-replicaset-snrk2 1/1 Running 0 114m# 当然也可以直接使用命令实现# 使用scale命令实现扩缩容， 后面--replicas=n直接指定目标数量即可[root@k8s-master01 ~]# kubectl scale rs pc-replicaset --replicas=2 -n devreplicaset.apps/pc-replicaset scaled# 命令运行完毕，立即查看，发现已经有4个开始准备退出了[root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-replicaset-6vmvt 0/1 Terminating 0 118mpc-replicaset-cftnp 0/1 Terminating 0 4m17spc-replicaset-fjlm6 0/1 Terminating 0 4m17spc-replicaset-fmb8f 1/1 Running 0 118mpc-replicaset-s2whj 0/1 Terminating 0 4m17spc-replicaset-snrk2 1/1 Running 0 118m#稍等片刻，就只剩下2个了[root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-replicaset-fmb8f 1/1 Running 0 119mpc-replicaset-snrk2 1/1 Running 0 119m 镜像升级 123456789101112131415161718# 编辑rs的容器镜像 - image: nginx:1.17.2[root@k8s-master01 ~]# kubectl edit rs pc-replicaset -n devreplicaset.apps/pc-replicaset edited# 再次查看，发现镜像版本已经变更了[root@k8s-master01 ~]# kubectl get rs -n dev -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES ...pc-replicaset 2 2 2 140m nginx nginx:1.17.2 ...# 同样的道理，也可以使用命令完成这个工作# kubectl set image rs rs名称 容器=镜像版本 -n namespace[root@k8s-master01 ~]# kubectl set image rs pc-replicaset nginx=nginx:1.17.1 -n devreplicaset.apps/pc-replicaset image updated# 再次查看，发现镜像版本已经变更了[root@k8s-master01 ~]# kubectl get rs -n dev -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES ...pc-replicaset 2 2 2 145m nginx nginx:1.17.1 ... 删除ReplicaSet 123456789101112131415161718# 使用kubectl delete命令会删除此RS以及它管理的Pod# 在kubernetes删除RS前，会将RS的replicasclear调整为0，等待所有的Pod被删除后，在执行RS对象的删除[root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n devreplicaset.apps &quot;pc-replicaset&quot; deleted[root@k8s-master01 ~]# kubectl get pod -n dev -o wideNo resources found in dev namespace.# 如果希望仅仅删除RS对象（保留Pod），可以使用kubectl delete命令时添加--cascade=false选项（不推荐）。[root@k8s-master01 ~]# kubectl delete rs pc-replicaset -n dev --cascade=falsereplicaset.apps &quot;pc-replicaset&quot; deleted[root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-replicaset-cl82j 1/1 Running 0 75spc-replicaset-dslhb 1/1 Running 0 75s# 也可以使用yaml直接删除(推荐)[root@k8s-master01 ~]# kubectl delete -f pc-replicaset.yamlreplicaset.apps &quot;pc-replicaset&quot; deleted 6.3 Deployment(Deploy)为了更好的解决服务编排的问题，kubernetes在V1.2版本开始，引入了Deployment控制器。值得一提的是，这种控制器并不直接管理pod，而是通过管理ReplicaSet来简介管理Pod，即：Deployment管理ReplicaSet，ReplicaSet管理Pod。所以Deployment比ReplicaSet功能更加强大。 Deployment主要功能有下面几个： 支持ReplicaSet的所有功能 支持发布的停止、继续 支持滚动升级和回滚版本 Deployment的资源清单文件： 1234567891011121314151617181920212223242526272829303132apiVersion: apps/v1 # 版本号kind: Deployment # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: deployspec: # 详情描述 replicas: 3 # 副本数量 revisionHistoryLimit: 3 # 保留历史版本 paused: false # 暂停部署，默认是false progressDeadlineSeconds: 600 # 部署超时时间（s），默认是600 strategy: # 策略 type: RollingUpdate # 滚动更新策略 rollingUpdate: # 滚动更新 maxSurge: 30% # 最大额外可以存在的副本数，可以为百分比，也可以为整数 maxUnavailable: 30% # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数 selector: # 选择器，通过它指定该控制器管理哪些pod matchLabels: # Labels匹配规则 app: nginx-pod matchExpressions: # Expressions匹配规则 - &#123;key: app, operator: In, values: [nginx-pod]&#125; template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 6.3.1 创建deployment创建pc-deployment.yaml，内容如下： 123456789101112131415161718apiVersion: apps/v1kind: Deployment metadata: name: pc-deployment namespace: devspec: replicas: 3 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 1234567891011121314151617181920212223# 创建deployment[root@k8s-master01 ~]# kubectl create -f pc-deployment.yaml --record=truedeployment.apps/pc-deployment created# 查看deployment# UP-TO-DATE 最新版本的pod的数量# AVAILABLE 当前可用的pod的数量[root@k8s-master01 ~]# kubectl get deploy pc-deployment -n devNAME READY UP-TO-DATE AVAILABLE AGEpc-deployment 3/3 3 3 15s# 查看rs# 发现rs的名称是在原来deployment的名字后面添加了一个10位数的随机串[root@k8s-master01 ~]# kubectl get rs -n devNAME DESIRED CURRENT READY AGEpc-deployment-6696798b78 3 3 3 23s# 查看pod[root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-deployment-6696798b78-d2c8n 1/1 Running 0 107spc-deployment-6696798b78-smpvp 1/1 Running 0 107spc-deployment-6696798b78-wvjd8 1/1 Running 0 107s 6.3.2 扩缩容1234567891011121314151617181920212223242526272829# 变更副本数量为5个[root@k8s-master01 ~]# kubectl scale deploy pc-deployment --replicas=5 -n devdeployment.apps/pc-deployment scaled# 查看deployment[root@k8s-master01 ~]# kubectl get deploy pc-deployment -n devNAME READY UP-TO-DATE AVAILABLE AGEpc-deployment 5/5 5 5 2m# 查看pod[root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-deployment-6696798b78-d2c8n 1/1 Running 0 4m19spc-deployment-6696798b78-jxmdq 1/1 Running 0 94spc-deployment-6696798b78-mktqv 1/1 Running 0 93spc-deployment-6696798b78-smpvp 1/1 Running 0 4m19spc-deployment-6696798b78-wvjd8 1/1 Running 0 4m19s# 编辑deployment的副本数量，修改spec:replicas: 4即可[root@k8s-master01 ~]# kubectl edit deploy pc-deployment -n devdeployment.apps/pc-deployment edited# 查看pod[root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-deployment-6696798b78-d2c8n 1/1 Running 0 5m23spc-deployment-6696798b78-jxmdq 1/1 Running 0 2m38spc-deployment-6696798b78-smpvp 1/1 Running 0 5m23spc-deployment-6696798b78-wvjd8 1/1 Running 0 5m23s 镜像更新 deployment支持两种更新策略:重建更新和滚动更新,可以通过strategy指定策略类型,支持两个属性: 1234567strategy：指定新的Pod替换旧的Pod的策略， 支持两个属性： type：指定策略类型，支持两种策略 Recreate：在创建出新的Pod之前会先杀掉所有已存在的Pod RollingUpdate：滚动更新，就是杀死一部分，就启动一部分，在更新过程中，存在两个版本Pod rollingUpdate：当type为RollingUpdate时生效，用于为RollingUpdate设置参数，支持两个属性： maxUnavailable：用来指定在升级过程中不可用Pod的最大数量，默认为25%。 maxSurge： 用来指定在升级过程中可以超过期望的Pod的最大数量，默认为25%。 重建更新 编辑pc-deployment.yaml,在spec节点下添加更新策略 123spec: strategy: # 策略 type: Recreate # 重建更新 创建deploy进行验证 1234567891011121314151617181920212223242526# 变更镜像[root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.2 -n devdeployment.apps/pc-deployment image updated# 观察升级过程[root@k8s-master01 ~]# kubectl get pods -n dev -wNAME READY STATUS RESTARTS AGEpc-deployment-5d89bdfbf9-65qcw 1/1 Running 0 31spc-deployment-5d89bdfbf9-w5nzv 1/1 Running 0 31spc-deployment-5d89bdfbf9-xpt7w 1/1 Running 0 31spc-deployment-5d89bdfbf9-xpt7w 1/1 Terminating 0 41spc-deployment-5d89bdfbf9-65qcw 1/1 Terminating 0 41spc-deployment-5d89bdfbf9-w5nzv 1/1 Terminating 0 41spc-deployment-675d469f8b-grn8z 0/1 Pending 0 0spc-deployment-675d469f8b-hbl4v 0/1 Pending 0 0spc-deployment-675d469f8b-67nz2 0/1 Pending 0 0spc-deployment-675d469f8b-grn8z 0/1 ContainerCreating 0 0spc-deployment-675d469f8b-hbl4v 0/1 ContainerCreating 0 0spc-deployment-675d469f8b-67nz2 0/1 ContainerCreating 0 0spc-deployment-675d469f8b-grn8z 1/1 Running 0 1spc-deployment-675d469f8b-67nz2 1/1 Running 0 1spc-deployment-675d469f8b-hbl4v 1/1 Running 0 2s 滚动更新 编辑pc-deployment.yaml,在spec节点下添加更新策略 123456spec: strategy: # 策略 type: RollingUpdate # 滚动更新策略 rollingUpdate: maxSurge: 25% maxUnavailable: 25% 创建deploy进行验证 12345678910111213141516171819202122232425262728293031323334# 变更镜像[root@k8s-master01 ~]# kubectl set image deployment pc-deployment nginx=nginx:1.17.3 -n dev deployment.apps/pc-deployment image updated# 观察升级过程[root@k8s-master01 ~]# kubectl get pods -n dev -wNAME READY STATUS RESTARTS AGEpc-deployment-c848d767-8rbzt 1/1 Running 0 31mpc-deployment-c848d767-h4p68 1/1 Running 0 31mpc-deployment-c848d767-hlmz4 1/1 Running 0 31mpc-deployment-c848d767-rrqcn 1/1 Running 0 31mpc-deployment-966bf7f44-226rx 0/1 Pending 0 0spc-deployment-966bf7f44-226rx 0/1 ContainerCreating 0 0spc-deployment-966bf7f44-226rx 1/1 Running 0 1spc-deployment-c848d767-h4p68 0/1 Terminating 0 34mpc-deployment-966bf7f44-cnd44 0/1 Pending 0 0spc-deployment-966bf7f44-cnd44 0/1 ContainerCreating 0 0spc-deployment-966bf7f44-cnd44 1/1 Running 0 2spc-deployment-c848d767-hlmz4 0/1 Terminating 0 34mpc-deployment-966bf7f44-px48p 0/1 Pending 0 0spc-deployment-966bf7f44-px48p 0/1 ContainerCreating 0 0spc-deployment-966bf7f44-px48p 1/1 Running 0 0spc-deployment-c848d767-8rbzt 0/1 Terminating 0 34mpc-deployment-966bf7f44-dkmqp 0/1 Pending 0 0spc-deployment-966bf7f44-dkmqp 0/1 ContainerCreating 0 0spc-deployment-966bf7f44-dkmqp 1/1 Running 0 2spc-deployment-c848d767-rrqcn 0/1 Terminating 0 34m# 至此，新版本的pod创建完毕，就版本的pod销毁完毕# 中间过程是滚动进行的，也就是边销毁边创建 滚动更新的过程： 镜像更新中rs的变化 1234567# 查看rs,发现原来的rs的依旧存在，只是pod数量变为了0，而后又新产生了一个rs，pod数量为4# 其实这就是deployment能够进行版本回退的奥妙所在，后面会详细解释[root@k8s-master01 ~]# kubectl get rs -n devNAME DESIRED CURRENT READY AGEpc-deployment-6696798b78 0 0 0 7m37spc-deployment-6696798b11 0 0 0 5m37spc-deployment-c848d76789 4 4 4 72s 6.3.3 版本回退deployment支持版本升级过程中的暂停、继续功能以及版本回退等诸多功能，下面具体来看. kubectl rollout： 版本升级相关功能，支持下面的选项： status 显示当前升级状态 history 显示 升级历史记录 pause 暂停版本升级过程 resume 继续已经暂停的版本升级过程 restart 重启版本升级过程 undo 回滚到上一级版本（可以使用–to-revision回滚到指定版本） 12345678910111213141516171819202122232425262728293031# 查看当前升级版本的状态[root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n devdeployment &quot;pc-deployment&quot; successfully rolled out# 查看升级历史记录[root@k8s-master01 ~]# kubectl rollout history deploy pc-deployment -n devdeployment.apps/pc-deploymentREVISION CHANGE-CAUSE1 kubectl create --filename=pc-deployment.yaml --record=true2 kubectl create --filename=pc-deployment.yaml --record=true3 kubectl create --filename=pc-deployment.yaml --record=true# 可以发现有三次版本记录，说明完成过两次升级# 版本回滚# 这里直接使用--to-revision=1回滚到了1版本， 如果省略这个选项，就是回退到上个版本，就是2版本[root@k8s-master01 ~]# kubectl rollout undo deployment pc-deployment --to-revision=1 -n devdeployment.apps/pc-deployment rolled back# 查看发现，通过nginx镜像版本可以发现到了第一版[root@k8s-master01 ~]# kubectl get deploy -n dev -o wideNAME READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES pc-deployment 4/4 4 4 74m nginx nginx:1.17.1 # 查看rs，发现第一个rs中有4个pod运行，后面两个版本的rs中pod为运行# 其实deployment之所以可是实现版本的回滚，就是通过记录下历史rs来实现的，# 一旦想回滚到哪个版本，只需要将当前版本pod数量降为0，然后将回滚版本的pod提升为目标数量就可以了[root@k8s-master01 ~]# kubectl get rs -n devNAME DESIRED CURRENT READY AGEpc-deployment-6696798b78 4 4 4 78mpc-deployment-966bf7f44 0 0 0 37mpc-deployment-c848d767 0 0 0 71m 6.3.4 金丝雀发布Deployment控制器支持控制更新过程中的控制，如“暂停(pause)”或“继续(resume)”更新操作。 比如有一批新的Pod资源创建完成后立即暂停更新过程，此时，仅存在一部分新版本的应用，主体部分还是旧的版本。然后，再筛选一小部分的用户请求路由到新版本的Pod应用，继续观察能否稳定地按期望的方式运行。确定没问题之后再继续完成余下的Pod资源滚动更新，否则立即回滚更新操作。这就是所谓的金丝雀发布。 1234567891011121314151617181920212223242526272829303132333435363738394041# 更新deployment的版本，并配置暂停deployment[root@k8s-master01 ~]# kubectl set image deploy pc-deployment nginx=nginx:1.17.4 -n dev &amp;&amp; kubectl rollout pause deployment pc-deployment -n devdeployment.apps/pc-deployment image updateddeployment.apps/pc-deployment paused#观察更新状态[root@k8s-master01 ~]# kubectl rollout status deploy pc-deployment -n dev Waiting for deployment &quot;pc-deployment&quot; rollout to finish: 2 out of 4 new replicas have been updated...# 监控更新的过程，可以看到已经新增了一个资源，但是并未按照预期的状态去删除一个旧的资源，就是因为使用了pause暂停命令[root@k8s-master01 ~]# kubectl get rs -n dev -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES pc-deployment-5d89bdfbf9 3 3 3 19m nginx nginx:1.17.1 pc-deployment-675d469f8b 0 0 0 14m nginx nginx:1.17.2 pc-deployment-6c9f56fcfb 2 2 2 3m16s nginx nginx:1.17.4 [root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-deployment-5d89bdfbf9-rj8sq 1/1 Running 0 7m33spc-deployment-5d89bdfbf9-ttwgg 1/1 Running 0 7m35spc-deployment-5d89bdfbf9-v4wvc 1/1 Running 0 7m34spc-deployment-6c9f56fcfb-996rt 1/1 Running 0 3m31spc-deployment-6c9f56fcfb-j2gtj 1/1 Running 0 3m31s# 确保更新的pod没问题了，继续更新[root@k8s-master01 ~]# kubectl rollout resume deploy pc-deployment -n devdeployment.apps/pc-deployment resumed# 查看最后的更新情况[root@k8s-master01 ~]# kubectl get rs -n dev -o wideNAME DESIRED CURRENT READY AGE CONTAINERS IMAGES pc-deployment-5d89bdfbf9 0 0 0 21m nginx nginx:1.17.1 pc-deployment-675d469f8b 0 0 0 16m nginx nginx:1.17.2 pc-deployment-6c9f56fcfb 4 4 4 5m11s nginx nginx:1.17.4 [root@k8s-master01 ~]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEpc-deployment-6c9f56fcfb-7bfwh 1/1 Running 0 37spc-deployment-6c9f56fcfb-996rt 1/1 Running 0 5m27spc-deployment-6c9f56fcfb-j2gtj 1/1 Running 0 5m27spc-deployment-6c9f56fcfb-rf84v 1/1 Running 0 37s 删除Deployment 123# 删除deployment，其下的rs和pod也将被删除[root@k8s-master01 ~]# kubectl delete -f pc-deployment.yamldeployment.apps &quot;pc-deployment&quot; deleted 6.4 Horizontal Pod Autoscaler(HPA)在前面的课程中，我们已经可以实现通过手工执行kubectl scale命令实现Pod扩容或缩容，但是这显然不符合Kubernetes的定位目标–自动化、智能化。 Kubernetes期望可以实现通过监测Pod的使用情况，实现pod数量的自动调整，于是就产生了Horizontal Pod Autoscaler（HPA）这种控制器。 HPA可以获取每个Pod利用率，然后和HPA中定义的指标进行对比，同时计算出需要伸缩的具体值，最后实现Pod的数量的调整。其实HPA与之前的Deployment一样，也属于一种Kubernetes资源对象，它通过追踪分析RC控制的所有目标Pod的负载变化情况，来确定是否需要针对性地调整目标Pod的副本数，这是HPA的实现原理。 接下来，我们来做一个实验 6.4.1 安装metrics-servermetrics-server可以用来收集集群中的资源使用情况 12345678910111213# 安装git[root@k8s-master01 ~]# yum install git -y# 获取metrics-server, 注意使用的版本[root@k8s-master01 ~]# git clone -b v0.3.6 https://github.com/kubernetes-incubator/metrics-server# 修改deployment, 注意修改的是镜像和初始化参数[root@k8s-master01 ~]# cd /root/metrics-server/deploy/1.8+/[root@k8s-master01 1.8+]# vim metrics-server-deployment.yaml按图中添加下面选项hostNetwork: trueimage: registry.cn-hangzhou.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6args:- --kubelet-insecure-tls- --kubelet-preferred-address-types=InternalIP,Hostname,InternalDNS,ExternalDNS,ExternalIP 1234567891011121314151617181920# 安装metrics-server[root@k8s-master01 1.8+]# kubectl apply -f ./# 查看pod运行情况[root@k8s-master01 1.8+]# kubectl get pod -n kube-systemmetrics-server-6b976979db-2xwbj 1/1 Running 0 90s# 使用kubectl top node 查看资源使用情况[root@k8s-master01 1.8+]# kubectl top nodeNAME CPU(cores) CPU% MEMORY(bytes) MEMORY%k8s-master01 289m 14% 1582Mi 54% k8s-node01 81m 4% 1195Mi 40% k8s-node02 72m 3% 1211Mi 41% [root@k8s-master01 1.8+]# kubectl top pod -n kube-systemNAME CPU(cores) MEMORY(bytes)coredns-6955765f44-7ptsb 3m 9Micoredns-6955765f44-vcwr5 3m 8Mietcd-master 14m 145Mi...# 至此,metrics-server安装完成 6.4.2 准备deployment和servie创建pc-hpa-pod.yaml文件，内容如下： 12345678910111213141516171819202122232425apiVersion: apps/v1kind: Deploymentmetadata: name: nginx namespace: devspec: strategy: # 策略 type: RollingUpdate # 滚动更新策略 replicas: 1 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 resources: # 资源配额 limits: # 限制资源（上限） cpu: &quot;1&quot; # CPU限制，单位是core数 requests: # 请求资源（下限） cpu: &quot;100m&quot; # CPU限制，单位是core数 1234# 创建deployment[root@k8s-master01 1.8+]# kubectl run nginx --image=nginx:1.17.1 --requests=cpu=100m -n dev# 创建service[root@k8s-master01 1.8+]# kubectl expose deployment nginx --type=NodePort --port=80 -n dev 12345678910# 查看[root@k8s-master01 1.8+]# kubectl get deployment,pod,svc -n devNAME READY UP-TO-DATE AVAILABLE AGEdeployment.apps/nginx 1/1 1 1 47sNAME READY STATUS RESTARTS AGEpod/nginx-7df9756ccc-bh8dr 1/1 Running 0 47sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/nginx NodePort 10.101.18.29 &lt;none&gt; 80:31830/TCP 35s 6.4.3 部署HPA创建pc-hpa.yaml文件，内容如下： 12345678910111213apiVersion: autoscaling/v1kind: HorizontalPodAutoscalermetadata: name: pc-hpa namespace: devspec: minReplicas: 1 #最小pod数量 maxReplicas: 10 #最大pod数量 targetCPUUtilizationPercentage: 3 # CPU使用率指标 scaleTargetRef: # 指定要控制的nginx信息 apiVersion: /v1 kind: Deployment name: nginx 12345678# 创建hpa[root@k8s-master01 1.8+]# kubectl create -f pc-hpa.yamlhorizontalpodautoscaler.autoscaling/pc-hpa created# 查看hpa [root@k8s-master01 1.8+]# kubectl get hpa -n devNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEpc-hpa Deployment/nginx 0%/3% 1 10 1 62s 6.4.4 测试使用压测工具对service地址192.168.5.4:31830进行压测，然后通过控制台查看hpa和pod的变化 hpa变化 1234567891011[root@k8s-master01 ~]# kubectl get hpa -n dev -wNAME REFERENCE TARGETS MINPODS MAXPODS REPLICAS AGEpc-hpa Deployment/nginx 0%/3% 1 10 1 4m11spc-hpa Deployment/nginx 0%/3% 1 10 1 5m19spc-hpa Deployment/nginx 22%/3% 1 10 1 6m50spc-hpa Deployment/nginx 22%/3% 1 10 4 7m5spc-hpa Deployment/nginx 22%/3% 1 10 8 7m21spc-hpa Deployment/nginx 6%/3% 1 10 8 7m51spc-hpa Deployment/nginx 0%/3% 1 10 8 9m6spc-hpa Deployment/nginx 0%/3% 1 10 8 13mpc-hpa Deployment/nginx 0%/3% 1 10 1 14m deployment变化 123456789101112131415161718192021[root@k8s-master01 ~]# kubectl get deployment -n dev -wNAME READY UP-TO-DATE AVAILABLE AGEnginx 1/1 1 1 11mnginx 1/4 1 1 13mnginx 1/4 1 1 13mnginx 1/4 1 1 13mnginx 1/4 4 1 13mnginx 1/8 4 1 14mnginx 1/8 4 1 14mnginx 1/8 4 1 14mnginx 1/8 8 1 14mnginx 2/8 8 2 14mnginx 3/8 8 3 14mnginx 4/8 8 4 14mnginx 5/8 8 5 14mnginx 6/8 8 6 14mnginx 7/8 8 7 14mnginx 8/8 8 8 15mnginx 8/1 8 8 20mnginx 8/1 8 8 20mnginx 1/1 1 1 20m pod变化 12345678910111213141516171819202122232425262728293031[root@k8s-master01 ~]# kubectl get pods -n dev -wNAME READY STATUS RESTARTS AGEnginx-7df9756ccc-bh8dr 1/1 Running 0 11mnginx-7df9756ccc-cpgrv 0/1 Pending 0 0snginx-7df9756ccc-8zhwk 0/1 Pending 0 0snginx-7df9756ccc-rr9bn 0/1 Pending 0 0snginx-7df9756ccc-cpgrv 0/1 ContainerCreating 0 0snginx-7df9756ccc-8zhwk 0/1 ContainerCreating 0 0snginx-7df9756ccc-rr9bn 0/1 ContainerCreating 0 0snginx-7df9756ccc-m9gsj 0/1 Pending 0 0snginx-7df9756ccc-g56qb 0/1 Pending 0 0snginx-7df9756ccc-sl9c6 0/1 Pending 0 0snginx-7df9756ccc-fgst7 0/1 Pending 0 0snginx-7df9756ccc-g56qb 0/1 ContainerCreating 0 0snginx-7df9756ccc-m9gsj 0/1 ContainerCreating 0 0snginx-7df9756ccc-sl9c6 0/1 ContainerCreating 0 0snginx-7df9756ccc-fgst7 0/1 ContainerCreating 0 0snginx-7df9756ccc-8zhwk 1/1 Running 0 19snginx-7df9756ccc-rr9bn 1/1 Running 0 30snginx-7df9756ccc-m9gsj 1/1 Running 0 21snginx-7df9756ccc-cpgrv 1/1 Running 0 47snginx-7df9756ccc-sl9c6 1/1 Running 0 33snginx-7df9756ccc-g56qb 1/1 Running 0 48snginx-7df9756ccc-fgst7 1/1 Running 0 66snginx-7df9756ccc-fgst7 1/1 Terminating 0 6m50snginx-7df9756ccc-8zhwk 1/1 Terminating 0 7m5snginx-7df9756ccc-cpgrv 1/1 Terminating 0 7m5snginx-7df9756ccc-g56qb 1/1 Terminating 0 6m50snginx-7df9756ccc-rr9bn 1/1 Terminating 0 7m5snginx-7df9756ccc-m9gsj 1/1 Terminating 0 6m50snginx-7df9756ccc-sl9c6 1/1 Terminating 0 6m50s 6.5 DaemonSet(DS)DaemonSet类型的控制器可以保证在集群中的每一台（或指定）节点上都运行一个副本。一般适用于日志收集、节点监控等场景。也就是说，如果一个Pod提供的功能是节点级别的（每个节点都需要且只需要一个），那么这类Pod就适合使用DaemonSet类型的控制器创建。 DaemonSet控制器的特点： 每当向集群中添加一个节点时，指定的 Pod 副本也将添加到该节点上 当节点从集群中移除时，Pod 也就被垃圾回收了 下面先来看下DaemonSet的资源清单文件 12345678910111213141516171819202122232425262728apiVersion: apps/v1 # 版本号kind: DaemonSet # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: daemonsetspec: # 详情描述 revisionHistoryLimit: 3 # 保留历史版本 updateStrategy: # 更新策略 type: RollingUpdate # 滚动更新策略 rollingUpdate: # 滚动更新 maxUnavailable: 1 # 最大不可用状态的 Pod 的最大值，可以为百分比，也可以为整数 selector: # 选择器，通过它指定该控制器管理哪些pod matchLabels: # Labels匹配规则 app: nginx-pod matchExpressions: # Expressions匹配规则 - &#123;key: app, operator: In, values: [nginx-pod]&#125; template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 创建pc-daemonset.yaml，内容如下： 1234567891011121314151617apiVersion: apps/v1kind: DaemonSet metadata: name: pc-daemonset namespace: devspec: selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 123456789101112131415161718# 创建daemonset[root@k8s-master01 ~]# kubectl create -f pc-daemonset.yamldaemonset.apps/pc-daemonset created# 查看daemonset[root@k8s-master01 ~]# kubectl get ds -n dev -o wideNAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE AGE CONTAINERS IMAGES pc-daemonset 2 2 2 2 2 24s nginx nginx:1.17.1 # 查看pod,发现在每个Node上都运行一个pod[root@k8s-master01 ~]# kubectl get pods -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE pc-daemonset-9bck8 1/1 Running 0 37s 10.244.1.43 node1 pc-daemonset-k224w 1/1 Running 0 37s 10.244.2.74 node2 # 删除daemonset[root@k8s-master01 ~]# kubectl delete -f pc-daemonset.yamldaemonset.apps &quot;pc-daemonset&quot; deleted 6.6 JobJob，主要用于负责**批量处理(一次要处理指定数量任务)短暂的一次性(每个任务仅运行一次就结束)**任务。Job特点如下： 当Job创建的pod执行成功结束时，Job将记录成功结束的pod数量 当成功结束的pod达到指定的数量时，Job将完成执行 Job的资源清单文件： 12345678910111213141516171819202122232425262728apiVersion: batch/v1 # 版本号kind: Job # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: jobspec: # 详情描述 completions: 1 # 指定job需要成功运行Pods的次数。默认值: 1 parallelism: 1 # 指定job在任一时刻应该并发运行Pods的数量。默认值: 1 activeDeadlineSeconds: 30 # 指定job可运行的时间期限，超过时间还未结束，系统将会尝试进行终止。 backoffLimit: 6 # 指定job失败后进行重试的次数。默认是6 manualSelector: true # 是否可以使用selector选择器选择pod，默认是false selector: # 选择器，通过它指定该控制器管理哪些pod matchLabels: # Labels匹配规则 app: counter-pod matchExpressions: # Expressions匹配规则 - &#123;key: app, operator: In, values: [counter-pod]&#125; template: # 模板，当副本数量不足时，会根据下面的模板创建pod副本 metadata: labels: app: counter-pod spec: restartPolicy: Never # 重启策略只能设置为Never或者OnFailure containers: - name: counter image: busybox:1.30 command: [&quot;bin/sh&quot;,&quot;-c&quot;,&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 2;done&quot;] 1234关于重启策略设置的说明： 如果指定为OnFailure，则job会在pod出现故障时重启容器，而不是创建pod，failed次数不变 如果指定为Never，则job会在pod出现故障时创建新的pod，并且故障pod不会消失，也不会重启，failed次数加1 如果指定为Always的话，就意味着一直重启，意味着job任务会重复去执行了，当然不对，所以不能设置为Always 创建pc-job.yaml，内容如下： 1234567891011121314151617181920apiVersion: batch/v1kind: Job metadata: name: pc-job namespace: devspec: manualSelector: true selector: matchLabels: app: counter-pod template: metadata: labels: app: counter-pod spec: restartPolicy: Never containers: - name: counter image: busybox:1.30 command: [&quot;bin/sh&quot;,&quot;-c&quot;,&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done&quot;] 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 创建job[root@k8s-master01 ~]# kubectl create -f pc-job.yamljob.batch/pc-job created# 查看job[root@k8s-master01 ~]# kubectl get job -n dev -o wide -wNAME COMPLETIONS DURATION AGE CONTAINERS IMAGES SELECTORpc-job 0/1 21s 21s counter busybox:1.30 app=counter-podpc-job 1/1 31s 79s counter busybox:1.30 app=counter-pod# 通过观察pod状态可以看到，pod在运行完毕任务后，就会变成Completed状态[root@k8s-master01 ~]# kubectl get pods -n dev -wNAME READY STATUS RESTARTS AGEpc-job-rxg96 1/1 Running 0 29spc-job-rxg96 0/1 Completed 0 33s# 接下来，调整下pod运行的总数量和并行数量 即：在spec下设置下面两个选项# completions: 6 # 指定job需要成功运行Pods的次数为6# parallelism: 3 # 指定job并发运行Pods的数量为3# 然后重新运行job，观察效果，此时会发现，job会每次运行3个pod，总共执行了6个pod[root@k8s-master01 ~]# kubectl get pods -n dev -wNAME READY STATUS RESTARTS AGEpc-job-684ft 1/1 Running 0 5spc-job-jhj49 1/1 Running 0 5spc-job-pfcvh 1/1 Running 0 5spc-job-684ft 0/1 Completed 0 11spc-job-v7rhr 0/1 Pending 0 0spc-job-v7rhr 0/1 Pending 0 0spc-job-v7rhr 0/1 ContainerCreating 0 0spc-job-jhj49 0/1 Completed 0 11spc-job-fhwf7 0/1 Pending 0 0spc-job-fhwf7 0/1 Pending 0 0spc-job-pfcvh 0/1 Completed 0 11spc-job-5vg2j 0/1 Pending 0 0spc-job-fhwf7 0/1 ContainerCreating 0 0spc-job-5vg2j 0/1 Pending 0 0spc-job-5vg2j 0/1 ContainerCreating 0 0spc-job-fhwf7 1/1 Running 0 2spc-job-v7rhr 1/1 Running 0 2spc-job-5vg2j 1/1 Running 0 3spc-job-fhwf7 0/1 Completed 0 12spc-job-v7rhr 0/1 Completed 0 12spc-job-5vg2j 0/1 Completed 0 12s# 删除job[root@k8s-master01 ~]# kubectl delete -f pc-job.yamljob.batch &quot;pc-job&quot; deleted 6.7 CronJob(CJ)CronJob控制器以 Job控制器资源为其管控对象，并借助它管理pod资源对象，Job控制器定义的作业任务在其控制器资源创建之后便会立即执行，但CronJob可以以类似于Linux操作系统的周期性任务作业计划的方式控制其运行时间点及重复运行的方式。也就是说，CronJob可以在特定的时间点(反复的)去运行job任务。 CronJob的资源清单文件： 123456789101112131415161718192021222324252627282930313233343536apiVersion: batch/v1beta1 # 版本号kind: CronJob # 类型 metadata: # 元数据 name: # rs名称 namespace: # 所属命名空间 labels: #标签 controller: cronjobspec: # 详情描述 schedule: # cron格式的作业调度运行时间点,用于控制任务在什么时间执行 concurrencyPolicy: # 并发执行策略，用于定义前一次作业运行尚未完成时是否以及如何运行后一次的作业 failedJobHistoryLimit: # 为失败的任务执行保留的历史记录数，默认为1 successfulJobHistoryLimit: # 为成功的任务执行保留的历史记录数，默认为3 startingDeadlineSeconds: # 启动作业错误的超时时长 jobTemplate: # job控制器模板，用于为cronjob控制器生成job对象;下面其实就是job的定义 metadata: spec: completions: 1 parallelism: 1 activeDeadlineSeconds: 30 backoffLimit: 6 manualSelector: true selector: matchLabels: app: counter-pod matchExpressions: 规则 - &#123;key: app, operator: In, values: [counter-pod]&#125; template: metadata: labels: app: counter-pod spec: restartPolicy: Never containers: - name: counter image: busybox:1.30 command: [&quot;bin/sh&quot;,&quot;-c&quot;,&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 20;done&quot;] 123456789101112131415需要重点解释的几个选项：schedule: cron表达式，用于指定任务的执行时间 */1 * * * * &lt;分钟&gt; &lt;小时&gt; &lt;日&gt; &lt;月份&gt; &lt;星期&gt; 分钟 值从 0 到 59. 小时 值从 0 到 23. 日 值从 1 到 31. 月 值从 1 到 12. 星期 值从 0 到 6, 0 代表星期日 多个时间可以用逗号隔开； 范围可以用连字符给出；*可以作为通配符； /表示每...concurrencyPolicy: Allow: 允许Jobs并发运行(默认) Forbid: 禁止并发运行，如果上一次运行尚未完成，则跳过下一次运行 Replace: 替换，取消当前正在运行的作业并用新作业替换它 创建pc-cronjob.yaml，内容如下： 12345678910111213141516171819apiVersion: batch/v1beta1kind: CronJobmetadata: name: pc-cronjob namespace: dev labels: controller: cronjobspec: schedule: &quot;*/1 * * * *&quot; jobTemplate: metadata: spec: template: spec: restartPolicy: Never containers: - name: counter image: busybox:1.30 command: [&quot;bin/sh&quot;,&quot;-c&quot;,&quot;for i in 9 8 7 6 5 4 3 2 1; do echo $i;sleep 3;done&quot;] 1234567891011121314151617181920212223242526# 创建cronjob[root@k8s-master01 ~]# kubectl create -f pc-cronjob.yamlcronjob.batch/pc-cronjob created# 查看cronjob[root@k8s-master01 ~]# kubectl get cronjobs -n devNAME SCHEDULE SUSPEND ACTIVE LAST SCHEDULE AGEpc-cronjob */1 * * * * False 0 &lt;none&gt; 6s# 查看job[root@k8s-master01 ~]# kubectl get jobs -n devNAME COMPLETIONS DURATION AGEpc-cronjob-1592587800 1/1 28s 3m26spc-cronjob-1592587860 1/1 28s 2m26spc-cronjob-1592587920 1/1 28s 86s# 查看pod[root@k8s-master01 ~]# kubectl get pods -n devpc-cronjob-1592587800-x4tsm 0/1 Completed 0 2m24spc-cronjob-1592587860-r5gv4 0/1 Completed 0 84spc-cronjob-1592587920-9dxxq 1/1 Running 0 24s# 删除cronjob[root@k8s-master01 ~]# kubectl delete -f pc-cronjob.yamlcronjob.batch &quot;pc-cronjob&quot; deleted 7. Service详解7.1 Service介绍在kubernetes中，pod是应用程序的载体，我们可以通过pod的ip来访问应用程序，但是pod的ip地址不是固定的，这也就意味着不方便直接采用pod的ip对服务进行访问。 为了解决这个问题，kubernetes提供了Service资源，Service会对提供同一个服务的多个pod进行聚合，并且提供一个统一的入口地址。通过访问Service的入口地址就能访问到后面的pod服务。 Service在很多情况下只是一个概念，真正起作用的其实是kube-proxy服务进程，每个Node节点上都运行着一个kube-proxy服务进程。当创建Service的时候会通过api-server向etcd写入创建的service的信息，而kube-proxy会基于监听的机制发现这种Service的变动，然后它会将最新的Service信息转换成对应的访问规则。 123456789101112# 10.97.97.97:80 是service提供的访问入口# 当访问这个入口的时候，可以发现后面有三个pod的服务在等待调用，# kube-proxy会基于rr（轮询）的策略，将请求分发到其中一个pod上去# 这个规则会同时在集群内的所有节点上都生成，所以在任何一个节点上访问都可以。[root@node1 ~]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.97.97.97:80 rr -&gt; 10.244.1.39:80 Masq 1 0 0 -&gt; 10.244.1.40:80 Masq 1 0 0 -&gt; 10.244.2.33:80 Masq 1 0 0 kube-proxy目前支持三种工作模式: 7.1.1 userspace 模式userspace模式下，kube-proxy会为每一个Service创建一个监听端口，发向Cluster IP的请求被Iptables规则重定向到kube-proxy监听的端口上，kube-proxy根据LB算法选择一个提供服务的Pod并和其建立链接，以将请求转发到Pod上。 该模式下，kube-proxy充当了一个四层负责均衡器的角色。由于kube-proxy运行在userspace中，在进行转发处理时会增加内核和用户空间之间的数据拷贝，虽然比较稳定，但是效率比较低。 7.1.2 iptables 模式iptables模式下，kube-proxy为service后端的每个Pod创建对应的iptables规则，直接将发向Cluster IP的请求重定向到一个Pod IP。 该模式下kube-proxy不承担四层负责均衡器的角色，只负责创建iptables规则。该模式的优点是较userspace模式效率更高，但不能提供灵活的LB策略，当后端Pod不可用时也无法进行重试。 7.1.3 ipvs 模式ipvs模式和iptables类似，kube-proxy监控Pod的变化并创建相应的ipvs规则。ipvs相对iptables转发效率更高。除此以外，ipvs支持更多的LB算法。 12345678910111213# 此模式必须安装ipvs内核模块，否则会降级为iptables# 开启ipvs[root@k8s-master01 ~]# kubectl edit cm kube-proxy -n kube-system# 修改mode: &quot;ipvs&quot;[root@k8s-master01 ~]# kubectl delete pod -l k8s-app=kube-proxy -n kube-system[root@node1 ~]# ipvsadm -LnIP Virtual Server version 1.2.1 (size=4096)Prot LocalAddress:Port Scheduler Flags -&gt; RemoteAddress:Port Forward Weight ActiveConn InActConnTCP 10.97.97.97:80 rr -&gt; 10.244.1.39:80 Masq 1 0 0 -&gt; 10.244.1.40:80 Masq 1 0 0 -&gt; 10.244.2.33:80 Masq 1 0 0 7.2 Service类型Service的资源清单文件： 12345678910111213141516kind: Service # 资源类型apiVersion: v1 # 资源版本metadata: # 元数据 name: service # 资源名称 namespace: dev # 命名空间spec: # 描述 selector: # 标签选择器，用于确定当前service代理哪些pod app: nginx type: # Service类型，指定service的访问方式 clusterIP: # 虚拟服务的ip地址 sessionAffinity: # session亲和性，支持ClientIP、None两个选项 ports: # 端口信息 - protocol: TCP port: 3017 # service端口 targetPort: 5003 # pod端口 nodePort: 31122 # 主机端口 ClusterIP：默认值，它是Kubernetes系统自动分配的虚拟IP，只能在集群内部访问 NodePort：将Service通过指定的Node上的端口暴露给外部，通过此方法，就可以在集群外部访问服务 LoadBalancer：使用外接负载均衡器完成到服务的负载分发，注意此模式需要外部云环境支持 ExternalName： 把集群外部的服务引入集群内部，直接使用 7.3 Service使用7.3.1 实验环境准备在使用service之前，首先利用Deployment创建出3个pod，注意要为pod设置app=nginx-pod的标签 创建deployment.yaml，内容如下： 1234567891011121314151617181920apiVersion: apps/v1kind: Deployment metadata: name: pc-deployment namespace: devspec: replicas: 3 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 123456789101112131415161718192021[root@k8s-master01 ~]# kubectl create -f deployment.yamldeployment.apps/pc-deployment created# 查看pod详情[root@k8s-master01 ~]# kubectl get pods -n dev -o wide --show-labelsNAME READY STATUS IP NODE LABELSpc-deployment-66cb59b984-8p84h 1/1 Running 10.244.1.39 node1 app=nginx-podpc-deployment-66cb59b984-vx8vx 1/1 Running 10.244.2.33 node2 app=nginx-podpc-deployment-66cb59b984-wnncx 1/1 Running 10.244.1.40 node1 app=nginx-pod# 为了方便后面的测试，修改下三台nginx的index.html页面（三台修改的IP地址不一致）# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh# echo &quot;10.244.1.39&quot; &gt; /usr/share/nginx/html/index.html#修改完毕之后，访问测试[root@k8s-master01 ~]# curl 10.244.1.3910.244.1.39[root@k8s-master01 ~]# curl 10.244.2.3310.244.2.33[root@k8s-master01 ~]# curl 10.244.1.4010.244.1.40 7.3.2 ClusterIP类型的Service创建service-clusterip.yaml文件 12345678910111213apiVersion: v1kind: Servicemetadata: name: service-clusterip namespace: devspec: selector: app: nginx-pod clusterIP: 10.97.97.97 # service的ip地址，如果不写，默认会生成一个 type: ClusterIP ports: - port: 80 # Service端口 targetPort: 80 # pod端口 1234567891011121314151617181920212223242526272829303132333435# 创建service[root@k8s-master01 ~]# kubectl create -f service-clusterip.yamlservice/service-clusterip created# 查看service[root@k8s-master01 ~]# kubectl get svc -n dev -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice-clusterip ClusterIP 10.97.97.97 &lt;none&gt; 80/TCP 13s app=nginx-pod# 查看service的详细信息# 在这里有一个Endpoints列表，里面就是当前service可以负载到的服务入口[root@k8s-master01 ~]# kubectl describe svc service-clusterip -n devName: service-clusteripNamespace: devLabels: &lt;none&gt;Annotations: &lt;none&gt;Selector: app=nginx-podType: ClusterIPIP: 10.97.97.97Port: &lt;unset&gt; 80/TCPTargetPort: 80/TCPEndpoints: 10.244.1.39:80,10.244.1.40:80,10.244.2.33:80Session Affinity: NoneEvents: &lt;none&gt;# 查看ipvs的映射规则[root@k8s-master01 ~]# ipvsadm -LnTCP 10.97.97.97:80 rr -&gt; 10.244.1.39:80 Masq 1 0 0 -&gt; 10.244.1.40:80 Masq 1 0 0 -&gt; 10.244.2.33:80 Masq 1 0 0# 访问10.97.97.97:80观察效果[root@k8s-master01 ~]# curl 10.97.97.97:8010.244.2.33 7.3.3 EndpointEndpoint是kubernetes中的一个资源对象，存储在etcd中，用来记录一个service对应的所有pod的访问地址，它是根据service配置文件中selector描述产生的。 一个Service由一组Pod组成，这些Pod通过Endpoints暴露出来，Endpoints是实现实际服务的端点集合。换句话说，service和pod之间的联系是通过endpoints实现的。 负载分发策略 对Service的访问被分发到了后端的Pod上去，目前kubernetes提供了两种负载分发策略： 如果不定义，默认使用kube-proxy的策略，比如随机、轮询 基于客户端地址的会话保持模式，即来自同一个客户端发起的所有请求都会转发到固定的一个Pod上 此模式可以使在spec中添加sessionAffinity:ClientIP选项 12345678910111213141516171819202122232425262728293031323334# 查看ipvs的映射规则【rr 轮询】[root@k8s-master01 ~]# ipvsadm -LnTCP 10.97.97.97:80 rr -&gt; 10.244.1.39:80 Masq 1 0 0 -&gt; 10.244.1.40:80 Masq 1 0 0 -&gt; 10.244.2.33:80 Masq 1 0 0# 循环访问测试[root@k8s-master01 ~]# while true;do curl 10.97.97.97:80; sleep 5; done;10.244.1.4010.244.1.3910.244.2.3310.244.1.4010.244.1.3910.244.2.33# 修改分发策略----sessionAffinity:ClientIP# 查看ipvs规则【persistent 代表持久】[root@k8s-master01 ~]# ipvsadm -LnTCP 10.97.97.97:80 rr persistent 10800 -&gt; 10.244.1.39:80 Masq 1 0 0 -&gt; 10.244.1.40:80 Masq 1 0 0 -&gt; 10.244.2.33:80 Masq 1 0 0# 循环访问测试[root@k8s-master01 ~]# while true;do curl 10.97.97.97; sleep 5; done;10.244.2.3310.244.2.3310.244.2.33 # 删除service[root@k8s-master01 ~]# kubectl delete -f service-clusterip.yamlservice &quot;service-clusterip&quot; deleted 7.3.4 HeadLiness类型的Service在某些场景中，开发人员可能不想使用Service提供的负载均衡功能，而希望自己来控制负载均衡策略，针对这种情况，kubernetes提供了HeadLiness Service，这类Service不会分配Cluster IP，如果想要访问service，只能通过service的域名进行查询。 创建service-headliness.yaml 12345678910111213apiVersion: v1kind: Servicemetadata: name: service-headliness namespace: devspec: selector: app: nginx-pod clusterIP: None # 将clusterIP设置为None，即可创建headliness Service type: ClusterIP ports: - port: 80 targetPort: 80 12345678910111213141516171819202122232425262728293031323334# 创建service[root@k8s-master01 ~]# kubectl create -f service-headliness.yamlservice/service-headliness created# 获取service， 发现CLUSTER-IP未分配[root@k8s-master01 ~]# kubectl get svc service-headliness -n dev -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE SELECTORservice-headliness ClusterIP None &lt;none&gt; 80/TCP 11s app=nginx-pod# 查看service详情[root@k8s-master01 ~]# kubectl describe svc service-headliness -n devName: service-headlinessNamespace: devLabels: &lt;none&gt;Annotations: &lt;none&gt;Selector: app=nginx-podType: ClusterIPIP: NonePort: &lt;unset&gt; 80/TCPTargetPort: 80/TCPEndpoints: 10.244.1.39:80,10.244.1.40:80,10.244.2.33:80Session Affinity: NoneEvents: &lt;none&gt;# 查看域名的解析情况[root@k8s-master01 ~]# kubectl exec -it pc-deployment-66cb59b984-8p84h -n dev /bin/sh/ # cat /etc/resolv.confnameserver 10.96.0.10search dev.svc.cluster.local svc.cluster.local cluster.local[root@k8s-master01 ~]# dig @10.96.0.10 service-headliness.dev.svc.cluster.localservice-headliness.dev.svc.cluster.local. 30 IN A 10.244.1.40service-headliness.dev.svc.cluster.local. 30 IN A 10.244.1.39service-headliness.dev.svc.cluster.local. 30 IN A 10.244.2.33 7.3.5 NodePort类型的Service在之前的样例中，创建的Service的ip地址只有集群内部才可以访问，如果希望将Service暴露给集群外部使用，那么就要使用到另外一种类型的Service，称为NodePort类型。NodePort的工作原理其实就是将service的端口映射到Node的一个端口上，然后就可以通过NodeIp:NodePort来访问service了。 创建service-nodeport.yaml 12345678910111213apiVersion: v1kind: Servicemetadata: name: service-nodeport namespace: devspec: selector: app: nginx-pod type: NodePort # service类型 ports: - port: 80 nodePort: 30002 # 指定绑定的node的端口(默认的取值范围是：30000-32767), 如果不指定，会默认分配 targetPort: 80 12345678910# 创建service[root@k8s-master01 ~]# kubectl create -f service-nodeport.yamlservice/service-nodeport created# 查看service[root@k8s-master01 ~]# kubectl get svc -n dev -o wideNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) SELECTORservice-nodeport NodePort 10.105.64.191 &lt;none&gt; 80:30002/TCP app=nginx-pod# 接下来可以通过电脑主机的浏览器去访问集群中任意一个nodeip的30002端口，即可访问到pod 7.3.6 LoadBalancer类型的ServiceLoadBalancer和NodePort很相似，目的都是向外部暴露一个端口，区别在于LoadBalancer会在集群的外部再来做一个负载均衡设备，而这个设备需要外部环境支持的，外部服务发送到这个设备上的请求，会被设备负载之后转发到集群中。 7.3.7 ExternalName类型的ServiceExternalName类型的Service用于引入集群外部的服务，它通过externalName属性指定外部一个服务的地址，然后在集群内部访问此service就可以访问到外部的服务了。 12345678apiVersion: v1kind: Servicemetadata: name: service-externalname namespace: devspec: type: ExternalName # service类型 externalName: www.baidu.com #改成ip地址也可以 12345678910# 创建service[root@k8s-master01 ~]# kubectl create -f service-externalname.yamlservice/service-externalname created# 域名解析[root@k8s-master01 ~]# dig @10.96.0.10 service-externalname.dev.svc.cluster.localservice-externalname.dev.svc.cluster.local. 30 IN CNAME www.baidu.com.www.baidu.com. 30 IN CNAME www.a.shifen.com.www.a.shifen.com. 30 IN A 39.156.66.18www.a.shifen.com. 30 IN A 39.156.66.14 7.4 Ingress介绍在前面课程中已经提到，Service对集群之外暴露服务的主要方式有两种：NotePort和LoadBalancer，但是这两种方式，都有一定的缺点： NodePort方式的缺点是会占用很多集群机器的端口，那么当集群服务变多的时候，这个缺点就愈发明显 LB方式的缺点是每个service需要一个LB，浪费、麻烦，并且需要kubernetes之外设备的支持 基于这种现状，kubernetes提供了Ingress资源对象，Ingress只需要一个NodePort或者一个LB就可以满足暴露多个Service的需求。工作机制大致如下图表示： 实际上，Ingress相当于一个7层的负载均衡器，是kubernetes对反向代理的一个抽象，它的工作原理类似于Nginx，可以理解成在Ingress里建立诸多映射规则，Ingress Controller通过监听这些配置规则并转化成Nginx的反向代理配置 , 然后对外部提供服务。在这里有两个核心概念： ingress：kubernetes中的一个对象，作用是定义请求如何转发到service的规则 ingress controller：具体实现反向代理及负载均衡的程序，对ingress定义的规则进行解析，根据配置的规则来实现请求转发，实现方式有很多，比如Nginx, Contour, Haproxy等等 Ingress（以Nginx为例）的工作原理如下： 用户编写Ingress规则，说明哪个域名对应kubernetes集群中的哪个Service Ingress控制器动态感知Ingress服务规则的变化，然后生成一段对应的Nginx反向代理配置 Ingress控制器会将生成的Nginx配置写入到一个运行着的Nginx服务中，并动态更新 到此为止，其实真正在工作的就是一个Nginx了，内部配置了用户定义的请求转发规则 7.5 Ingress使用7.5.1 环境准备 搭建ingress环境1234567891011121314151617181920212223# 创建文件夹[root@k8s-master01 ~]# mkdir ingress-controller[root@k8s-master01 ~]# cd ingress-controller/# 获取ingress-nginx，本次案例使用的是0.30版本[root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml[root@k8s-master01 ingress-controller]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/baremetal/service-nodeport.yaml# 修改mandatory.yaml文件中的仓库# 修改quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0# 为quay-mirror.qiniu.com/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0# 创建ingress-nginx[root@k8s-master01 ingress-controller]# kubectl apply -f ./# 查看ingress-nginx[root@k8s-master01 ingress-controller]# kubectl get pod -n ingress-nginxNAME READY STATUS RESTARTS AGEpod/nginx-ingress-controller-fbf967dd5-4qpbp 1/1 Running 0 12h# 查看service[root@k8s-master01 ingress-controller]# kubectl get svc -n ingress-nginxNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEingress-nginx NodePort 10.98.75.163 &lt;none&gt; 80:32240/TCP,443:31335/TCP 11h 7.5.2 准备service和pod为了后面的实验比较方便，创建如下图所示的模型 创建tomcat-nginx.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475apiVersion: apps/v1kind: Deploymentmetadata: name: nginx-deployment namespace: devspec: replicas: 3 selector: matchLabels: app: nginx-pod template: metadata: labels: app: nginx-pod spec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80---apiVersion: apps/v1kind: Deploymentmetadata: name: tomcat-deployment namespace: devspec: replicas: 3 selector: matchLabels: app: tomcat-pod template: metadata: labels: app: tomcat-pod spec: containers: - name: tomcat image: tomcat:8.5-jre10-slim ports: - containerPort: 8080---apiVersion: v1kind: Servicemetadata: name: nginx-service namespace: devspec: selector: app: nginx-pod clusterIP: None type: ClusterIP ports: - port: 80 targetPort: 80---apiVersion: v1kind: Servicemetadata: name: tomcat-service namespace: devspec: selector: app: tomcat-pod clusterIP: None type: ClusterIP ports: - port: 8080 targetPort: 8080 12345678# 创建[root@k8s-master01 ~]# kubectl create -f tomcat-nginx.yaml# 查看[root@k8s-master01 ~]# kubectl get svc -n devNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEnginx-service ClusterIP None &lt;none&gt; 80/TCP 48stomcat-service ClusterIP None &lt;none&gt; 8080/TCP 48s 7.5.3 Http代理创建ingress-http.yaml 123456789101112131415161718192021apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-http namespace: devspec: rules: - host: nginx.itheima.com http: paths: - path: / backend: serviceName: nginx-service servicePort: 80 - host: tomcat.itheima.com http: paths: - path: / backend: serviceName: tomcat-service servicePort: 8080 123456789101112131415161718192021# 创建[root@k8s-master01 ~]# kubectl create -f ingress-http.yamlingress.extensions/ingress-http created# 查看[root@k8s-master01 ~]# kubectl get ing ingress-http -n devNAME HOSTS ADDRESS PORTS AGEingress-http nginx.itheima.com,tomcat.itheima.com 80 22s# 查看详情[root@k8s-master01 ~]# kubectl describe ing ingress-http -n dev...Rules:Host Path Backends---- ---- --------nginx.itheima.com / nginx-service:80 (10.244.1.96:80,10.244.1.97:80,10.244.2.112:80)tomcat.itheima.com / tomcat-service:8080(10.244.1.94:8080,10.244.1.95:8080,10.244.2.111:8080)...# 接下来,在本地电脑上配置host文件,解析上面的两个域名到192.168.109.100(master)上# 然后,就可以分别访问tomcat.itheima.com:32240 和 nginx.itheima.com:32240 查看效果了 7.5.4 Https代理创建证书 12345# 生成证书openssl req -x509 -sha256 -nodes -days 365 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj &quot;/C=CN/ST=BJ/L=BJ/O=nginx/CN=itheima.com&quot;# 创建密钥kubectl create secret tls tls-secret --key tls.key --cert tls.crt 创建ingress-https.yaml 1234567891011121314151617181920212223242526apiVersion: extensions/v1beta1kind: Ingressmetadata: name: ingress-https namespace: devspec: tls: - hosts: - nginx.itheima.com - tomcat.itheima.com secretName: tls-secret # 指定秘钥 rules: - host: nginx.itheima.com http: paths: - path: / backend: serviceName: nginx-service servicePort: 80 - host: tomcat.itheima.com http: paths: - path: / backend: serviceName: tomcat-service servicePort: 8080 12345678910111213141516171819202122# 创建[root@k8s-master01 ~]# kubectl create -f ingress-https.yamlingress.extensions/ingress-https created# 查看[root@k8s-master01 ~]# kubectl get ing ingress-https -n devNAME HOSTS ADDRESS PORTS AGEingress-https nginx.itheima.com,tomcat.itheima.com 10.104.184.38 80, 443 2m42s# 查看详情[root@k8s-master01 ~]# kubectl describe ing ingress-https -n dev...TLS: tls-secret terminates nginx.itheima.com,tomcat.itheima.comRules:Host Path Backends---- ---- --------nginx.itheima.com / nginx-service:80 (10.244.1.97:80,10.244.1.98:80,10.244.2.119:80)tomcat.itheima.com / tomcat-service:8080(10.244.1.99:8080,10.244.2.117:8080,10.244.2.120:8080)...# 下面可以通过浏览器访问https://nginx.itheima.com:31335 和 https://tomcat.itheima.com:31335来查看了 8. 数据存储在前面已经提到，容器的生命周期可能很短，会被频繁地创建和销毁。那么容器在销毁时，保存在容器中的数据也会被清除。这种结果对用户来说，在某些情况下是不乐意看到的。为了持久化保存容器的数据，kubernetes引入了Volume的概念。 Volume是Pod中能够被多个容器访问的共享目录，它被定义在Pod上，然后被一个Pod里的多个容器挂载到具体的文件目录下，kubernetes通过Volume实现同一个Pod中不同容器之间的数据共享以及数据的持久化存储。Volume的生命容器不与Pod中单个容器的生命周期相关，当容器终止或者重启时，Volume中的数据也不会丢失。 kubernetes的Volume支持多种类型，比较常见的有下面几个： 简单存储：EmptyDir、HostPath、NFS 高级存储：PV、PVC 配置存储：ConfigMap、Secret 8.1 基本存储8.1.1 EmptyDirEmptyDir是最基础的Volume类型，一个EmptyDir就是Host上的一个空目录。 EmptyDir是在Pod被分配到Node时创建的，它的初始内容为空，并且无须指定宿主机上对应的目录文件，因为kubernetes会自动分配一个目录，当Pod销毁时， EmptyDir中的数据也会被永久删除。 EmptyDir用途如下： 临时空间，例如用于某些应用程序运行时所需的临时目录，且无须永久保留 一个容器需要从另一个容器中获取数据的目录（多容器共享目录） 接下来，通过一个容器之间文件共享的案例来使用一下EmptyDir。 在一个Pod中准备两个容器nginx和busybox，然后声明一个Volume分别挂在到两个容器的目录中，然后nginx容器负责向Volume中写日志，busybox中通过命令将日志内容读到控制台。 创建一个volume-emptydir.yaml 1234567891011121314151617181920212223apiVersion: v1kind: Podmetadata: name: volume-emptydir namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 volumeMounts: # 将logs-volume挂在到nginx容器中，对应的目录为 /var/log/nginx - name: logs-volume mountPath: /var/log/nginx - name: busybox image: busybox:1.30 command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;tail -f /logs/access.log&quot;] # 初始命令，动态读取指定文件中内容 volumeMounts: # 将logs-volume 挂在到busybox容器中，对应的目录为 /logs - name: logs-volume mountPath: /logs volumes: # 声明volume， name为logs-volume，类型为emptyDir - name: logs-volume emptyDir: &#123;&#125; 12345678910111213141516# 创建Pod[root@k8s-master01 ~]# kubectl create -f volume-emptydir.yamlpod/volume-emptydir created# 查看pod[root@k8s-master01 ~]# kubectl get pods volume-emptydir -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE ...... volume-emptydir 2/2 Running 0 97s 10.42.2.9 node1 ......# 通过podIp访问nginx[root@k8s-master01 ~]# curl 10.42.2.9......# 通过kubectl logs命令查看指定容器的标准输出[root@k8s-master01 ~]# kubectl logs -f volume-emptydir -n dev -c busybox10.42.1.0 - - [27/Jun/2021:15:08:54 +0000] &quot;GET / HTTP/1.1&quot; 200 612 &quot;-&quot; &quot;curl/7.29.0&quot; &quot;-&quot; 8.1.2 HostPath上节课提到，EmptyDir中数据不会被持久化，它会随着Pod的结束而销毁，如果想简单的将数据持久化到主机中，可以选择HostPath。 HostPath就是将Node主机中一个实际目录挂在到Pod中，以供容器使用，这样的设计就可以保证Pod销毁了，但是数据依据可以存在于Node主机上。 创建一个volume-hostpath.yaml： 12345678910111213141516171819202122232425apiVersion: v1kind: Podmetadata: name: volume-hostpath namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 volumeMounts: - name: logs-volume mountPath: /var/log/nginx - name: busybox image: busybox:1.30 command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;tail -f /logs/access.log&quot;] volumeMounts: - name: logs-volume mountPath: /logs volumes: - name: logs-volume hostPath: path: /root/logs type: DirectoryOrCreate # 目录存在就使用，不存在就先创建后使用 12345678关于type的值的一点说明： DirectoryOrCreate 目录存在就使用，不存在就先创建后使用 Directory 目录必须存在 FileOrCreate 文件存在就使用，不存在就先创建后使用 File 文件必须存在 Socket unix套接字必须存在 CharDevice 字符设备必须存在 BlockDevice 块设备必须存在 1234567891011121314151617181920# 创建Pod[root@k8s-master01 ~]# kubectl create -f volume-hostpath.yamlpod/volume-hostpath created# 查看Pod[root@k8s-master01 ~]# kubectl get pods volume-hostpath -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE ......pod-volume-hostpath 2/2 Running 0 16s 10.42.2.10 node1 ......#访问nginx[root@k8s-master01 ~]# curl 10.42.2.10[root@k8s-master01 ~]# kubectl logs -f volume-emptydir -n dev -c busybox# 接下来就可以去host的/root/logs目录下查看存储的文件了### 注意: 下面的操作需要到Pod所在的节点运行（案例中是node1）[root@node1 ~]# ls /root/logs/access.log error.log# 同样的道理，如果在此目录下创建一个文件，到容器中也是可以看到的 8.1.3 NFSHostPath可以解决数据持久化的问题，但是一旦Node节点故障了，Pod如果转移到了别的节点，又会出现问题了，此时需要准备单独的网络存储系统，比较常用的用NFS、CIFS。 NFS是一个网络文件存储系统，可以搭建一台NFS服务器，然后将Pod中的存储直接连接到NFS系统上，这样的话，无论Pod在节点上怎么转移，只要Node跟NFS的对接没问题，数据就可以成功访问。 1）首先要准备nfs的服务器，这里为了简单，直接是master节点做nfs服务器 12345678910111213# 在nfs上安装nfs服务[root@nfs ~]# yum install nfs-utils -y# 准备一个共享目录[root@nfs ~]# mkdir /root/data/nfs -pv# 将共享目录以读写权限暴露给192.168.5.0/24网段中的所有主机[root@nfs ~]# vim /etc/exports[root@nfs ~]# more /etc/exports/root/data/nfs 192.168.5.0/24(rw,no_root_squash)# 启动nfs服务[root@nfs ~]# systemctl restart nfs 2）接下来，要在的每个node节点上都安装下nfs，这样的目的是为了node节点可以驱动nfs设备 12# 在node上安装nfs服务，注意不需要启动[root@k8s-master01 ~]# yum install nfs-utils -y 3）接下来，就可以编写pod的配置文件了，创建volume-nfs.yaml 12345678910111213141516171819202122232425apiVersion: v1kind: Podmetadata: name: volume-nfs namespace: devspec: containers: - name: nginx image: nginx:1.17.1 ports: - containerPort: 80 volumeMounts: - name: logs-volume mountPath: /var/log/nginx - name: busybox image: busybox:1.30 command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;tail -f /logs/access.log&quot;] volumeMounts: - name: logs-volume mountPath: /logs volumes: - name: logs-volume nfs: server: 192.168.5.6 #nfs服务器地址 path: /root/data/nfs #共享文件路径 4）最后，运行下pod，观察结果 123456789101112# 创建pod[root@k8s-master01 ~]# kubectl create -f volume-nfs.yamlpod/volume-nfs created# 查看pod[root@k8s-master01 ~]# kubectl get pods volume-nfs -n devNAME READY STATUS RESTARTS AGEvolume-nfs 2/2 Running 0 2m9s# 查看nfs服务器上的共享目录，发现已经有文件了[root@k8s-master01 ~]# ls /root/data/access.log error.log 8.2 高级存储前面已经学习了使用NFS提供存储，此时就要求用户会搭建NFS系统，并且会在yaml配置nfs。由于kubernetes支持的存储系统有很多，要求客户全都掌握，显然不现实。为了能够屏蔽底层存储实现的细节，方便用户使用， kubernetes引入PV和PVC两种资源对象。 PV（Persistent Volume）是持久化卷的意思，是对底层的共享存储的一种抽象。一般情况下PV由kubernetes管理员进行创建和配置，它与底层具体的共享存储技术有关，并通过插件完成与共享存储的对接。 PVC（Persistent Volume Claim）是持久卷声明的意思，是用户对于存储需求的一种声明。换句话说，PVC其实就是用户向kubernetes系统发出的一种资源需求申请。 使用了PV和PVC之后，工作可以得到进一步的细分： 存储：存储工程师维护 PV： kubernetes管理员维护 PVC：kubernetes用户维护 8.2.1 PVPV是存储资源的抽象，下面是资源清单文件: 1234567891011apiVersion: v1 kind: PersistentVolumemetadata: name: pv2spec: nfs: # 存储类型，与底层真正存储对应 capacity: # 存储能力，目前只支持存储空间的设置 storage: 2Gi accessModes: # 访问模式 storageClassName: # 存储类别 persistentVolumeReclaimPolicy: # 回收策略 PV 的关键配置参数说明： 存储类型 底层实际存储的类型，kubernetes支持多种存储类型，每种存储类型的配置都有所差异 存储能力（capacity） 目前只支持存储空间的设置( storage=1Gi )，不过未来可能会加入IOPS、吞吐量等指标的配置 访问模式（accessModes） 用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式： ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载 ReadOnlyMany（ROX）： 只读权限，可以被多个节点挂载 ReadWriteMany（RWX）：读写权限，可以被多个节点挂载 需要注意的是，底层不同的存储类型可能支持的访问模式不同 回收策略（persistentVolumeReclaimPolicy） 当PV不再被使用了之后，对其的处理方式。目前支持三种策略： Retain （保留） 保留数据，需要管理员手工清理数据 Recycle（回收） 清除 PV 中的数据，效果相当于执行 rm -rf /thevolume/* Delete （删除） 与 PV 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务 需要注意的是，底层不同的存储类型可能支持的回收策略不同 存储类别 PV可以通过storageClassName参数指定一个存储类别 具有特定类别的PV只能与请求了该类别的PVC进行绑定 未设定类别的PV则只能与不请求任何类别的PVC进行绑定 状态（status） 一个 PV 的生命周期中，可能会处于4中不同的阶段： Available（可用）： 表示可用状态，还未被任何 PVC 绑定 Bound（已绑定）： 表示 PV 已经被 PVC 绑定 Released（已释放）： 表示 PVC 被删除，但是资源还未被集群重新声明 Failed（失败）： 表示该 PV 的自动回收失败 实验 使用NFS作为存储，来演示PV的使用，创建3个PV，对应NFS中的3个暴露的路径。 准备NFS环境 1234567891011# 创建目录[root@nfs ~]# mkdir /root/data/&#123;pv1,pv2,pv3&#125; -pv# 暴露服务[root@nfs ~]# more /etc/exports/root/data/pv1 192.168.5.0/24(rw,no_root_squash)/root/data/pv2 192.168.5.0/24(rw,no_root_squash)/root/data/pv3 192.168.5.0/24(rw,no_root_squash)# 重启服务[root@nfs ~]# systemctl restart nfs 创建pv.yaml 123456789101112131415161718192021222324252627282930313233343536373839404142434445apiVersion: v1kind: PersistentVolumemetadata: name: pv1spec: capacity: storage: 1Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain nfs: path: /root/data/pv1 server: 192.168.5.6---apiVersion: v1kind: PersistentVolumemetadata: name: pv2spec: capacity: storage: 2Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain nfs: path: /root/data/pv2 server: 192.168.5.6 ---apiVersion: v1kind: PersistentVolumemetadata: name: pv3spec: capacity: storage: 3Gi accessModes: - ReadWriteMany persistentVolumeReclaimPolicy: Retain nfs: path: /root/data/pv3 server: 192.168.5.6 123456789101112# 创建 pv[root@k8s-master01 ~]# kubectl create -f pv.yamlpersistentvolume/pv1 createdpersistentvolume/pv2 createdpersistentvolume/pv3 created# 查看pv[root@k8s-master01 ~]# kubectl get pv -o wideNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS AGE VOLUMEMODEpv1 1Gi RWX Retain Available 10s Filesystempv2 2Gi RWX Retain Available 10s Filesystempv3 3Gi RWX Retain Available 9s Filesystem 8.2.2 PVCPVC是资源的申请，用来声明对存储空间、访问模式、存储类别需求信息。下面是资源清单文件: 123456789101112apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvc namespace: devspec: accessModes: # 访问模式 selector: # 采用标签对PV选择 storageClassName: # 存储类别 resources: # 请求空间 requests: storage: 5Gi PVC 的关键配置参数说明： 访问模式（accessModes） 用于描述用户应用对存储资源的访问权限 选择条件（selector） 通过Label Selector的设置，可使PVC对于系统中己存在的PV进行筛选 存储类别（storageClassName） PVC在定义时可以设定需要的后端存储的类别，只有设置了该class的pv才能被系统选出 资源请求（Resources ） 描述对存储资源的请求 实验 创建pvc.yaml，申请pv 1234567891011121314151617181920212223242526272829303132333435apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvc1 namespace: devspec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvc2 namespace: devspec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi---apiVersion: v1kind: PersistentVolumeClaimmetadata: name: pvc3 namespace: devspec: accessModes: - ReadWriteMany resources: requests: storage: 1Gi 12345678910111213141516171819# 创建pvc[root@k8s-master01 ~]# kubectl create -f pvc.yamlpersistentvolumeclaim/pvc1 createdpersistentvolumeclaim/pvc2 createdpersistentvolumeclaim/pvc3 created# 查看pvc[root@k8s-master01 ~]# kubectl get pvc -n dev -o wideNAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE VOLUMEMODEpvc1 Bound pv1 1Gi RWX 15s Filesystempvc2 Bound pv2 2Gi RWX 15s Filesystempvc3 Bound pv3 3Gi RWX 15s Filesystem# 查看pv[root@k8s-master01 ~]# kubectl get pv -o wideNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM AGE VOLUMEMODEpv1 1Gi RWx Retain Bound dev/pvc1 3h37m Filesystempv2 2Gi RWX Retain Bound dev/pvc2 3h37m Filesystempv3 3Gi RWX Retain Bound dev/pvc3 3h37m Filesystem 创建pods.yaml, 使用pv 12345678910111213141516171819202122232425262728293031323334353637apiVersion: v1kind: Podmetadata: name: pod1 namespace: devspec: containers: - name: busybox image: busybox:1.30 command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;while true;do echo pod1 &gt;&gt; /root/out.txt; sleep 10; done;&quot;] volumeMounts: - name: volume mountPath: /root/ volumes: - name: volume persistentVolumeClaim: claimName: pvc1 readOnly: false---apiVersion: v1kind: Podmetadata: name: pod2 namespace: devspec: containers: - name: busybox image: busybox:1.30 command: [&quot;/bin/sh&quot;,&quot;-c&quot;,&quot;while true;do echo pod2 &gt;&gt; /root/out.txt; sleep 10; done;&quot;] volumeMounts: - name: volume mountPath: /root/ volumes: - name: volume persistentVolumeClaim: claimName: pvc2 readOnly: false 1234567891011121314151617181920212223242526272829303132# 创建pod[root@k8s-master01 ~]# kubectl create -f pods.yamlpod/pod1 createdpod/pod2 created# 查看pod[root@k8s-master01 ~]# kubectl get pods -n dev -o wideNAME READY STATUS RESTARTS AGE IP NODE pod1 1/1 Running 0 14s 10.244.1.69 node1 pod2 1/1 Running 0 14s 10.244.1.70 node1 # 查看pvc[root@k8s-master01 ~]# kubectl get pvc -n dev -o wideNAME STATUS VOLUME CAPACITY ACCESS MODES AGE VOLUMEMODEpvc1 Bound pv1 1Gi RWX 94m Filesystempvc2 Bound pv2 2Gi RWX 94m Filesystempvc3 Bound pv3 3Gi RWX 94m Filesystem# 查看pv[root@k8s-master01 ~]# kubectl get pv -n dev -o wideNAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM AGE VOLUMEMODEpv1 1Gi RWX Retain Bound dev/pvc1 5h11m Filesystempv2 2Gi RWX Retain Bound dev/pvc2 5h11m Filesystempv3 3Gi RWX Retain Bound dev/pvc3 5h11m Filesystem# 查看nfs中的文件存储[root@nfs ~]# more /root/data/pv1/out.txtnode1node1[root@nfs ~]# more /root/data/pv2/out.txtnode2node2 8.2.3 生命周期PVC和PV是一一对应的，PV和PVC之间的相互作用遵循以下生命周期： 资源供应：管理员手动创建底层存储和PV 资源绑定：用户创建PVC，kubernetes负责根据PVC的声明去寻找PV，并绑定 在用户定义好PVC之后，系统将根据PVC对存储资源的请求在已存在的PV中选择一个满足条件的 一旦找到，就将该PV与用户定义的PVC进行绑定，用户的应用就可以使用这个PVC了 如果找不到，PVC则会无限期处于Pending状态，直到等到系统管理员创建了一个符合其要求的PV PV一旦绑定到某个PVC上，就会被这个PVC独占，不能再与其他PVC进行绑定了 资源使用：用户可在pod中像volume一样使用pvc Pod使用Volume的定义，将PVC挂载到容器内的某个路径进行使用。 资源释放：用户删除pvc来释放pv 当存储资源使用完毕后，用户可以删除PVC，与该PVC绑定的PV将会被标记为“已释放”，但还不能立刻与其他PVC进行绑定。通过之前PVC写入的数据可能还被留在存储设备上，只有在清除之后该PV才能再次使用。 资源回收：kubernetes根据pv设置的回收策略进行资源的回收 对于PV，管理员可以设定回收策略，用于设置与之绑定的PVC释放资源之后如何处理遗留数据的问题。只有PV的存储空间完成回收，才能供新的PVC绑定和使用 8.3 配置存储8.3.1 ConfigMapConfigMap是一种比较特殊的存储卷，它的主要作用是用来存储配置信息的。 创建configmap.yaml，内容如下： 123456789apiVersion: v1kind: ConfigMapmetadata: name: configmap namespace: devdata: info: | username:admin password:123456 接下来，使用此配置文件创建configmap 12345678910111213141516171819# 创建configmap[root@k8s-master01 ~]# kubectl create -f configmap.yamlconfigmap/configmap created# 查看configmap详情[root@k8s-master01 ~]# kubectl describe cm configmap -n devName: configmapNamespace: devLabels: &lt;none&gt;Annotations: &lt;none&gt;Data====info:----username:adminpassword:123456Events: &lt;none&gt; 接下来创建一个pod-configmap.yaml，将上面创建的configmap挂载进去 12345678910111213141516apiVersion: v1kind: Podmetadata: name: pod-configmap namespace: devspec: containers: - name: nginx image: nginx:1.17.1 volumeMounts: # 将configmap挂载到目录 - name: config mountPath: /configmap/config volumes: # 引用configmap - name: config configMap: name: configmap 123456789101112131415161718192021# 创建pod[root@k8s-master01 ~]# kubectl create -f pod-configmap.yamlpod/pod-configmap created# 查看pod[root@k8s-master01 ~]# kubectl get pod pod-configmap -n devNAME READY STATUS RESTARTS AGEpod-configmap 1/1 Running 0 6s#进入容器[root@k8s-master01 ~]# kubectl exec -it pod-configmap -n dev /bin/sh# cd /configmap/config/# lsinfo# more infousername:adminpassword:123456# 可以看到映射已经成功，每个configmap都映射成了一个目录# key---&gt;文件 value----&gt;文件中的内容# 此时如果更新configmap的内容, 容器中的值也会动态更新 8.3.2 Secret在kubernetes中，还存在一种和ConfigMap非常类似的对象，称为Secret对象。它主要用于存储敏感信息，例如密码、秘钥、证书等等。 首先使用base64对数据进行编码 1234[root@k8s-master01 ~]# echo -n &#x27;admin&#x27; | base64 #准备usernameYWRtaW4=[root@k8s-master01 ~]# echo -n &#x27;123456&#x27; | base64 #准备passwordMTIzNDU2 接下来编写secret.yaml，并创建Secret 123456789apiVersion: v1kind: Secretmetadata: name: secret namespace: devtype: Opaquedata: username: YWRtaW4= password: MTIzNDU2 123456789101112131415# 创建secret[root@k8s-master01 ~]# kubectl create -f secret.yamlsecret/secret created# 查看secret详情[root@k8s-master01 ~]# kubectl describe secret secret -n devName: secretNamespace: devLabels: &lt;none&gt;Annotations: &lt;none&gt;Type: OpaqueData====password: 6 bytesusername: 5 bytes 创建pod-secret.yaml，将上面创建的secret挂载进去： 12345678910111213141516apiVersion: v1kind: Podmetadata: name: pod-secret namespace: devspec: containers: - name: nginx image: nginx:1.17.1 volumeMounts: # 将secret挂载到目录 - name: config mountPath: /secret/config volumes: - name: config secret: secretName: secret 1234567891011121314151617# 创建pod[root@k8s-master01 ~]# kubectl create -f pod-secret.yamlpod/pod-secret created# 查看pod[root@k8s-master01 ~]# kubectl get pod pod-secret -n devNAME READY STATUS RESTARTS AGEpod-secret 1/1 Running 0 2m28s# 进入容器，查看secret信息，发现已经自动解码了[root@k8s-master01 ~]# kubectl exec -it pod-secret /bin/sh -n dev/ # ls /secret/config/password username/ # more /secret/config/usernameadmin/ # more /secret/config/password123456 至此，已经实现了利用secret实现了信息的编码。 9. 安全认证9.1 访问控制概述Kubernetes作为一个分布式集群的管理工具，保证集群的安全性是其一个重要的任务。所谓的安全性其实就是保证对Kubernetes的各种客户端进行认证和鉴权操作。 客户端 在Kubernetes集群中，客户端通常有两类： User Account：一般是独立于kubernetes之外的其他服务管理的用户账号。 Service Account：kubernetes管理的账号，用于为Pod中的服务进程在访问Kubernetes时提供身份标识。 认证、授权与准入控制 ApiServer是访问及管理资源对象的唯一入口。任何一个请求访问ApiServer，都要经过下面三个流程： Authentication（认证）：身份鉴别，只有正确的账号才能够通过认证 Authorization（授权）： 判断用户是否有权限对访问的资源执行特定的动作 Admission Control（准入控制）：用于补充授权机制以实现更加精细的访问控制功能。 9.2 认证管理Kubernetes集群安全的最关键点在于如何识别并认证客户端身份，它提供了3种客户端身份认证方式： HTTP Base认证：通过用户名+密码的方式认证 1这种认证方式是把“用户名:密码”用BASE64算法进行编码后的字符串放在HTTP请求中的Header Authorization域里发送给服务端。服务端收到后进行解码，获取用户名及密码，然后进行用户身份认证的过程。 HTTP Token认证：通过一个Token来识别合法用户 1这种认证方式是用一个很长的难以被模仿的字符串--Token来表明客户身份的一种方式。每个Token对应一个用户名，当客户端发起API调用请求时，需要在HTTP Header里放入Token，API Server接到Token后会跟服务器中保存的token进行比对，然后进行用户身份认证的过程。 HTTPS证书认证：基于CA根证书签名的双向数字证书认证方式 1这种认证方式是安全性最高的一种方式，但是同时也是操作起来最麻烦的一种方式。 HTTPS认证大体分为3个过程： 证书申请和下发 1HTTPS通信双方的服务器向CA机构申请证书，CA机构下发根证书、服务端证书及私钥给申请者 客户端和服务端的双向认证 123451&gt; 客户端向服务器端发起请求，服务端下发自己的证书给客户端， 客户端接收到证书后，通过私钥解密证书，在证书中获得服务端的公钥， 客户端利用服务器端的公钥认证证书中的信息，如果一致，则认可这个服务器2&gt; 客户端发送自己的证书给服务器端，服务端接收到证书后，通过私钥解密证书， 在证书中获得客户端的公钥，并用该公钥认证证书信息，确认客户端是否合法 服务器端和客户端进行通信 12服务器端和客户端协商好加密方案后，客户端会产生一个随机的秘钥并加密，然后发送到服务器端。服务器端接收这个秘钥后，双方接下来通信的所有内容都通过该随机秘钥加密 注意: Kubernetes允许同时配置多种认证方式，只要其中任意一个方式认证通过即可 9.3 授权管理授权发生在认证成功之后，通过认证就可以知道请求用户是谁， 然后Kubernetes会根据事先定义的授权策略来决定用户是否有权限访问，这个过程就称为授权。 每个发送到ApiServer的请求都带上了用户和资源的信息：比如发送请求的用户、请求的路径、请求的动作等，授权就是根据这些信息和授权策略进行比较，如果符合策略，则认为授权通过，否则会返回错误。 API Server目前支持以下几种授权策略： AlwaysDeny：表示拒绝所有请求，一般用于测试 AlwaysAllow：允许接收所有请求，相当于集群不需要授权流程（Kubernetes默认的策略） ABAC：基于属性的访问控制，表示使用用户配置的授权规则对用户请求进行匹配和控制 Webhook：通过调用外部REST服务对用户进行授权 Node：是一种专用模式，用于对kubelet发出的请求进行访问控制 RBAC：基于角色的访问控制（kubeadm安装方式下的默认选项） RBAC(Role-Based Access Control) 基于角色的访问控制，主要是在描述一件事情：给哪些对象授予了哪些权限 其中涉及到了下面几个概念： 对象：User、Groups、ServiceAccount 角色：代表着一组定义在资源上的可操作动作(权限)的集合 绑定：将定义好的角色跟用户绑定在一起 RBAC引入了4个顶级资源对象： Role、ClusterRole：角色，用于指定一组权限 RoleBinding、ClusterRoleBinding：角色绑定，用于将角色（权限）赋予给对象 Role、ClusterRole 一个角色就是一组权限的集合，这里的权限都是许可形式的（白名单）。 12345678910# Role只能对命名空间内的资源进行授权，需要指定nameapcekind: RoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: namespace: dev name: authorization-rolerules:- apiGroups: [&quot;&quot;] # 支持的API组列表,&quot;&quot; 空字符串，表示核心API群 resources: [&quot;pods&quot;] # 支持的资源对象列表 verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] # 允许的对资源对象的操作方法列表 123456789# ClusterRole可以对集群范围内资源、跨namespaces的范围资源、非资源类型进行授权kind: ClusterRoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: authorization-clusterrolerules:- apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] 需要详细说明的是，rules中的参数： apiGroups: 支持的API组列表 1&quot;&quot;,&quot;apps&quot;, &quot;autoscaling&quot;, &quot;batch&quot; resources：支持的资源对象列表 123&quot;services&quot;, &quot;endpoints&quot;, &quot;pods&quot;,&quot;secrets&quot;,&quot;configmaps&quot;,&quot;crontabs&quot;,&quot;deployments&quot;,&quot;jobs&quot;,&quot;nodes&quot;,&quot;rolebindings&quot;,&quot;clusterroles&quot;,&quot;daemonsets&quot;,&quot;replicasets&quot;,&quot;statefulsets&quot;,&quot;horizontalpodautoscalers&quot;,&quot;replicationcontrollers&quot;,&quot;cronjobs&quot; verbs：对资源对象的操作方法列表 1&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;, &quot;delete&quot;, &quot;exec&quot; RoleBinding、ClusterRoleBinding 角色绑定用来把一个角色绑定到一个目标对象上，绑定目标可以是User、Group或者ServiceAccount。 1234567891011121314# RoleBinding可以将同一namespace中的subject绑定到某个Role下，则此subject即具有该Role定义的权限kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: authorization-role-binding namespace: devsubjects:- kind: User name: heima apiGroup: rbac.authorization.k8s.ioroleRef: kind: Role name: authorization-role apiGroup: rbac.authorization.k8s.io 12345678910111213# ClusterRoleBinding在整个集群级别和所有namespaces将特定的subject与ClusterRole绑定，授予权限kind: ClusterRoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: authorization-clusterrole-bindingsubjects:- kind: User name: heima apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: authorization-clusterrole apiGroup: rbac.authorization.k8s.io RoleBinding引用ClusterRole进行授权 RoleBinding可以引用ClusterRole，对属于同一命名空间内ClusterRole定义的资源主体进行授权。 1一种很常用的做法就是，集群管理员为集群范围预定义好一组角色（ClusterRole），然后在多个命名空间中重复使用这些ClusterRole。这样可以大幅提高授权管理工作效率，也使得各个命名空间下的基础性授权规则与使用体验保持一致。 123456789101112131415# 虽然authorization-clusterrole是一个集群角色，但是因为使用了RoleBinding# 所以heima只能读取dev命名空间中的资源kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: authorization-role-binding-ns namespace: devsubjects:- kind: User name: heima apiGroup: rbac.authorization.k8s.ioroleRef: kind: ClusterRole name: authorization-clusterrole apiGroup: rbac.authorization.k8s.io 实战：创建一个只能管理dev空间下Pods资源的账号 创建账号 12345678910111213141516171819202122232425262728# 1) 创建证书[root@k8s-master01 pki]# cd /etc/kubernetes/pki/[root@k8s-master01 pki]# (umask 077;openssl genrsa -out devman.key 2048)# 2) 用apiserver的证书去签署# 2-1) 签名申请，申请的用户是devman,组是devgroup[root@k8s-master01 pki]# openssl req -new -key devman.key -out devman.csr -subj &quot;/CN=devman/O=devgroup&quot; # 2-2) 签署证书[root@k8s-master01 pki]# openssl x509 -req -in devman.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out devman.crt -days 3650# 3) 设置集群、用户、上下文信息[root@k8s-master01 pki]# kubectl config set-cluster kubernetes --embed-certs=true --certificate-authority=/etc/kubernetes/pki/ca.crt --server=https://192.168.109.100:6443[root@k8s-master01 pki]# kubectl config set-credentials devman --embed-certs=true --client-certificate=/etc/kubernetes/pki/devman.crt --client-key=/etc/kubernetes/pki/devman.key[root@k8s-master01 pki]# kubectl config set-context devman@kubernetes --cluster=kubernetes --user=devman# 切换账户到devman[root@k8s-master01 pki]# kubectl config use-context devman@kubernetesSwitched to context &quot;devman@kubernetes&quot;.# 查看dev下pod，发现没有权限[root@k8s-master01 pki]# kubectl get pods -n devError from server (Forbidden): pods is forbidden: User &quot;devman&quot; cannot list resource &quot;pods&quot; in API group &quot;&quot; in the namespace &quot;dev&quot;# 切换到admin账户[root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetesSwitched to context &quot;kubernetes-admin@kubernetes&quot;. 2） 创建Role和RoleBinding，为devman用户授权 12345678910111213141516171819202122232425kind: RoleapiVersion: rbac.authorization.k8s.io/v1beta1metadata: namespace: dev name: dev-rolerules:- apiGroups: [&quot;&quot;] resources: [&quot;pods&quot;] verbs: [&quot;get&quot;, &quot;watch&quot;, &quot;list&quot;] ---kind: RoleBindingapiVersion: rbac.authorization.k8s.io/v1beta1metadata: name: authorization-role-binding namespace: devsubjects:- kind: User name: devman apiGroup: rbac.authorization.k8s.ioroleRef: kind: Role name: dev-role apiGroup: rbac.authorization.k8s.io 123[root@k8s-master01 pki]# kubectl create -f dev-role.yamlrole.rbac.authorization.k8s.io/dev-role createdrolebinding.rbac.authorization.k8s.io/authorization-role-binding created 切换账户，再次验证 1234567891011121314# 切换账户到devman[root@k8s-master01 pki]# kubectl config use-context devman@kubernetesSwitched to context &quot;devman@kubernetes&quot;.# 再次查看[root@k8s-master01 pki]# kubectl get pods -n devNAME READY STATUS RESTARTS AGEnginx-deployment-66cb59b984-8wp2k 1/1 Running 0 4d1hnginx-deployment-66cb59b984-dc46j 1/1 Running 0 4d1hnginx-deployment-66cb59b984-thfck 1/1 Running 0 4d1h# 为了不影响后面的学习,切回admin账户[root@k8s-master01 pki]# kubectl config use-context kubernetes-admin@kubernetesSwitched to context &quot;kubernetes-admin@kubernetes&quot;. 9.4 准入控制通过了前面的认证和授权之后，还需要经过准入控制处理通过之后，apiserver才会处理这个请求。 准入控制是一个可配置的控制器列表，可以通过在Api-Server上通过命令行设置选择执行哪些准入控制器： 12--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,PersistentVolumeLabel, DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds 只有当所有的准入控制器都检查通过之后，apiserver才执行该请求，否则返回拒绝。 当前可配置的Admission Control准入控制如下： AlwaysAdmit：允许所有请求 AlwaysDeny：禁止所有请求，一般用于测试 AlwaysPullImages：在启动容器之前总去下载镜像 DenyExecOnPrivileged：它会拦截所有想在Privileged Container上执行命令的请求 ImagePolicyWebhook：这个插件将允许后端的一个Webhook程序来完成admission controller的功能。 Service Account：实现ServiceAccount实现了自动化 SecurityContextDeny：这个插件将使用SecurityContext的Pod中的定义全部失效 ResourceQuota：用于资源配额管理目的，观察所有请求，确保在namespace上的配额不会超标 LimitRanger：用于资源限制管理，作用于namespace上，确保对Pod进行资源限制 InitialResources：为未设置资源请求与限制的Pod，根据其镜像的历史资源的使用情况进行设置 NamespaceLifecycle：如果尝试在一个不存在的namespace中创建资源对象，则该创建请求将被拒绝。当删除一个namespace时，系统将会删除该namespace中所有对象。 DefaultStorageClass：为了实现共享存储的动态供应，为未指定StorageClass或PV的PVC尝试匹配默认的StorageClass，尽可能减少用户在申请PVC时所需了解的后端存储细节 DefaultTolerationSeconds：这个插件为那些没有设置forgiveness tolerations并具有notready:NoExecute和unreachable:NoExecute两种taints的Pod设置默认的“容忍”时间，为5min PodSecurityPolicy：这个插件用于在创建或修改Pod时决定是否根据Pod的security context和可用的PodSecurityPolicy对Pod的安全策略进行控制 10. DashBoard之前在kubernetes中完成的所有操作都是通过命令行工具kubectl完成的。其实，为了提供更丰富的用户体验，kubernetes还开发了一个基于web的用户界面（Dashboard）。用户可以使用Dashboard部署容器化的应用，还可以监控应用的状态，执行故障排查以及管理kubernetes中各种资源。 10.1 部署Dashboard 下载yaml，并运行Dashboard 1234567891011121314151617181920212223242526272829303132# 下载yaml[root@k8s-master01 ~]# wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0/aio/deploy/recommended.yaml# 修改kubernetes-dashboard的Service类型kind: ServiceapiVersion: v1metadata: labels: k8s-app: kubernetes-dashboard name: kubernetes-dashboard namespace: kubernetes-dashboardspec: type: NodePort # 新增 ports: - port: 443 targetPort: 8443 nodePort: 30009 # 新增 selector: k8s-app: kubernetes-dashboard# 部署[root@k8s-master01 ~]# kubectl create -f recommended.yaml# 查看namespace下的kubernetes-dashboard下的资源[root@k8s-master01 ~]# kubectl get pod,svc -n kubernetes-dashboardNAME READY STATUS RESTARTS AGEpod/dashboard-metrics-scraper-c79c65bb7-zwfvw 1/1 Running 0 111spod/kubernetes-dashboard-56484d4c5-z95z5 1/1 Running 0 111sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/dashboard-metrics-scraper ClusterIP 10.96.89.218 &lt;none&gt; 8000/TCP 111sservice/kubernetes-dashboard NodePort 10.104.178.171 &lt;none&gt; 443:30009/TCP 111s 2）创建访问账户，获取token 123456789101112131415161718192021222324# 创建账号[root@k8s-master01-1 ~]# kubectl create serviceaccount dashboard-admin -n kubernetes-dashboard# 授权[root@k8s-master01-1 ~]# kubectl create clusterrolebinding dashboard-admin-rb --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:dashboard-admin# 获取账号token[root@k8s-master01 ~]# kubectl get secrets -n kubernetes-dashboard | grep dashboard-admindashboard-admin-token-xbqhh kubernetes.io/service-account-token 3 2m35s[root@k8s-master01 ~]# kubectl describe secrets dashboard-admin-token-xbqhh -n kubernetes-dashboardName: dashboard-admin-token-xbqhhNamespace: kubernetes-dashboardLabels: &lt;none&gt;Annotations: kubernetes.io/service-account.name: dashboard-admin kubernetes.io/service-account.uid: 95d84d80-be7a-4d10-a2e0-68f90222d039Type: kubernetes.io/service-account-tokenData====namespace: 20 bytestoken: eyJhbGciOiJSUzI1NiIsImtpZCI6ImJrYkF4bW5XcDhWcmNGUGJtek5NODFuSXl1aWptMmU2M3o4LTY5a2FKS2cifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJkYXNoYm9hcmQtYWRtaW4tdG9rZW4teGJxaGgiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC5uYW1lIjoiZGFzaGJvYXJkLWFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiOTVkODRkODAtYmU3YS00ZDEwLWEyZTAtNjhmOTAyMjJkMDM5Iiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50Omt1YmVybmV0ZXMtZGFzaGJvYXJkOmRhc2hib2FyZC1hZG1pbiJ9.NAl7e8ZfWWdDoPxkqzJzTB46sK9E8iuJYnUI9vnBaY3Jts7T1g1msjsBnbxzQSYgAG--cV0WYxjndzJY_UWCwaGPrQrt_GunxmOK9AUnzURqm55GR2RXIZtjsWVP2EBatsDgHRmuUbQvTFOvdJB4x3nXcYLN2opAaMqg3rnU2rr-A8zCrIuX_eca12wIp_QiuP3SF-tzpdLpsyRfegTJZl6YnSGyaVkC9id-cxZRb307qdCfXPfCHR_2rt5FVfxARgg_C0e3eFHaaYQO7CitxsnIoIXpOFNAR8aUrmopJyODQIPqBWUehb7FhlU1DCduHnIIXVC_UICZ-MKYewBDLwca.crt: 1025 bytes 3）通过浏览器访问Dashboard的UI 在登录页面上输入上面的token 出现下面的页面代表成功 10.2 使用DashBoard本章节以Deployment为例演示DashBoard的使用 查看 选择指定的命名空间dev，然后点击Deployments，查看dev空间下的所有deployment 扩缩容 在Deployment上点击规模，然后指定目标副本数量，点击确定 编辑 在Deployment上点击编辑，然后修改yaml文件，点击确定 查看Pod 点击Pods, 查看pods列表 操作Pod 选中某个Pod，可以对其执行日志（logs）、进入执行（exec）、编辑、删除操作 Dashboard提供了kubectl的绝大部分功能，这里不再一一演示 原视频地址 原文章地址","categories":[],"tags":[],"author":"lwy"},{"title":"redis相关知识","slug":"redis相关知识","date":"2022-02-12T06:22:21.000Z","updated":"2022-02-13T13:26:58.487Z","comments":true,"path":"2022/02/12/redis相关知识/","link":"","permalink":"https://lwy0518.github.io/2022/02/12/redis%E7%9B%B8%E5%85%B3%E7%9F%A5%E8%AF%86/","excerpt":"目录1、NoSQL数据库简介2、Redis6概述和安装3、常用五大数据类型4、Redis6配置文件详解5、Redis6的发布和订阅6、Redis6新数据类型7、Jedis操作Redis68、Redis6与SpringBoot整合9、Redis6的事务操作10、Redis6持久化之RDB11、Redis6持久化之AOF12、Redis6的主从复制13、Redis6集群14、Redis6应用问题解决15、Redis6新功能","text":"目录1、NoSQL数据库简介2、Redis6概述和安装3、常用五大数据类型4、Redis6配置文件详解5、Redis6的发布和订阅6、Redis6新数据类型7、Jedis操作Redis68、Redis6与SpringBoot整合9、Redis6的事务操作10、Redis6持久化之RDB11、Redis6持久化之AOF12、Redis6的主从复制13、Redis6集群14、Redis6应用问题解决15、Redis6新功能 1、NoSQL数据库简介1.1、技术发展1.2、NoSQL数据库概述1.3、行式存储数据库1.4、图关系型数据库1.5、数据库排名2、Redis6概述和安装2.1、Redis6概述2.2、Redis6在Liunx系统安装3、常用五大数据类型3.1、键（key）操作3.2、Redis字符串(String)3.3、Redis列表(List)3.4、Redis集合(Set)3.5、Redis哈希(Hash)3.6、Redis有序集合Zset4、Redis6配置文件详解5、Redis6的发布和订阅5.1、什么是发布和订阅5.2、Redis的发布和订阅5.3、发布订阅命令行实现6、Redis6新数据类型6.1、Bitmaps6.2、HyperLogLog6.3、Geospatial7、Jedis操作Redis67.1、Jedis常用操作7.2、Jedis实例-模拟验证码发送8、Redis6与SpringBoot整合9、Redis6的事务操作9.1、Redis的事务定义9.2、事务操作Multi、Exec、discard9.3、事务的错误处理9.4、事务冲突的问题（乐观锁和悲观锁）9.5、Redis事务三特性9.6、Redis事务案例-商品秒杀案例10、Redis6持久化之RDB11、Redis6持久化之AOF12、Redis6的主从复制13、Redis6集群14、Redis6应用问题解决14.1、缓存穿透14.2、缓存击穿14.3、缓存雪崩14.4、分布式锁15、Redis6新功能15.1、ACL15.2、IO多线程15.3、工具支持 Cluster15.4、Redis新功能持续关注一、NoSQL数据库简介1.1 技术发展技术的分类 1、解决功能性的问题：Java、Jsp、RDBMS、Tomcat、HTML、Linux、JDBC、SVN 2、解决扩展性的问题：Struts、Spring、SpringMVC、Hibernate、Mybatis 3、解决性能的问题：NoSQL、Java线程、Hadoop、Nginx、MQ、ElasticSearch 1.1.1 Web1.0时代Web1.0的时代，数据访问量很有限，用一夫当关的高性能的单点服务器可以解决大部分问题。 1.1.2 Web2.0时代随着Web2.0的时代的到来，用户访问量大幅度提升，同时产生了大量的用户数据。加上后来的智能移动设备的普及，所有的互联网平台都面临了巨大的性能挑战。 1.1.3 解决CPU及内存压力 1.1.4 解决IO压力 1.2 NoSQL数据库1.2.1 NoSQL数据库概述NoSQL(NoSQL = Not Only SQL)，意即“不仅仅是SQL”，泛指非关系型的数据库。 NoSQL 不依赖业务逻辑方式存储，而以简单的key-value模式存储。因此大大的增加了数据库的扩展能力。 不遵循SQL标准。 不支持ACID。 远超于SQL的性能。 1.2.2 NoSQL适用场景 对数据高并发的读写 海量数据的读写 对数据高可扩展性的 1.2.3 NoSQL不适用场景 需要事务支持 基于sql的结构化查询存储，处理复杂的关系,需要即席查询。 用不着sql的和用了sql也不行的情况，请考虑用NoSql） 1.2.4 Memcache ü 很早出现的NoSql数据库ü 数据都在内存中，一般不持久化ü 支持简单的key-value模式，支持类型单一ü 一般是作为缓存数据库辅助持久化的数据库 1.2.5 Redis 几乎覆盖了Memcached的绝大部分功能数据都在内存中，支持持久化，主要用作备份恢复除了支持简单的key-value模式，还支持多种数据结构的存储，比如 list、set、hash、zset等。一般是作为缓存数据库辅助持久化的数据库 1.2.6 MongoDB ü 高性能、开源、模式自由(schema free)的文档型数据库ü 数据都在内存中， 如果内存不足，把不常用的数据保存到硬盘ü 虽然是key-value模式，但是对value（尤其是json）提供了丰富的查询功能ü 支持二进制数据及大型对象ü 可以根据数据的特点替代RDBMS ，成为独立的数据库。或者配合RDBMS，存储特定的数据。 1.3 行式存储数据库（大数据时代）1.3.2 行式数据库 1.3.3 列式数据库 Hbase HBase是Hadoop项目中的数据库。它用于需要对大量的数据进行随机、实时的读写操作的场景中。 HBase的目标就是处理数据量非常庞大的表，可以用普通的计算机处理超过10亿行数据，还可处理有数百万列元素的数据表。 Cassandra[kəˈsændrə] Apache Cassandra是一款免费的开源NoSQL数据库，其设计目的在于管理由大量商用服务器构建起来的庞大集群上的海量数据集(数据量通常达到PB级别)。在众多显著特性当中，Cassandra最为卓越的长处是对写入及读取操作进行规模调整，而且其不强调主集群的设计思路能够以相对直观的方式简化各集群的创建与扩展流程。 计算机存储单位 计算机存储单位一般用B，KB，MB，GB，TB，EB，ZB，YB，BB来表示，它们之间的关系是： 位 bit (比特)(Binary Digits)：存放一位二进制数，即 0 或 1，最小的存储单位。 字节 byte：8个二进制位为一个字节(B)，最常用的单位。 1KB (Kilobyte 千字节)=1024B， 1MB (Megabyte 兆字节 简称“兆”)=1024KB， 1GB (Gigabyte 吉字节 又称“千兆”)=1024MB， 1TB (Trillionbyte 万亿字节 太字节)=1024GB，其中1024=2^10 ( 2 的10次方)， 1PB（Petabyte 千万亿字节 拍字节）=1024TB， 1EB（Exabyte 百亿亿字节 艾字节）=1024PB， 1ZB (Zettabyte 十万亿亿字节 泽字节)= 1024 EB, 1YB (Jottabyte 一亿亿亿字节 尧字节)= 1024 ZB, 1BB (Brontobyte 一千亿亿亿字节)= 1024 YB. 注：“兆”为百万级数量单位。 1.4 图关系型数据库 主要应用：社会关系，公共交通网络，地图及网络拓谱(n*(n-1)/2) 1.5 DB-Engines 数据库排名http://db-engines.com/en/ranking 二、Redis概述安装Ø Redis是一个开源的key-value存储系统。 Ø 和Memcached类似，它支持存储的value类型相对更多，包括string(字符串)、list(链表)、set(集合)、zset(sorted set –有序集合)和hash（哈希类型）。 Ø 这些数据类型都支持push/pop、add/remove及取交集并集和差集及更丰富的操作，而且这些操作都是原子性的。 Ø 在此基础上，Redis支持各种不同方式的排序。 Ø 与memcached一样，为了保证效率，数据都是缓存在内存中。 Ø 区别的是Redis会周期性的把更新的数据写入磁盘或者把修改操作写入追加的记录文件。 Ø 并且在此基础上实现了master-slave(主从)同步。 2.1 应用场景2.1.1 配合关系型数据库做高速缓存Ø 高频次，热门访问的数据，降低数据库IO Ø 分布式架构，做session共享 2.1.2 多样的数据结构存储持久化数据 2.2 Redis安装 Redis官方网站 Redis中文官方网站 http://redis.io http://redis.cn/ 2.2.1 安装版本Ø 6.2.1 for Linux（redis-6.2.1.tar.gz） Ø 不用考虑在windows环境下对Redis的支持 2.2.2 安装步骤准备工作：下载安装最新版的gcc编译器 2.2.3 安装C 语言的编译环境 yum install centos-release-scl scl-utils-build yum install -y devtoolset-8-toolchain scl enable devtoolset-8 bash 测试 gcc版本 gcc –version 下载redis-6.2.1.tar.gz放/opt目录 解压命令：tar -zxvf redis-6.2.1.tar.gz 解压完成后进入目录：cd redis-6.2.1 在redis-6.2.1目录下再次执行make命令（只是编译好） 如果没有准备好C语言编译环境，make 会报错—Jemalloc/jemalloc.h：没有那个文件 解决方案：运行make distclean 在redis-6.2.1目录下再次执行make命令（只是编译好） 跳过make test 继续执行: make install 2.2.4 安装目录：/usr/local/bin查看默认安装目录： redis-benchmark:性能测试工具，可以在自己本子运行，看看自己本子性能如何 redis-check-aof：修复有问题的AOF文件，rdb和aof后面讲 redis-check-dump：修复有问题的dump.rdb文件 redis-sentinel：Redis集群使用 redis-server：Redis服务器启动命令 redis-cli：客户端，操作入口 2.2.5 前台启动（不推荐）前台启动，命令行窗口不能关闭，否则服务器停止 2.2.6 后台启动（推荐） 备份redis.conf 拷贝一份redis.conf到其他目录 cp /opt/redis-3.2.5/redis.conf /myredis 后台启动设置daemonize no改成yes 修改redis.conf(128行)文件将里面的daemonize no 改成 yes，让服务在后台启动 Redis启动 redis-server/myredis/redis.conf 用客户端访问：redis-cli 多个端口可以：redis-cli -p6379 测试验证： ping Redis关闭 单实例关闭：redis-cli shutdown 也可以进入终端后再关闭 多实例关闭，指定端口关闭：redis-cli -p 6379 shutdown 2.2.7 Redis介绍相关知识 端口6379从何而来Alessia Merz 默认16个数据库，类似数组下标从0开始，初始默认使用0号库使用命令 select 来切换数据库。如: select 8 统一密码管理，所有库同样密码。dbsize查看当前数据库的key的数量flushdb清空当前库flushall通杀全部库 Redis是单线程+多路IO复用技术 多路复用是指使用一个线程来检查多个文件描述符（Socket）的就绪状态，比如调用select和poll函数，传入多个文件描述符，如果有一个文件描述符就绪，则返回，否则阻塞直到超时。得到就绪状态后进行真正的操作可以在同一个线程里执行，也可以启动线程执行（比如使用线程池） 串行 vs 多线程+锁（memcached） vs 单线程+多路IO复用(Redis) （与Memcache三点不同: 支持多数据类型，支持持久化，单线程+多路IO复用） 三、常用五大数据类型哪里去获得redis常见数据类型操作命令http://www.redis.cn/commands.html 3.1 Redis键(key)**keys *** 查看当前库所有key (匹配：keys *1) exists key: 判断某个key是否存在 type key : 查看你的key是什么类型 del key : 删除指定的key数据 unlink key: 根据value选择非阻塞删除 仅将keys从keyspace元数据中删除，真正的删除会在后续异步操作。 expire key 10: 10秒钟：为给定的key设置过期时间 ttl key: 查看还有多少秒过期，-1表示永不过期，-2表示已过期 select: 命令切换数据库 dbsize: 查看当前数据库的key的数量 flushdb: 清空当前库 flushall: 通杀全部库 3.2 Redis字符串(String)3.2.1 简介String是Redis最基本的类型，你可以理解成与Memcached一模一样的类型，一个key对应一个value。 String类型是二进制安全的。意味着Redis的string可以包含任何数据。比如jpg图片或者序列化的对象。 String类型是Redis最基本的数据类型，一个Redis中字符串value最多可以是512M 3.2.2 常用命令set 添加键值对 *NX：当数据库中key不存在时，可以将key-value添加数据库 *XX：当数据库中key存在时，可以将key-value添加数据库，与NX参数互斥 *EX：key的超时秒数 *PX：key的超时毫秒数，与EX互斥 get 查询对应键值 append 将给定的 追加到原值的末尾 strlen 获得值的长度 setnx 只有在 key 不存在时 设置 key 的值 incr 将 key 中储存的数字值增1 只能对数字值操作，如果为空，新增值为1 decr 将 key 中储存的数字值减1 只能对数字值操作，如果为空，新增值为-1 incrby / decrby &lt;步长&gt;将 key 中储存的数字值增减。自定义步长。 原子性 所谓原子操作是指不会被线程调度机制打断的操作；这种操作一旦开始，就一直运行到结束，中间不会有任何 context switch （切换到另一个线程）。（1）在单线程中， 能够在单条指令中完成的操作都可以认为是”原子操作”，因为中断只能发生于指令之间。（2）在多线程中，不能被其它进程（线程）打断的操作就叫原子操作。Redis单命令的原子性主要得益于Redis的单线程。案例：java中的i++是否是原子操作？不是i=0;两个线程分别对i进行++100次,值是多少？ 2~200i=0i++i=99 i=1 i++i=2i=0 i++ i=1 i++ i=100 mset ….. 同时设置一个或多个 key-value对 mget ….. 同时获取一个或多个 value msetnx ….. 同时设置一个或多个 key-value 对，当且仅当所有给定 key 都不存在。 原子性，有一个失败则都失败 getrange &lt;起始位置&gt;&lt;结束位置&gt; 获得值的范围，类似java中的substring，前包，后包 setrange &lt;起始位置&gt; 用 覆写所储存的字符串值，从&lt;起始位置&gt;开始(索引从0开始)。 setex &lt;过期时间&gt; 设置键值的同时，设置过期时间，单位秒。 getset 以新换旧，设置了新值同时获得旧值。 3.2.3 数据结构String的数据结构为简单动态字符串(Simple Dynamic String,缩写SDS)。是可以修改的字符串，内部结构实现上类似于Java的ArrayList，采用预分配冗余空间的方式来减少内存的频繁分配. 如图中所示，内部为当前字符串实际分配的空间capacity一般要高于实际字符串长度len。当字符串长度小于1M时，扩容都是加倍现有的空间，如果超过1M，扩容时一次只会多扩1M的空间。需要注意的是字符串最大长度为512M。 3.3 Redis列表(List)3.3.1 简介单键多值 Redis 列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部（左边）或者尾部（右边）。 它的底层实际是个双向链表，对两端的操作性能很高，通过索引下标的操作中间的节点性能会较差。 3.3.2 常用命令 lpush/rpush …. 从左边/右边插入一个或多个值。 lpop/rpop 从左边/右边吐出一个值。值在键在，值光键亡。 rpoplpush 从列表右边吐出一个值，插到列表左边。 lrange 按照索引下标获得元素(从左到右) lrange mylist 0 -1 0左边第一个，-1右边第一个，（0-1表示获取所有） lindex 按照索引下标获得元素(从左到右) llen 获得列表长度 linsert before 在的后面插入插入值 lrem 从左边删除n个value(从左到右) lset将列表key下标为index的值替换成value 3.3.3 数据结构List的数据结构为快速链表quickList。 首先在列表元素较少的情况下会使用一块连续的内存存储，这个结构是ziplist，也即是压缩列表。 它将所有的元素紧挨着一起存储，分配的是一块连续的内存。 当数据量比较多的时候才会改成quicklist。 因为普通的链表需要的附加指针空间太大，会比较浪费空间。比如这个列表里存的只是int类型的数据，结构上还需要两个额外的指针prev和next。 Redis将链表和ziplist结合起来组成了quicklist。也就是将多个ziplist使用双向指针串起来使用。这样既满足了快速的插入删除性能，又不会出现太大的空间冗余。 3.4 Redis集合(Set)3.4.1 简介Redis set对外提供的功能与list类似是一个列表的功能，特殊之处在于set是可以自动排重的，当你需要存储一个列表数据，又不希望出现重复数据时，set是一个很好的选择，并且set提供了判断某个成员是否在一个set集合内的重要接口，这个也是list所不能提供的。 Redis的Set是string类型的无序集合。它底层其实是一个value为null的hash表，所以添加，删除，查找的复杂度都是O(1)。 一个算法，随着数据的增加，执行时间的长短，如果是O(1)，数据增加，查找数据的时间不变 3.4.2 常用命令sadd ….. 将一个或多个 member 元素加入到集合 key 中，已经存在的 member 元素将被忽略 smembers 取出该集合的所有值。 sismember 判断集合是否为含有该值，有1，没有0 scard返回该集合的元素个数。 srem …. 删除集合中的某个元素。 spop 随机从该集合中吐出一个值。 srandmember 随机从该集合中取出n个值。不会从集合中删除 。 smove value把集合中一个值从一个集合移动到另一个集合 sinter 返回两个集合的交集元素。 sunion 返回两个集合的并集元素。 sdiff 返回两个集合的差集元素(key1中的，不包含key2中的) 3.4.3 数据结构Set数据结构是dict字典，字典是用哈希表实现的。 Java中HashSet的内部实现使用的是HashMap，只不过所有的value都指向同一个对象。Redis的set结构也是一样，它的内部也使用hash结构，所有的value都指向同一个内部值。 3.5 Redis哈希(Hash)3.5.1 简介Redis hash 是一个键值对集合。 Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。 类似Java里面的Map&lt;String,Object&gt; 用户ID为查找的key，存储的value用户对象包含姓名，年龄，生日等信息，如果用普通的key/value结构来存储 主要有以下2种存储方式： 每次修改用户的某个属性需要，先反序列化改好后再序列化回去。开销较大。 用户ID数据冗余 通过 key(用户ID) + field(属性标签) 就可以操作对应属性数据了，既不需要重复存储数据，也不会带来序列化和并发修改控制的问题 3.5.2 常用命令 hset 给集合中的 键赋值 hget 从集合取出 value hmset … 批量设置hash的值 hexists查看哈希表 key 中，给定域 field 是否存在。 hkeys 列出该hash集合的所有field hvals 列出该hash集合的所有value hincrby 为哈希表 key 中的域 field 的值加上增量 1 -1 hsetnx 将哈希表 key 中的域 field 的值设置为 value ，当且仅当域 field 不存在 . 3.5.3 数据结构Hash类型对应的数据结构是两种：ziplist（压缩列表），hashtable（哈希表）。当field-value长度较短且个数较少时，使用ziplist，否则使用hashtable。 3.6 Redis有序集合Zset(sorted set)3.6.1 简介Redis有序集合zset与普通集合set非常相似，是一个没有重复元素的字符串集合。 不同之处是有序集合的每个成员都关联了一个评分（score）,这个评分（score）被用来按照从最低分到最高分的方式排序集合中的成员。集合的成员是唯一的，但是评分可以是重复了 。 因为元素是有序的, 所以你也可以很快的根据评分（score）或者次序（position）来获取一个范围的元素。 访问有序集合的中间元素也是非常快的,因此你能够使用有序集合作为一个没有重复成员的智能列表。 3.6.2 常用命令 zadd … 将一个或多个 member 元素及其 score 值加入到有序集 key 当中。 zrange [WITHSCORES] 返回有序集 key 中，下标在之间的元素 带WITHSCORES，可以让分数一起和值返回到结果集。 zrangebyscore key minmax [withscores] [limit offset count] 返回有序集 key 中，所有 score 值介于 min 和 max 之间(包括等于 min 或 max )的成员。有序集成员按 score 值递增(从小到大)次序排列。 zrevrangebyscore key maxmin [withscores] [limit offset count] 同上，改为从大到小排列。 zincrby 为元素的score加上增量 zrem 删除该集合下，指定值的元素 zcount 统计该集合，分数区间内的元素个数 zrank 返回该值在集合中的排名，从0开始。 案例：如何利用zset实现一个文章访问量的排行榜？ 3.6.3 数据结构SortedSet(zset)是Redis提供的一个非常特别的数据结构，一方面它等价于Java的数据结构Map&lt;String, Double&gt;，可以给每一个元素value赋予一个权重score，另一方面它又类似于TreeSet，内部的元素会按照权重score进行排序，可以得到每个元素的名次，还可以通过score的范围来获取元素的列表。 zset底层使用了两个数据结构 （1）hash，hash的作用就是关联元素value和权重score，保障元素value的唯一性，可以通过元素value找到相应的score值。 （2）跳跃表，跳跃表的目的在于给元素value排序，根据score的范围获取元素列表。 3.6.4 跳跃表（跳表）1、简介 有序集合在生活中比较常见，例如根据成绩对学生排名，根据得分对玩家排名等。对于有序集合的底层实现，可以用数组、平衡树、链表等。数组不便元素的插入、删除；平衡树或红黑树虽然效率高但结构复杂；链表查询需要遍历所有效率低。Redis采用的是跳跃表。跳跃表效率堪比红黑树，实现远比红黑树简单。 2、实例 对比有序链表和跳跃表，从链表中查询出51 （1） 有序链表 要查找值为51的元素，需要从第一个元素开始依次查找、比较才能找到。共需要6次比较。 （2） 跳跃表 从第2层开始，1节点比51节点小，向后比较。 21节点比51节点小，继续向后比较，后面就是NULL了，所以从21节点向下到第1层 在第1层，41节点比51节点小，继续向后，61节点比51节点大，所以从41向下 在第0层，51节点为要查找的节点，节点被找到，共查找4次。 从此可以看出跳跃表比有序链表效率要高 四、Redis配置文件介绍自定义目录：/myredis/redis.conf 4.1 Units单位配置大小单位,开头定义了一些基本的度量单位，只支持bytes，不支持bit 大小写不敏感 4.2 INCLUDES包含 类似jsp中的include，多实例的情况可以把公用的配置文件提取出来 4.3 网络相关配置4.3.1 bind默认情况bind=127.0.0.1只能接受本机的访问请求 不写的情况下，无限制接受任何ip地址的访问 生产环境肯定要写你应用服务器的地址；服务器是需要远程访问的，所以需要将其注释掉 如果开启了protected-mode，那么在没有设定bind ip且没有设密码的情况下，Redis只允许接受本机的响应 保存配置，停止服务，重启启动查看进程，不再是本机访问了。 4.3.2 protected-mode将本机访问保护模式设置no 4.3.3 Port端口号，默认 6379 4.3.4 tcp-backlog设置tcp的backlog，backlog其实是一个连接队列，backlog队列总和=未完成三次握手队列 + 已经完成三次握手队列。 在高并发环境下你需要一个高backlog值来避免慢客户端连接问题。 注意Linux内核会将这个值减小到/proc/sys/net/core/somaxconn的值（128），所以需要确认增大/proc/sys/net/core/somaxconn和/proc/sys/net/ipv4/tcp_max_syn_backlog（128）两个值来达到想要的效果 4.3.5 timeout一个空闲的客户端维持多少秒会关闭，0表示关闭该功能。即永不关闭。 4.3.6 tcp-keepalive对访问客户端的一种心跳检测，每个n秒检测一次。 单位为秒，如果设置为0，则不会进行Keepalive检测，建议设置成60 4.4 GENERAL通用4.4.1 daemonize是否为后台进程，设置为yes 守护进程，后台启动 4.4.2 pidfile存放pid文件的位置，每个实例会产生一个不同的pid文件 4.4.3 loglevel指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为notice 四个级别根据使用阶段来选择，生产环境选择notice 或者warning 4.4.4 logfile日志文件名称 4.4.5 databases 16设定库的数量 默认16，默认数据库为0，可以使用SELECT 命令在连接上指定数据库id 4.5 SECURITY安全4.5.1 设置密码 访问密码的查看、设置和取消 在命令中设置密码，只是临时的。重启redis服务器，密码就还原了。 永久设置，需要再配置文件中进行设置。 4.6 LIMITS限制4.6.1 maxclientsØ 设置redis同时可以与多少个客户端进行连接。 Ø 默认情况下为10000个客户端。 Ø 如果达到了此限制，redis则会拒绝新的连接请求，并且向这些连接请求方发出“max number of clients reached”以作回应。 4.6.2 maxmemoryØ 建议必须设置，否则，将内存占满，造成服务器宕机 Ø 设置redis可以使用的内存量。一旦到达内存使用上限，redis将会试图移除内部数据，移除规则可以通过maxmemory-policy来指定。 Ø 如果redis无法根据移除规则来移除内存中的数据，或者设置了“不允许移除”，那么redis则会针对那些需要申请内存的指令返回错误信息，比如SET、LPUSH等。 Ø 但是对于无内存申请的指令，仍然会正常响应，比如GET等。如果你的redis是主redis（说明你的redis有从redis），那么在设置内存使用上限时，需要在系统中留出一些内存空间给同步队列缓存，只有在你设置的是“不移除”的情况下，才不用考虑这个因素。 4.6.3 maxmemory-policyØ volatile-lru：使用LRU算法移除key，只对设置了过期时间的键；（最近最少使用） Ø allkeys-lru：在所有集合key中，使用LRU算法移除key Ø volatile-random：在过期集合中移除随机的key，只对设置了过期时间的键 Ø allkeys-random：在所有集合key中，移除随机的key Ø volatile-ttl：移除那些TTL值最小的key，即那些最近要过期的key Ø noeviction：不进行移除。针对写操作，只是返回错误信息 4.6.4 maxmemory-samplesØ 设置样本数量，LRU算法和最小TTL算法都并非是精确的算法，而是估算值，所以你可以设置样本的大小，redis默认会检查这么多个key并选择其中LRU的那个。 Ø 一般设置3到7的数字，数值越小样本越不准确，但性能消耗越小。 五、Redis的发布和订阅5.1 什么是发布和订阅Redis 发布订阅 (pub/sub) 是一种消息通信模式：发送者 (pub) 发送消息，订阅者 (sub) 接收消息。 Redis 客户端可以订阅任意数量的频道。 5.2 Redis的发布和订阅1、客户端可以订阅频道如下图 2、当给这个频道发布消息后，消息就会发送给订阅的客户端 5.3 发布订阅命令行实现1、 打开一个客户端订阅channel1 SUBSCRIBE channel1 2、打开另一个客户端，给channel1发布消息hello publish channel1 hello 返回的1是订阅者数量 3、打开第一个客户端可以看到发送的消息 注：发布的消息没有持久化，如果在订阅的客户端收不到hello，只能收到订阅后发布的消息 六、Redis新数据类型6.1 Bitmaps6.1.1 简介现代计算机用二进制（位） 作为信息的基础单位， 1个字节等于8位， 例如“abc”字符串是由3个字节组成， 但实际在计算机存储时将其用二进制表示， “abc”分别对应的ASCII码分别是97、 98、 99， 对应的二进制分别是01100001、 01100010和01100011，如下图 合理地使用操作位能够有效地提高内存使用率和开发效率。 Redis提供了Bitmaps这个“数据类型”可以实现对位的操作： （1） Bitmaps本身不是一种数据类型， 实际上它就是字符串（key-value） ， 但是它可以对字符串的位进行操作。 （2） Bitmaps单独提供了一套命令， 所以在Redis中使用Bitmaps和使用字符串的方法不太相同。 可以把Bitmaps想象成一个以位为单位的数组， 数组的每个单元只能存储0和1， 数组的下标在Bitmaps中叫做偏移量。 6.1.2 命令1、setbit （1）格式 setbit设置Bitmaps中某个偏移量的值（0或1） *offset:偏移量从0开始 （2）实例 每个独立用户是否访问过网站存放在Bitmaps中， 将访问的用户记做1， 没有访问的用户记做0， 用偏移量作为用户的id。 设置键的第offset个位的值（从0算起） ， 假设现在有20个用户，userid=1， 6， 11， 15， 19的用户对网站进行了访问， 那么当前Bitmaps初始化结果如图 unique:users:20201106代表2020-11-06这天的独立访问用户的Bitmaps 注： 很多应用的用户id以一个指定数字（例如10000） 开头， 直接将用户id和Bitmaps的偏移量对应势必会造成一定的浪费， 通常的做法是每次做setbit操作时将用户id减去这个指定数字。 在第一次初始化Bitmaps时， 假如偏移量非常大， 那么整个初始化过程执行会比较慢， 可能会造成Redis的阻塞。 2、getbit （1）格式 getbit获取Bitmaps中某个偏移量的值 获取键的第offset位的值（从0开始算） （2）实例 获取id=8的用户是否在2020-11-06这天访问过， 返回0说明没有访问过： 注：因为100根本不存在，所以也是返回0 3、bitcount 统计字符串被设置为1的bit数。一般情况下，给定的整个字符串都会被进行计数，通过指定额外的 start 或 end 参数，可以让计数只在特定的位上进行。start 和 end 参数的设置，都可以使用负数值：比如 -1 表示最后一个位，而 -2 表示倒数第二个位，start、end 是指bit组的字节的下标数，二者皆包含。 （1）格式 bitcount[start end] 统计字符串从start字节到end字节比特值为1的数量 （2）实例 计算2022-11-06这天的独立访问用户数量 start和end代表起始和结束字节数， 下面操作计算用户id在第1个字节到第3个字节之间的独立访问用户数， 对应的用户id是11， 15， 19。 举例： K1 【01000001 01000000 00000000 00100001】，对应【0，1，2，3】 bitcount K1 1 2 ： 统计下标1、2字节组中bit=1的个数，即01000000 00000000 –》bitcount K1 1 2 –》1 bitcount K1 1 3 ： 统计下标1、2字节组中bit=1的个数，即01000000 00000000 00100001 –》bitcount K1 1 3 –》3 bitcount K1 0 -2 ： 统计下标0到下标倒数第2，字节组中bit=1的个数，即01000001 01000000 00000000 –》bitcount K1 0 -2 –》3 注意：redis的setbit设置或清除的是bit位置，而bitcount计算的是byte位置。 4、bitop (1)格式 bitop and(or/not/xor) [key…] bitop是一个复合操作， 它可以做多个Bitmaps的and（交集） 、 or（并集） 、 not（非） 、 xor（异或） 操作并将结果保存在destkey中。 (2)实例 2020-11-04 日访问网站的userid=1,2,5,9。 setbit unique:users:20201104 1 1 setbit unique:users:20201104 2 1 setbit unique:users:20201104 5 1 setbit unique:users:20201104 9 1 2020-11-03 日访问网站的userid=0,1,4,9。 setbit unique:users:20201103 0 1 setbit unique:users:20201103 1 1 setbit unique:users:20201103 4 1 setbit unique:users:20201103 9 1 计算出两天都访问过网站的用户数量 bitop and unique:users:and:20201104_03 unique:users:20201103unique:users:20201104 计算出任意一天都访问过网站的用户数量（例如月活跃就是类似这种） ， 可以使用or求并集 6.1.3 Bitmaps与set对比假设网站有1亿用户， 每天独立访问的用户有5千万， 如果每天用集合类型和Bitmaps分别存储活跃用户可以得到表 set和Bitmaps存储一天活跃用户对比 数据类型 每个用户id占用空间 需要存储的用户量 全部内存量 集合类型 64位 50000000 64位*50000000 = 400MB Bitmaps 1位 100000000 1位*100000000 = 12.5MB 很明显， 这种情况下使用Bitmaps能节省很多的内存空间， 尤其是随着时间推移节省的内存还是非常可观的 set和Bitmaps存储独立用户空间对比 数据类型 一天 一个月 一年 集合类型 400MB 12GB 144GB Bitmaps 12.5MB 375MB 4.5GB 但Bitmaps并不是万金油， 假如该网站每天的独立访问用户很少， 例如只有10万（大量的僵尸用户） ， 那么两者的对比如下表所示， 很显然， 这时候使用Bitmaps就不太合适了， 因为基本上大部分位都是0。 set和Bitmaps存储一天活跃用户对比（独立用户比较少） 数据类型 每个userid占用空间 需要存储的用户量 全部内存量 集合类型 64位 100000 64位*100000 = 800KB Bitmaps 1位 100000000 1位*100000000 = 12.5MB 6.2 HyperLogLog6.2.1 简介在工作当中，我们经常会遇到与统计相关的功能需求，比如统计网站PV（PageView页面访问量）,可以使用Redis的incr、incrby轻松实现。 但像UV（UniqueVisitor，独立访客）、独立IP数、搜索记录数等需要去重和计数的问题如何解决？这种求集合中不重复元素个数的问题称为基数问题。 解决基数问题有很多种方案： （1）数据存储在MySQL表中，使用distinct count计算不重复个数 （2）使用Redis提供的hash、set、bitmaps等数据结构来处理 以上的方案结果精确，但随着数据不断增加，导致占用空间越来越大，对于非常大的数据集是不切实际的。 能否能够降低一定的精度来平衡存储空间？Redis推出了HyperLogLog Redis HyperLogLog 是用来做基数统计的算法，HyperLogLog 的优点是，在输入元素的数量或者体积非常非常大时，计算基数所需的空间总是固定的、并且是很小的。 在 Redis 里面，每个 HyperLogLog 键只需要花费 12 KB 内存，就可以计算接近 2^64 个不同元素的基数。这和计算基数时，元素越多耗费内存就越多的集合形成鲜明对比。 但是，因为 HyperLogLog 只会根据输入元素来计算基数，而不会储存输入元素本身，所以 HyperLogLog 不能像集合那样，返回输入的各个元素。 什么是基数? 比如数据集 {1, 3, 5, 7, 5, 7, 8}， 那么这个数据集的基数集为 {1, 3, 5 ,7, 8}, 基数(不重复元素)为5。 基数估计就是在误差可接受的范围内，快速计算基数。 6.2.2 命令1、pfadd （1）格式 pfadd &lt; element&gt; [element …] 添加指定元素到 HyperLogLog 中 （2）实例 将所有元素添加到指定HyperLogLog数据结构中。如果执行命令后HLL估计的近似基数发生变化，则返回1，否则返回0。 2、pfcount （1）格式 pfcount [key …] 计算HLL的近似基数，可以计算多个HLL，比如用HLL存储每天的UV，计算一周的UV可以使用7天的UV合并计算即可 （2）实例 3、pfmerge （1）格式 pfmerge [sourcekey …] 将一个或多个HLL合并后的结果存储在另一个HLL中，比如每月活跃用户可以使用每天的活跃用户来合并计算可得 （2）实例 6.3 Geospatial6.3.1 简介Redis 3.2 中增加了对GEO类型的支持。GEO，Geographic，地理信息的缩写。该类型，就是元素的2维坐标，在地图上就是经纬度。redis基于该类型，提供了经纬度设置，查询，范围查询，距离查询，经纬度Hash等常见操作。 6.3.2 命令1、geoadd （1）格式 geoadd&lt; longitude&gt; [longitude latitude member…] 添加地理位置（经度，纬度，名称） （2）实例 geoadd china:city 121.47 31.23 shanghai geoadd china:city 106.50 29.53 chongqing 114.05 22.52 shenzhen 116.38 39.90 beijing 两极无法直接添加，一般会下载城市数据，直接通过 Java 程序一次性导入。 有效的经度从 -180 度到 180 度。有效的纬度从 -85.05112878 度到 85.05112878 度。 当坐标位置超出指定范围时，该命令将会返回一个错误。 已经添加的数据，是无法再次往里面添加的。 2、geopos （1）格式 geopos [member…] 获得指定地区的坐标值 （2）实例 3、geodist （1）格式 geodist [m|km|ft|mi ] 获取两个位置之间的直线距离 （2）实例 获取两个位置之间的直线距离 单位： m 表示单位为米[默认值]。 km 表示单位为千米。 mi 表示单位为英里。 ft 表示单位为英尺。 如果用户没有显式地指定单位参数， 那么 GEODIST 默认使用米作为单位 4、georadius （1）格式 georadius&lt; longitude&gt;radius m|km|ft|mi 以给定的经纬度为中心，找出某一半径内的元素 经度 纬度 距离 单位 （2）实例 七、Redis_Jedis_测试7.1 Jedis所需要的jar包 &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt; 7.2 连接Redis注意事项禁用Linux的防火墙：Linux(CentOS7)里执行命令 systemctl stop/disable firewalld.service redis.conf中注释掉bind 127.0.0.1 ,然后 protected-mode no 7.3 Jedis常用操作7.3.1 创建动态的工程7.3.2 创建测试程序 package com.atguigu.jedis; import redis.clients.jedis.Jedis; public class Demo01 {public static void main(String[] args) { Jedis jedis = new Jedis(&quot;192.168.137.3&quot;,6379); String pong = jedis.ping(); System.out.println(&quot;连接成功：&quot;+pong); jedis.close(); &#125; } 7.4 测试相关数据类型7.4.1 Jedis-API: Key1234567891011jedis.set(&quot;k1&quot;, &quot;v1&quot;);jedis.set(&quot;k2&quot;, &quot;v2&quot;);jedis.set(&quot;k3&quot;, &quot;v3&quot;);Set&lt;String&gt; keys = jedis.keys(&quot;*&quot;);System.out.println(keys.size());for (String key : keys) &#123; System.out.println(key);&#125;System.out.println(jedis.exists(&quot;k1&quot;));System.out.println(jedis.ttl(&quot;k1&quot;)); System.out.println(jedis.get(&quot;k1&quot;)); 7.4.2 Jedis-API: String12jedis.mset(&quot;str1&quot;,&quot;v1&quot;,&quot;str2&quot;,&quot;v2&quot;,&quot;str3&quot;,&quot;v3&quot;);System.out.println(jedis.mget(&quot;str1&quot;,&quot;str2&quot;,&quot;str3&quot;)); 7.4.3 Jedis-API: List1234List&lt;String&gt; list = jedis.lrange(&quot;mylist&quot;,0,-1);for (String element : list) &#123;System.out.println(element);&#125; 7.4.4 Jedis-API: set123456789jedis.sadd(&quot;orders&quot;, &quot;order01&quot;);jedis.sadd(&quot;orders&quot;, &quot;order02&quot;);jedis.sadd(&quot;orders&quot;, &quot;order03&quot;);jedis.sadd(&quot;orders&quot;, &quot;order04&quot;);Set&lt;String&gt; smembers = jedis.smembers(&quot;orders&quot;);for (String order : smembers) &#123;System.out.println(order);&#125;jedis.srem(&quot;orders&quot;, &quot;order02&quot;); 7.4.5 Jedis-API: hash1234567891011jedis.hset(&quot;hash1&quot;,&quot;userName&quot;,&quot;lisi&quot;);System.out.println(jedis.hget(&quot;hash1&quot;,&quot;userName&quot;));Map&lt;String,String&gt; map = new HashMap&lt;String,String&gt;();map.put(&quot;telphone&quot;,&quot;13810169999&quot;);map.put(&quot;address&quot;,&quot;atguigu&quot;);map.put(&quot;email&quot;,&quot;abc@163.com&quot;);jedis.hmset(&quot;hash2&quot;,map);List&lt;String&gt; result = jedis.hmget(&quot;hash2&quot;, &quot;telphone&quot;,&quot;email&quot;);for (String element : result) &#123;System.out.println(element);&#125; 7.4.6 Jedis-API: zset123456789jedis.zadd(&quot;zset01&quot;, 100d, &quot;z3&quot;);jedis.zadd(&quot;zset01&quot;, 90d, &quot;l4&quot;);jedis.zadd(&quot;zset01&quot;, 80d, &quot;w5&quot;);jedis.zadd(&quot;zset01&quot;, 70d, &quot;z6&quot;); Set&lt;String&gt; zrange = jedis.zrange(&quot;zset01&quot;, 0, -1);for (String e : zrange) &#123;System.out.println(e);&#125; 八、Redis_Jedis_实例8.1 完成一个手机验证码功能要求： 1、输入手机号，点击发送后随机生成6位数字码，2分钟有效 2、输入验证码，点击验证，返回成功或失败 3、每个手机号每天只能输入3次 九、Redis与Spring Boot整合Spring Boot整合Redis非常简单，只需要按如下步骤整合即可 9.1 整合步骤1、 在pom.xml文件中引入redis相关依赖 123456789101112&lt;!-- redis --&gt;&lt;dependency&gt;&lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;&lt;artifactId&gt;spring-boot-starter-data-redis&lt;/artifactId&gt;&lt;/dependency&gt;&lt;!-- spring2.X集成redis所需common-pool2--&gt;&lt;dependency&gt;&lt;groupId&gt;org.apache.commons&lt;/groupId&gt;&lt;artifactId&gt;commons-pool2&lt;/artifactId&gt;&lt;version&gt;2.6.0&lt;/version&gt;&lt;/dependency&gt; 2、 application.properties配置redis配置 12345678910111213141516#Redis服务器地址spring.redis.host=192.168.140.136#Redis服务器连接端口spring.redis.port=6379#Redis数据库索引（默认为0）spring.redis.database= 0#连接超时时间（毫秒）spring.redis.timeout=1800000#连接池最大连接数（使用负值表示没有限制）spring.redis.lettuce.pool.max-active=20#最大阻塞等待时间(负数表示没限制)spring.redis.lettuce.pool.max-wait=-1#连接池中的最大空闲连接spring.redis.lettuce.pool.max-idle=5#连接池中的最小空闲连接spring.redis.lettuce.pool.min-idle=0 3、 添加redis配置类 1234567891011121314151617181920212223242526272829303132333435363738394041424344@EnableCaching@Configurationpublic class RedisConfig extends CachingConfigurerSupport &#123; @Bean public RedisTemplate&lt;String, Object&gt; redisTemplate(RedisConnectionFactory factory) &#123; RedisTemplate&lt;String, Object&gt; template = new RedisTemplate&lt;&gt;(); RedisSerializer&lt;String&gt; redisSerializer = new StringRedisSerializer(); Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class); ObjectMapper om = new ObjectMapper(); om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(om); template.setConnectionFactory(factory);//key序列化方式 template.setKeySerializer(redisSerializer);//value序列化 template.setValueSerializer(jackson2JsonRedisSerializer);//value hashmap序列化 template.setHashValueSerializer(jackson2JsonRedisSerializer); return template; &#125; @Bean public CacheManager cacheManager(RedisConnectionFactory factory) &#123; RedisSerializer&lt;String&gt; redisSerializer = new StringRedisSerializer(); Jackson2JsonRedisSerializer jackson2JsonRedisSerializer = new Jackson2JsonRedisSerializer(Object.class);//解决查询缓存转换异常的问题 ObjectMapper om = new ObjectMapper(); om.setVisibility(PropertyAccessor.ALL, JsonAutoDetect.Visibility.ANY); om.enableDefaultTyping(ObjectMapper.DefaultTyping.NON_FINAL); jackson2JsonRedisSerializer.setObjectMapper(om);// 配置序列化（解决乱码的问题）,过期时间600秒 RedisCacheConfiguration config = RedisCacheConfiguration.defaultCacheConfig() .entryTtl(Duration.ofSeconds(600)) .serializeKeysWith(RedisSerializationContext.SerializationPair.fromSerializer(redisSerializer)) .serializeValuesWith(RedisSerializationContext.SerializationPair.fromSerializer(jackson2JsonRedisSerializer)) .disableCachingNullValues(); RedisCacheManager cacheManager = RedisCacheManager.builder(factory) .cacheDefaults(config) .build(); return cacheManager; &#125;&#125; 4、测试一下 RedisTestController中添加测试方法 123456789101112131415@RestController@RequestMapping(&quot;/redisTest&quot;)public class RedisTestController &#123; @Autowired private RedisTemplate redisTemplate; @GetMapping public String testRedis() &#123; //设置值到redis redisTemplate.opsForValue().set(&quot;name&quot;,&quot;lucy&quot;); //从redis获取值 String name = (String)redisTemplate.opsForValue().get(&quot;name&quot;); return name; &#125;&#125; 十、Redis_事务_锁机制_秒杀10.1 Redis的事务定义 Redis事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 Redis事务的主要作用就是串联多个命令防止别的命令插队。 10.2 Multi、Exec、discard从输入Multi命令开始，输入的命令都会依次进入命令队列中，但不会执行，直到输入Exec后，Redis会将之前的命令队列中的命令依次执行。 组队的过程中可以通过discard来放弃组队。 案例： 组队成功，提交成功 组队阶段报错，提交失败 组队成功，提交有成功有失败情况 10.3 事务的错误处理组队中某个命令出现了报告错误，执行时整个的所有队列都会被取消。 如果执行阶段某个命令报出了错误，则只有报错的命令不会被执行，而其他的命令都会执行，不会回滚。 10.4 为什么要做成事务想想一个场景：有很多人有你的账户,同时去参加双十一抢购 10.5 事务冲突的问题10.5.1 例子一个请求想给金额减8000 一个请求想给金额减5000 一个请求想给金额减1000 10.5.2 悲观锁 悲观锁(Pessimistic Lock)：顾名思义，就是很悲观，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会block直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。 10.5.3 乐观锁 乐观锁(Optimistic Lock)：顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量。Redis就是利用这种check-and-set机制实现事务的。 10.5.4 WATCH key [key …]在执行multi之前，先执行watch key1 [key2],可以监视一个(或多个) key ，如果在事务执行之前这个(或这些) key 被其他命令所改动，那么事务将被打断。 10.5.6 unwatch取消 WATCH 命令对所有 key 的监视。 如果在执行 WATCH 命令之后，EXEC 命令或DISCARD 命令先被执行了的话，那么就不需要再执行UNWATCH 了。 http://doc.redisfans.com/transaction/exec.html 10.6 Redis事务三特性Ø 单独的隔离操作 n 事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。 Ø 没有隔离级别的概念 n 队列中的命令没有提交之前都不会实际被执行，因为事务提交前任何指令都不会被实际执行 Ø 不保证原子性 n 事务中如果有一条命令执行失败，其后的命令仍然会被执行，没有回滚 十一、Redis_事务_秒杀案例11.1 解决计数器和人员记录的事务操作 11.2 Redis事务–秒杀并发模拟使用工具ab模拟测试 CentOS6 默认安装 CentOS7需要手动安装 11.2.1 联网：yum install httpd-tools11.2.2 无网络（1） 进入cd /run/media/root/CentOS 7 x86_64/Packages（路径跟centos6不同） （2） 顺序安装 apr-1.4.8-3.el7.x86_64.rpm apr-util-1.5.2-6.el7.x86_64.rpm httpd-tools-2.4.6-67.el7.centos.x86_64.rpm 11.2.3 测试及结果 通过ab测试 vim postfile 模拟表单提交参数,以&amp;符号结尾;存放当前目录。 内容：prodid=0101&amp; ab -n 2000 -c 200 -k -p ~/postfile -T application/x-www-form-urlencoded http://192.168.2.115:8081/Seckill/doseckill 超卖 11.3 超卖问题 11.4 利用乐观锁淘汰用户，解决超卖问题。 12345678910111213141516171819202122232425262728293031323334353637383940//增加乐观锁jedis.watch(qtkey); //3.判断库存String qtkeystr = jedis.get(qtkey);if(qtkeystr==null || &quot;&quot;.equals(qtkeystr.trim())) &#123;System.out.println(&quot;未初始化库存&quot;);jedis.close();return false ;&#125; int qt = Integer.parseInt(qtkeystr);if(qt&lt;=0) &#123;System.err.println(&quot;已经秒光&quot;);jedis.close();return false;&#125; //增加事务Transaction multi = jedis.multi(); //4.减少库存//jedis.decr(qtkey);multi.decr(qtkey); //5.加人//jedis.sadd(usrkey, uid);multi.sadd(usrkey, uid); //执行事务List&lt;Object&gt; list = multi.exec(); //判断事务提交是否失败if(list==null || list.size()==0) &#123;System.out.println(&quot;秒杀失败&quot;);jedis.close();return false;&#125;System.err.println(&quot;秒杀成功&quot;);jedis.close(); 11.5 继续增加并发测试11.5.1 连接有限制ab -n 2000 -c 200 -k -p postfile -T ‘application/x-www-form-urlencoded’ http://192.168.140.1:8080/seckill/doseckill 增加-r参数，-r Don’t exit on socket receive errors. ab -n 2000 -c 100 -r -p postfile -T ‘application/x-www-form-urlencoded’ http://192.168.140.1:8080/seckill/doseckill 11.5.2 已经秒光，可是还有库存ab -n 2000 -c 100 -p postfile -T ‘application/x-www-form-urlencoded’ http://192.168.137.1:8080/seckill/doseckill 已经秒光，可是还有库存。原因，就是乐观锁导致很多请求都失败。先点的没秒到，后点的可能秒到了。 11.5.3 连接超时，通过连接池解决 11.5.4 连接池节省每次连接redis服务带来的消耗，把连接好的实例反复利用。 通过参数管理连接的行为 代码见项目中 链接池参数 MaxTotal：控制一个pool可分配多少个jedis实例，通过pool.getResource()来获取；如果赋值为-1，则表示不限制；如果pool已经分配了MaxTotal个jedis实例，则此时pool的状态为exhausted。 maxIdle：控制一个pool最多有多少个状态为idle(空闲)的jedis实例； MaxWaitMillis：表示当borrow一个jedis实例时，最大的等待毫秒数，如果超过等待时间，则直接抛JedisConnectionException； testOnBorrow：获得一个jedis实例的时候是否检查连接可用性（ping()）；如果为true，则得到的jedis实例均是可用的； 11.6 解决库存遗留问题11.6.1 LUA脚本 Lua 是一个小巧的脚本语言，Lua脚本可以很容易的被C/C++ 代码调用，也可以反过来调用C/C++的函数，Lua并没有提供强大的库，一个完整的Lua解释器不过200k，所以Lua不适合作为开发独立应用程序的语言，而是作为嵌入式脚本语言。 很多应用程序、游戏使用LUA作为自己的嵌入式脚本语言，以此来实现可配置性、可扩展性。 这其中包括魔兽争霸地图、魔兽世界、博德之门、愤怒的小鸟等众多游戏插件或外挂。 https://www.w3cschool.cn/lua/ 11.6.2 LUA脚本在Redis中的优势将复杂的或者多步的redis操作，写为一个脚本，一次提交给redis执行，减少反复连接redis的次数。提升性能。 LUA脚本是类似redis事务，有一定的原子性，不会被其他命令插队，可以完成一些redis事务性的操作。 但是注意redis的lua脚本功能，只有在Redis 2.6以上的版本才可以使用。 利用lua脚本淘汰用户，解决超卖问题。 redis 2.6版本以后，通过lua脚本解决争抢问题，实际上是redis 利用其单线程的特性，用任务队列的方式解决多任务并发问题。 11.7 Redis_事务_秒杀案例_代码11.7.1 项目结构 11.7.2 第一版：简单版老师点10次，正常秒杀 同学一起点试一试，秒杀也是正常的。这是因为还达不到并发的效果。 使用工具ab模拟并发测试，会出现超卖情况。查看库存会出现负数。 11.7.3 第二版：加事务-乐观锁(解决超卖),但出现遗留库存和连接超时11.7.4 第三版：连接池解决超时问题11.7.5 第四版：解决库存依赖问题，LUA脚本12345678910111213141516local userid=KEYS[1]; local prodid=KEYS[2];local qtkey=&quot;sk:&quot;..prodid..&quot;:qt&quot;;local usersKey=&quot;sk:&quot;..prodid.&quot;:usr&#x27;; local userExists=redis.call(&quot;sismember&quot;,usersKey,userid);if tonumber(userExists)==1 then return 2;endlocal num= redis.call(&quot;get&quot; ,qtkey);if tonumber(num)&lt;=0 then return 0; else redis.call(&quot;decr&quot;,qtkey); redis.call(&quot;sadd&quot;,usersKey,userid);endreturn 1; 十二、Redis持久化之RDB12.1 总体介绍官网介绍：http://www.redis.io Redis 提供了2个不同形式的持久化方式。 RDB（Redis DataBase） AOF（Append Of File） 12.2 RDB（Redis DataBase）12.2.1 官网介绍 12.2.2 是什么在指定的时间间隔内将内存中的数据集快照写入磁盘， 也就是行话讲的Snapshot快照，它恢复时是将快照文件直接读到内存里 12.2.3 备份是如何执行的Redis会单独创建（fork）一个子进程来进行持久化，会先将数据写入到 一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。 整个过程中，主进程是不进行任何IO操作的，这就确保了极高的性能 如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那RDB方式要比AOF方式更加的高效。RDB的缺点是最后一次持久化后的数据可能丢失。 12.2.4 Fork Fork的作用是复制一个与当前进程一样的进程。新进程的所有数据（变量、环境变量、程序计数器等） 数值都和原进程一致，但是是一个全新的进程，并作为原进程的子进程 在Linux程序中，fork()会产生一个和父进程完全相同的子进程，但子进程在此后多会exec系统调用，出于效率考虑，Linux中引入了“写时复制技术” 一般情况父进程和子进程会共用同一段物理内存，只有进程空间的各段的内容要发生变化时，才会将父进程的内容复制一份给子进程。 12.2.5 RDB持久化流程 12.2.6 dump.rdb文件在redis.conf中配置文件名称，默认为dump.rdb 12.2.7 配置位置rdb文件的保存路径，也可以修改。默认为Redis启动时命令行所在的目录下 dir “/myredis/“ 12.2.8 如何触发RDB快照；保持策略 配置文件中默认的快照配置 命令save VS bgsave save ：save时只管保存，其它不管，全部阻塞。手动保存。不建议。 bgsave：Redis会在后台异步进行快照操作， 快照同时还可以响应客户端请求。 可以通过lastsave 命令获取最后一次成功执行快照的时间 flushall命令 执行flushall命令，也会产生dump.rdb文件，但里面是空的，无意义 SNAPSHOTTING快照 Save 格式：save 秒钟 写操作次数 RDB是整个内存的压缩过的Snapshot，RDB的数据结构，可以配置复合的快照触发条件， 默认是1分钟内改了1万次，或5分钟内改了10次，或15分钟内改了1次。 禁用 不设置save指令，或者给save传入空字符串 stop-writes-on-bgsave-error 当Redis无法写入磁盘的话，直接关掉Redis的写操作。推荐yes. rdbcompression 压缩文件 对于存储到磁盘中的快照，可以设置是否进行压缩存储。如果是的话，redis会采用LZF算法进行压缩。 如果你不想消耗CPU来进行压缩的话，可以设置为关闭此功能。推荐yes. rdbchecksum 检查完整性 在存储快照后，还可以让redis使用CRC64算法来进行数据校验， 但是这样做会增加大约10%的性能消耗，如果希望获取到最大的性能提升，可以关闭此功能 推荐yes. rdb的备份 先通过config get dir 查询rdb文件的目录 将*.rdb的文件拷贝到别的地方 rdb的恢复 关闭Redis 先把备份的文件拷贝到工作目录下 cp dump2.rdb dump.rdb 启动Redis, 备份数据会直接加载 12.2.9 优势 适合大规模的数据恢复 对数据完整性和一致性要求不高更适合使用 节省磁盘空间 恢复速度快 12.2.10 劣势l Fork的时候，内存中的数据被克隆了一份，大致2倍的膨胀性需要考虑 l 虽然Redis在fork时使用了写时拷贝技术,但是如果数据庞大时还是比较消耗性能。 l 在备份周期在一定间隔时间做一次备份，所以如果Redis意外down掉的话，就会丢失最后一次快照后的所有修改。 12.2.11 如何停止动态停止RDB：redis-cli config set save “”#save后给空值，表示禁用保存策略 12.2.12 小总结 十三、Redis持久化之AOF13.1 AOF（Append Only File）13.1.1 是什么以日志的形式来记录每个写操作（增量保存），将Redis执行过的所有写指令记录下来(读操作不记录)， 只许追加文件但不可以改写文件，redis启动之初会读取该文件重新构建数据，换言之，redis 重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作 13.1.2 AOF持久化流程（1）客户端的请求写命令会被append追加到AOF缓冲区内； （2）AOF缓冲区根据AOF持久化策略[always,everysec,no]将操作sync同步到磁盘的AOF文件中； （3）AOF文件大小超过重写策略或手动重写时，会对AOF文件rewrite重写，压缩AOF文件容量； （4）Redis服务重启时，会重新load加载AOF文件中的写操作达到数据恢复的目的； 13.1.3 AOF默认不开启可以在redis.conf中配置文件名称，默认为 appendonly.aof AOF文件的保存路径，同RDB的路径一致。 13.1.4 AOF和RDB同时开启，redis听谁的？AOF和RDB同时开启，系统默认取AOF的数据（数据不会存在丢失） 13.1.5 AOF启动/修复/恢复 AOF的备份机制和性能虽然和RDB不同, 但是备份和恢复的操作同RDB一样，都是拷贝备份文件，需要恢复时再拷贝到Redis工作目录下，启动系统即加载。 正常恢复 修改默认的appendonly no，改为yes 将有数据的aof文件复制一份保存到对应目录(查看目录：config get dir) 恢复：重启redis然后重新加载 异常恢复 修改默认的appendonly no，改为yes 如遇到AOF文件损坏，通过/usr/local/bin/redis-check-aof–fix appendonly.aof进行恢复 备份被写坏的AOF文件 恢复：重启redis，然后重新加载 13.1.6 AOF同步频率设置appendfsync always 始终同步，每次Redis的写入都会立刻记入日志；性能较差但数据完整性比较好 appendfsync everysec 每秒同步，每秒记入日志一次，如果宕机，本秒的数据可能丢失。 appendfsync no redis不主动进行同步，把同步时机交给操作系统。 13.1.7 Rewrite压缩 是什么： AOF采用文件追加方式，文件会越来越大为避免出现此种情况，新增了重写机制, 当AOF文件的大小超过所设定的阈值时，Redis就会启动AOF文件的内容压缩， 只保留可以恢复数据的最小指令集.可以使用命令bgrewriteaof 重写原理，如何实现重写 AOF文件持续增长而过大时，会fork出一条新进程来将文件重写(也是先写临时文件最后再rename)，redis4.0版本后的重写，是指上就是把rdb 的快照，以二级制的形式附在新的aof头部，作为已有的历史数据，替换掉原来的流水账操作。 no-appendfsync-on-rewrite： 如果 no-appendfsync-on-rewrite=yes ,不写入aof文件只写入缓存，用户请求不会阻塞，但是在这段时间如果宕机会丢失这段时间的缓存数据。（降低数据安全性，提高性能） 如果 no-appendfsync-on-rewrite=no, 还是会把数据往磁盘里刷，但是遇到重写操作，可能会发生阻塞。（数据安全，但是性能降低） 触发机制，何时重写 Redis会记录上次重写时的AOF大小，默认配置是当AOF文件大小是上次rewrite后大小的一倍且文件大于64M时触发 重写虽然可以节约大量磁盘空间，减少恢复时间。但是每次重写还是有一定的负担的，因此设定Redis要满足一定条件才会进行重写。 auto-aof-rewrite-percentage：设置重写的基准值，文件达到100%时开始重写（文件是原来重写后文件的2倍时触发） auto-aof-rewrite-min-size：设置重写的基准值，最小文件64MB。达到这个值开始重写。 例如：文件达到70MB开始重写，降到50MB，下次什么时候开始重写？100MB 系统载入时或者上次重写完毕时，Redis会记录此时AOF大小，设为base_size, 如果Redis的AOF当前大小&gt;= base_size +base_size*100% (默认)且当前大小&gt;=64mb(默认)的情况下，Redis会对AOF进行重写。 3、重写流程 （1）bgrewriteaof触发重写，判断是否当前有bgsave或bgrewriteaof在运行，如果有，则等待该命令结束后再继续执行。 （2）主进程fork出子进程执行重写操作，保证主进程不会阻塞。 （3）子进程遍历redis内存中数据到临时文件，客户端的写请求同时写入aof_buf缓冲区和aof_rewrite_buf重写缓冲区保证原AOF文件完整以及新AOF文件生成期间的新的数据修改动作不会丢失。 （4）1).子进程写完新的AOF文件后，向主进程发信号，父进程更新统计信息。2).主进程把aof_rewrite_buf中的数据写入到新的AOF文件。 （5）使用新的AOF文件覆盖旧的AOF文件，完成AOF重写。 13.1.8 优势 备份机制更稳健，丢失数据概率更低。 可读的日志文本，通过操作AOF稳健，可以处理误操作。 13.1.9 劣势 比起RDB占用更多的磁盘空间。 恢复备份速度要慢。 每次读写都同步的话，有一定的性能压力。 存在个别Bug，造成恢复不能。 13.1.10 小总结 13.2 总结(Which one)13.2.1 用哪个好官方推荐两个都启用。 如果对数据不敏感，可以选单独用RDB。 不建议单独用 AOF，因为可能会出现Bug。 如果只是做纯内存缓存，可以都不用。 13.2.2 官网建议 RDB持久化方式能够在指定的时间间隔能对你的数据进行快照存储 AOF持久化方式记录每次对服务器写的操作,当服务器重启的时候会重新执行这些命令来恢复原始的数据,AOF命令以redis协议追加保存每次写的操作到文件末尾. Redis还能对AOF文件进行后台重写,使得AOF文件的体积不至于过大 只做缓存：如果你只希望你的数据在服务器运行的时候存在,你也可以不使用任何持久化方式. 同时开启两种持久化方式 在这种情况下,当redis重启的时候会优先载入AOF文件来恢复原始的数据, 因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整. RDB的数据不实时，同时使用两者时服务器重启也只会找AOF文件。那要不要只使用AOF呢？ 建议不要，因为RDB更适合用于备份数据库(AOF在不断变化不好备份)， 快速重启，而且不会有AOF可能潜在的bug，留着作为一个万一的手段。 性能建议 123456因为RDB文件只用作后备用途，建议只在Slave上持久化RDB文件，而且只要15分钟备份一次就够了，只保留save 900 1这条规则。 如果使用AOF，好处是在最恶劣情况下也只会丢失不超过两秒数据，启动脚本较简单只load自己的AOF文件就可以了。代价,一是带来了持续的IO，二是AOF rewrite的最后将rewrite过程中产生的新数据写到新文件造成的阻塞几乎是不可避免的。只要硬盘许可，应该尽量减少AOF rewrite的频率，AOF重写的基础大小默认值64M太小了，可以设到5G以上。默认超过原大小100%大小时重写可以改到适当的数值。 十四、Redis_主从复制14.1 是什么主机数据更新后根据配置和策略， 自动同步到备机的master/slaver机制，Master以写为主，Slave以读为主 14.2 能干嘛 读写分离，性能扩展 容灾快速恢复 14.3 怎么玩：主从复制拷贝多个redis.conf文件include(写绝对路径) 开启daemonize yes Pid文件名字pidfile 指定端口port Log文件名字 dump.rdb名字dbfilename Appendonly 关掉或者换名字 14.3.1 新建redis6379.conf，填写以下内容include /myredis/redis.conf pidfile /var/run/redis_6379.pid port 6379 dbfilename dump6379.rdb 14.3.2 新建redis6380.conf，填写以下内容 14.3.3 新建redis6381.conf，填写以下内容 slave-priority 10 设置从机的优先级，值越小，优先级越高，用于选举主机时使用。默认100 14.3.4 启动三台redis服务器 14.3.5 查看系统进程，看看三台服务器是否启动 14.3.6 查看三台主机运行情况info replication 打印主从复制的相关信息 14.3.7 配从(库)不配主(库)slaveof 成为某个实例的从服务器 1、在6380和6381上执行: slaveof 127.0.0.1 6379 2、在主机上写，在从机上可以读取数据 在从机上写数据报错 3、主机挂掉，重启就行，一切如初 4、从机重启需重设：slaveof 127.0.0.1 6379 可以将配置增加到文件中。永久生效。 14.4 常用3招14.4.1 一主二仆切入点问题？slave1、slave2是从头开始复制还是从切入点开始复制?比如从k4进来，那之前的k1,k2,k3是否也可以复制？ 从机是否可以写？set可否？ 主机shutdown后情况如何？从机是上位还是原地待命？ 主机又回来了后，主机新增记录，从机还能否顺利复制？ 其中一台从机down后情况如何？依照原有它能跟上大部队吗？ 14.4.2 薪火相传上一个Slave可以是下一个slave的Master，Slave同样可以接收其他 slaves的连接和同步请求，那么该slave作为了链条中下一个的master, 可以有效减轻master的写压力,去中心化降低风险。 用 slaveof 中途变更转向:会清除之前的数据，重新建立拷贝最新的 风险是一旦某个slave宕机，后面的slave都没法备份 主机挂了，从机还是从机，无法写数据了 14.4.3 反客为主当一个master宕机后，后面的slave可以立刻升为master，其后面的slave不用做任何修改。 用 slaveof no one 将从机变为主机。 14.5 复制原理 Slave启动成功连接到master后会发送一个sync命令 Master接到命令启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令， 在后台进程执行完毕之后，master将传送整个数据文件到slave,以完成一次完全同步 全量复制：而slave服务在接收到数据库文件数据后，将其存盘并加载到内存中。 增量复制：Master继续将新的所有收集到的修改命令依次传给slave,完成同步 但是只要是重新连接master,一次完全同步（全量复制)将被自动执行 14.6 哨兵模式(sentinel)14.6.1 是什么反客为主的自动版，能够后台监控主机是否故障，如果故障了根据投票数自动将从库转换为主库 14.6.2 怎么玩(使用步骤) 调整为一主二仆模式，6379带着6380、6381 自定义的/myredis目录下新建sentinel.conf文件，名字绝不能错 配置哨兵,填写内容 sentinel monitor mymaster 127.0.0.1 6379 1 其中mymaster为监控对象起的服务器名称， 1 为至少有多少个哨兵同意迁移的数量。 启动哨兵 /usr/local/bin redis做压测可以用自带的redis-benchmark工具 执行redis-sentinel /myredis/sentinel.conf 当主机挂掉，从机选举中产生新的主机 (大概10秒左右可以看到哨兵窗口日志，切换了新的主机) 哪个从机会被选举为主机呢？根据优先级别：slave-priority 原主机重启后会变为从机。 复制延时 由于所有的写操作都是先在Master上操作，然后同步更新到Slave上，所以从Master同步到Slave机器有一定的延迟，当系统很繁忙的时候，延迟问题会更加严重，Slave机器数量的增加也会使这个问题更加严重。 14.6.3 故障恢复 优先级在redis.conf中默认：slave-priority 100，值越小优先级越高 偏移量是指获得原主机数据最全的 每个redis实例启动后都会随机生成一个40位的runid 14.6.4 主从复制123456789101112131415161718192021private static JedisSentinelPool jedisSentinelPool=null;public static Jedis getJedisFromSentinel()&#123;if(jedisSentinelPool==null)&#123; Set&lt;String&gt; sentinelSet=new HashSet&lt;&gt;(); sentinelSet.add(&quot;192.168.11.103:26379&quot;); JedisPoolConfig jedisPoolConfig =new JedisPoolConfig(); jedisPoolConfig.setMaxTotal(10); //最大可用连接数jedisPoolConfig.setMaxIdle(5); //最大闲置连接数jedisPoolConfig.setMinIdle(5); //最小闲置连接数jedisPoolConfig.setBlockWhenExhausted(true); //连接耗尽是否等待jedisPoolConfig.setMaxWaitMillis(2000); //等待时间jedisPoolConfig.setTestOnBorrow(true); //取连接的时候进行一下测试 ping pongjedisSentinelPool=new JedisSentinelPool(&quot;mymaster&quot;,sentinelSet,jedisPoolConfig);return jedisSentinelPool.getResource(); &#125;else&#123;return jedisSentinelPool.getResource(); &#125;&#125; 十五、Redis集群15.1 问题容量不够，redis如何进行扩容？ 并发写操作， redis如何分摊？ 另外，主从模式，薪火相传模式，主机宕机，导致ip地址发生变化，应用程序中配置需要修改对应的主机地址、端口等信息。 之前通过代理主机来解决，但是redis3.0中提供了解决方案。就是无中心化集群配置。 15.2 什么是集群Redis 集群实现了对Redis的水平扩容，即启动N个redis节点，将整个数据库分布存储在这N个节点中，每个节点存储总数据的1/N。 Redis 集群通过分区（partition）来提供一定程度的可用性（availability）： 即使集群中有一部分节点失效或者无法进行通讯， 集群也可以继续处理命令请求。 15.3 删除持久化数据将rdb,aof文件都删除掉。 15.4 制作6个实例，6379,6380,6381,6389,6390,639115.4.1 配置基本信息开启daemonize yes Pid文件名字 指定端口 Log文件名字 Dump.rdb名字 Appendonly 关掉或者换名字 15.4.2 redis cluster配置修改cluster-enabled yes 打开集群模式 cluster-config-file nodes-6379.conf 设定节点配置文件名 cluster-node-timeout 15000 设定节点失联时间，超过该时间（毫秒），集群自动进行主从切换。 123456789include /home/bigdata/redis.confport 6379pidfile &quot;/var/run/redis_6379.pid&quot;dbfilename &quot;dump6379.rdb&quot;dir &quot;/home/bigdata/redis_cluster&quot;logfile &quot;/home/bigdata/redis_cluster/redis_err_6379.log&quot;cluster-enabled yescluster-config-file nodes-6379.confcluster-node-timeout 15000 15.4.3 修改好redis6379.conf文件，拷贝多个redis.conf文件 15.4.4 使用查找替换修改另外5个文件例如：:%s/6379/6380 15.4.5 启动6个redis服务 15.5 将六个节点合成一个集群组合之前，请确保所有redis实例启动后，nodes-xxxx.conf文件都生成正常。 合体： cd /opt/redis-6.2.1/src 1redis-cli --cluster create --cluster-replicas 1 192.168.11.101:6379 192.168.11.101:6380 192.168.11.101:6381 192.168.11.101:6389 192.168.11.101:6390 192.168.11.101:6391 此处不要用127.0.0.1， 请用真实IP地址 –replicas 1 采用最简单的方式配置集群，一台主机，一台从机，正好三组。 普通方式登录 可能直接进入读主机，存储数据时，会出现MOVED重定向操作。所以，应该以集群方式登录。 15.6 -c 采用集群策略连接，设置数据会自动切换到相应的写主机 15.7 通过 cluster nodes 命令查看集群信息 15.8 redis cluster 如何分配这六个节点?一个集群至少要有三个主节点。 选项 –cluster-replicas 1 表示我们希望为集群中的每个主节点创建一个从节点。 分配原则尽量保证每个主数据库运行在不同的IP地址，每个从库和主库不在一个IP地址上。 15.9 什么是slots[OK] All nodes agree about slots configuration. Check for open slots… Check slots coverage… [OK] All 16384 slots covered. 一个 Redis 集群包含 16384 个插槽（hash slot）， 数据库中的每个键都属于这 16384 个插槽的其中一个， 集群使用公式 CRC16(key) % 16384 来计算键 key 属于哪个槽， 其中 CRC16(key) 语句用于计算键 key 的 CRC16 校验和 。 集群中的每个节点负责处理一部分插槽。 举个例子， 如果一个集群可以有主节点， 其中： 节点 A 负责处理 0 号至 5460 号插槽。 节点 B 负责处理 5461 号至 10922 号插槽。 节点 C 负责处理 10923 号至 16383 号插槽。 15.10 在集群中录入值在redis-cli每次录入、查询键值，redis都会计算出该key应该送往的插槽，如果不是该客户端对应服务器的插槽，redis会报错，并告知应前往的redis实例地址和端口。 redis-cli客户端提供了 –c 参数实现自动重定向。 如 redis-cli -c –p 6379 登入后，再录入、查询键值对可以自动重定向。 不在一个slot下的键值，是不能使用mget,mset等多键操作。 可以通过{}来定义组的概念，从而使key中{}内相同内容的键值对放到一个slot中去。 15.11 查询集群中的值CLUSTER GETKEYSINSLOT 返回 count 个 slot 槽中的键。 15.12 故障恢复如果主节点下线？从节点能否自动升为主节点？注意：15秒超时 主节点恢复后，主从关系会如何？主节点回来变成从机。 如果所有某一段插槽的主从节点都宕掉，redis服务是否还能继续? 如果某一段插槽的主从都挂掉，而cluster-require-full-coverage 为yes ，那么 ，整个集群都挂掉 如果某一段插槽的主从都挂掉，而cluster-require-full-coverage 为no ，那么，该插槽数据全都不能使用，也无法存储。 redis.conf中的参数 cluster-require-full-coverage 15.13 集群的Jedis开发即使连接的不是主机，集群会自动切换主机存储。主机写，从机读。 无中心化主从集群。无论从哪台主机写的数据，其他主机上都能读到数据。 123456789public class JedisClusterTest &#123; public static void main(String[] args) &#123; Set&lt;HostAndPort&gt;set =new HashSet&lt;HostAndPort&gt;(); set.add(new HostAndPort(&quot;192.168.31.211&quot;,6379)); JedisCluster jedisCluster=new JedisCluster(set); jedisCluster.set(&quot;k1&quot;, &quot;v1&quot;); System.out.println(jedisCluster.get(&quot;k1&quot;)); &#125;&#125; 15.14 Redis 集群提供了以下好处实现扩容 分摊压力 无中心配置相对简单 15.15 Redis 集群的不足多键操作是不被支持的 多键的Redis事务是不被支持的。lua脚本不被支持 由于集群方案出现较晚，很多公司已经采用了其他的集群方案，而代理或者客户端分片的方案想要迁移至redis cluster，需要整体迁移而不是逐步过渡，复杂度较大。 十六、Redis应用问题解决16.1 缓存穿透16.1.1 问题描述key对应的数据在数据源并不存在，每次针对此key的请求从缓存获取不到，请求都会压到数据源，从而可能压垮数据源。比如用一个不存在的用户id获取用户信息，不论缓存还是数据库都没有，若黑客利用此漏洞进行攻击可能压垮数据库。 16.1.2 解决方案一个一定不存在缓存及查询不到的数据，由于缓存是不命中时被动写的，并且出于容错考虑，如果从存储层查不到数据则不写入缓存，这将导致这个不存在的数据每次请求都要到存储层去查询，失去了缓存的意义。 解决方案： （1） 对空值缓存：如果一个查询返回的数据为空（不管是数据是否不存在），我们仍然把这个空结果（null）进行缓存，设置空结果的过期时间会很短，最长不超过五分钟 （2） 设置可访问的名单（白名单）： 使用bitmaps类型定义一个可以访问的名单，名单id作为bitmaps的偏移量，每次访问和bitmap里面的id进行比较，如果访问id不在bitmaps里面，进行拦截，不允许访问。 （3） 采用布隆过滤器：(布隆过滤器（Bloom Filter）是1970年由布隆提出的。它实际上是一个很长的二进制向量(位图)和一系列随机映射函数（哈希函数）。 布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。) 将所有可能存在的数据哈希到一个足够大的bitmaps中，一个一定不存在的数据会被 这个bitmaps拦截掉，从而避免了对底层存储系统的查询压力。 （4） 进行实时监控：当发现Redis的命中率开始急速降低，需要排查访问对象和访问的数据，和运维人员配合，可以设置黑名单限制服务 16.2 缓存击穿16.2.1 问题描述key对应的数据存在，但在redis中过期，此时若有大量并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。 16.2.2 解决方案key可能会在某些时间点被超高并发地访问，是一种非常“热点”的数据。这个时候，需要考虑一个问题：缓存被“击穿”的问题。 解决问题： （1）预先设置热门数据：在redis高峰访问之前，把一些热门数据提前存入到redis里面，加大这些热门数据key的时长 （2）实时调整：现场监控哪些数据热门，实时调整key的过期时长 （3）使用锁： （1） 就是在缓存失效的时候（判断拿出来的值为空），不是立即去load db。 （2） 先使用缓存工具的某些带成功操作返回值的操作（比如Redis的SETNX）去set一个mutex key （3） 当操作返回成功时，再进行load db的操作，并回设缓存,最后删除mutex key； （4） 当操作返回失败，证明有线程在load db，当前线程睡眠一段时间再重试整个get缓存的方法。 16.3 缓存雪崩16.3.1 问题描述key对应的数据存在，但在redis中过期，此时若有大量并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。 缓存雪崩与缓存击穿的区别在于这里针对很多key缓存，前者则是某一个key 正常访问 缓存失效瞬间 16.3.2 解决方案缓存失效时的雪崩效应对底层系统的冲击非常可怕！ 解决方案： （1） 构建多级缓存架构：nginx缓存 + redis缓存 +其他缓存（ehcache等） （2） 使用锁或队列： 用加锁或者队列的方式保证来保证不会有大量的线程对数据库一次性进行读写，从而避免失效时大量的并发请求落到底层存储系统上。不适用高并发情况 （3） 设置过期标志更新缓存： 记录缓存数据是否过期（设置提前量），如果过期会触发通知另外的线程在后台去更新实际key的缓存。 （4） 将缓存失效时间分散开： 比如我们可以在原有的失效时间基础上增加一个随机值，比如1-5分钟随机，这样每一个缓存的过期时间的重复率就会降低，就很难引发集体失效的事件。 16.4 分布式锁16.4.1 问题描述随着业务发展的需要，原单体单机部署的系统被演化成分布式集群系统后，由于分布式系统多线程、多进程并且分布在不同机器上，这将使原单机部署情况下的并发控制锁策略失效，单纯的Java API并不能提供分布式锁的能力。为了解决这个问题就需要一种跨JVM的互斥机制来控制共享资源的访问，这就是分布式锁要解决的问题！ 分布式锁主流的实现方案： 基于数据库实现分布式锁 基于缓存（Redis等） 基于Zookeeper 每一种分布式锁解决方案都有各自的优缺点： 性能：redis最高 可靠性：zookeeper最高 这里，我们就基于redis实现分布式锁。 16.4.2 解决方案：使用redis实现分布式锁redis:命令 # set sku:1:info “OK” NX PX 10000 EX second ：设置键的过期时间为 second 秒。 SET key value EX second 效果等同于 SETEX key second value 。 PX millisecond ：设置键的过期时间为 millisecond 毫秒。 SET key value PX millisecond 效果等同于 PSETEX key millisecond value 。 NX ：只在键不存在时，才对键进行设置操作。 SET key value NX 效果等同于 SETNX key value 。 XX ：只在键已经存在时，才对键进行设置操作。 多个客户端同时获取锁（setnx） 获取成功，执行业务逻辑{从db获取数据，放入缓存}，执行完成释放锁（del） 其他客户端等待重试 编写代码 Redis: set num 0 12345678910111213141516171819202122232425262728@GetMapping(&quot;testLock&quot;)public void testLock()&#123; //1获取锁，setne Boolean lock = redisTemplate.opsForValue().setIfAbsent(&quot;lock&quot;, &quot;111&quot;); //2获取锁成功、查询num的值 if(lock)&#123; Object value = redisTemplate.opsForValue().get(&quot;num&quot;); //2.1判断num为空return if(StringUtils.isEmpty(value))&#123; return; &#125; //2.2有值就转成成int int num = Integer.parseInt(value+&quot;&quot;); //2.3把redis的num加1 redisTemplate.opsForValue().set(&quot;num&quot;, ++num); //2.4释放锁，del redisTemplate.delete(&quot;lock&quot;); &#125;else&#123; //3获取锁失败、每隔0.1秒再获取 try &#123; Thread.sleep(100); testLock(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 重启，服务集群，通过网关压力测试： ab -n 1000 -c 100 http://192.168.140.1:8080/test/testLock 查看redis中num的值： 基本实现。 问题：setnx刚好获取到锁，业务逻辑出现异常，导致锁无法释放 解决：设置过期时间，自动释放锁。 16.4.4 优化之设置锁的过期时间设置过期时间有两种方式： 首先想到通过expire设置过期时间（缺乏原子性：如果在setnx和expire之间出现异常，锁也无法释放） 在set时指定过期时间（推荐） 设置过期时间： 压力测试肯定也没有问题。自行测试 问题：可能会释放其他服务器的锁。 场景：如果业务逻辑的执行时间是7s。执行流程如下 index1业务逻辑没执行完，3秒后锁被自动释放。 index2获取到锁，执行业务逻辑，3秒后锁被自动释放。 index3获取到锁，执行业务逻辑 index1业务逻辑执行完成，开始调用del释放锁，这时释放的是index3的锁，导致index3的业务只执行1s就被别人释放。 最终等于没锁的情况。 解决：setnx获取锁时，设置一个指定的唯一值（例如：uuid）；释放前获取这个值，判断是否自己的锁 16.4.5 优化之UUID防误删 问题：删除操作缺乏原子性。 场景： index1执行删除时，查询到的lock值确实和uuid相等 uuid=v1 set(lock,uuid)； index1执行删除前，lock刚好过期时间已到，被redis自动释放 在redis中没有了lock，没有了锁。 index2获取了lock index2线程获取到了cpu的资源，开始执行方法 uuid=v2 set(lock,uuid)； index1执行删除，此时会把index2的lock删除 index1 因为已经在方法中了，所以不需要重新上锁。index1有执行的权限。index1已经比较完成了，这个时候，开始执行 删除的index2的锁！ 16.4.6 优化之LUA脚本保证删除的原子性1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@GetMapping(&quot;testLockLua&quot;)public void testLockLua() &#123; //1 声明一个uuid ,将做为一个value 放入我们的key所对应的值中 String uuid = UUID.randomUUID().toString(); //2 定义一个锁：lua 脚本可以使用同一把锁，来实现删除！ String skuId = &quot;25&quot;; // 访问skuId 为25号的商品 100008348542 String locKey = &quot;lock:&quot; + skuId; // 锁住的是每个商品的数据 // 3 获取锁 Boolean lock = redisTemplate.opsForValue().setIfAbsent(locKey, uuid, 3, TimeUnit.SECONDS); // 第一种： lock 与过期时间中间不写任何的代码。 // redisTemplate.expire(&quot;lock&quot;,10, TimeUnit.SECONDS);//设置过期时间 // 如果true if (lock) &#123; // 执行的业务逻辑开始 // 获取缓存中的num 数据 Object value = redisTemplate.opsForValue().get(&quot;num&quot;); // 如果是空直接返回 if (StringUtils.isEmpty(value)) &#123; return; &#125; // 不是空 如果说在这出现了异常！ 那么delete 就删除失败！ 也就是说锁永远存在！ int num = Integer.parseInt(value + &quot;&quot;); // 使num 每次+1 放入缓存 redisTemplate.opsForValue().set(&quot;num&quot;, String.valueOf(++num)); /*使用lua脚本来锁*/ // 定义lua 脚本 String script = &quot;if redis.call(&#x27;get&#x27;, KEYS[1]) == ARGV[1] then return redis.call(&#x27;del&#x27;, KEYS[1]) else return 0 end&quot;; // 使用redis执行lua执行 DefaultRedisScript&lt;Long&gt; redisScript = new DefaultRedisScript&lt;&gt;(); redisScript.setScriptText(script); // 设置一下返回值类型 为Long // 因为删除判断的时候，返回的0,给其封装为数据类型。如果不封装那么默认返回String 类型， // 那么返回字符串与0 会有发生错误。 redisScript.setResultType(Long.class); // 第一个要是script 脚本 ，第二个需要判断的key，第三个就是key所对应的值。 redisTemplate.execute(redisScript, Arrays.asList(locKey), uuid); &#125; else &#123; // 其他线程等待 try &#123; // 睡眠 Thread.sleep(1000); // 睡醒了之后，调用方法。 testLockLua(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; Lua 脚本详解： 项目中正确使用： 1231. 定义key，key应该是为每个sku定义的，也就是每个sku有一把锁。String locKey =&quot;lock:&quot;+skuId; // 锁住的是每个商品的数据Boolean lock = redisTemplate.opsForValue().setIfAbsent(locKey, uuid,3,TimeUnit.SECONDS); 总结 1、加锁 1234// 1. 从redis中获取锁,set k1 v1 px 20000 nxString uuid = UUID.randomUUID().toString();Boolean lock = this.redisTemplate.opsForValue() .setIfAbsent(&quot;lock&quot;, uuid, 2, TimeUnit.SECONDS); 2、使用lua释放锁 12345678// 2. 释放锁 delString script = &quot;if redis.call(&#x27;get&#x27;, KEYS[1]) == ARGV[1] then return redis.call(&#x27;del&#x27;, KEYS[1]) else return 0 end&quot;;// 设置lua脚本返回的数据类型DefaultRedisScript&lt;Long&gt; redisScript = new DefaultRedisScript&lt;&gt;();// 设置lua脚本返回类型为LongredisScript.setResultType(Long.class);redisScript.setScriptText(script);redisTemplate.execute(redisScript, Arrays.asList(&quot;lock&quot;),uuid); 3、重试 12Thread.sleep(500);testLock(); 为了确保分布式锁可用，我们至少要确保锁的实现同时满足以下四个条件： - 互斥性。在任意时刻，只有一个客户端能持有锁。 - 不会发生死锁。即使有一个客户端在持有锁的期间崩溃而没有主动解锁，也能保证后续其他客户端能加锁。 - 解铃还须系铃人。加锁和解锁必须是同一个客户端，客户端自己不能把别人加的锁给解了。 - 加锁和解锁必须具有原子性。 十七、 Redis6.0新功能17.1 ACL17.1.1 简介Redis ACL是Access Control List（访问控制列表）的缩写，该功能允许根据可以执行的命令和可以访问的键来限制某些连接。 在Redis 5版本之前，Redis 安全规则只有密码控制 还有通过rename 来调整高危命令比如 flushdb ， KEYS* ， shutdown 等。Redis 6 则提供ACL的功能对用户进行更细粒度的权限控制 ： （1）接入权限:用户名和密码 （2）可以执行的命令 （3）可以操作的 KEY 参考官网：https://redis.io/topics/acl 17.1.2 命令1、使用acl list命令展现用户权限列表 （1）数据说明 2、使用acl cat命令 （1）查看添加权限指令类别 （2）加参数类型名可以查看类型下具体命令 3、使用acl whoami命令查看当前用户 4、使用aclsetuser命令创建和编辑用户ACL （1）ACL规则 下面是有效ACL规则的列表。某些规则只是用于激活或删除标志，或对用户ACL执行给定更改的单个单词。其他规则是字符前缀，它们与命令或类别名称、键模式等连接在一起。 ACL规则 类型 参数 说明 启动和禁用用户 on 激活某用户账号 off 禁用某用户账号。注意，已验证的连接仍然可以工作。如果默认用户被标记为off，则新连接将在未进行身份验证的情况下启动，并要求用户使用AUTH选项发送AUTH或HELLO，以便以某种方式进行身份验证。 权限的添加删除 + 将指令添加到用户可以调用的指令列表中 - 从用户可执行指令列表移除指令 +@ 添加该类别中用户要调用的所有指令，有效类别为@admin、@set、@sortedset…等，通过调用ACL CAT命令查看完整列表。特殊类别@all表示所有命令，包括当前存在于服务器中的命令，以及将来将通过模块加载的命令。 -@ 从用户可调用指令中移除类别 allcommands +@all的别名 nocommand -@all的别名 可操作键的添加或删除 ~ 添加可作为用户可操作的键的模式。例如~*允许所有的键 （2）通过命令创建新用户默认权限 acl setuser user1 在上面的示例中，我根本没有指定任何规则。如果用户不存在，这将使用just created的默认属性来创建用户。如果用户已经存在，则上面的命令将不执行任何操作。 （3）设置有用户名、密码、ACL权限、并启用的用户 acl setuser user2 on &gt;password ~cached:* +get (4)切换用户，验证权限 17.2 IO多线程17.2.1 简介Redis6终于支撑多线程了，告别单线程了吗？ IO多线程其实指客户端交互部分的网络IO交互处理模块多线程，而非执行命令多线程。Redis6执行命令依然是单线程。 17.2.2 原理架构Redis 6 加入多线程,但跟 Memcached 这种从 IO处理到数据访问多线程的实现模式有些差异。Redis 的多线程部分只是用来处理网络数据的读写和协议解析，执行命令仍然是单线程。之所以这么设计是不想因为多线程而变得复杂，需要去控制 key、lua、事务，LPUSH/LPOP 等等的并发问题。整体的设计大体如下: 另外，多线程IO默认也是不开启的，需要再配置文件中配置 io-threads-do-reads yes io-threads 4 17.3 工具支持 Cluster之前老版Redis想要搭集群需要单独安装ruby环境，Redis 5 将 redis-trib.rb 的功能集成到 redis-cli 。另外官方 redis-benchmark 工具开始支持 cluster 模式了，通过多线程的方式对多个分片进行压测。 17.4 Redis新功能持续关注Redis6新功能还有： 1、RESP3新的 Redis 通信协议：优化服务端与客户端之间通信 2、Client side caching客户端缓存：基于 RESP3 协议实现的客户端缓存功能。为了进一步提升缓存的性能，将客户端经常访问的数据cache到客户端。减少TCP网络交互。 3、Proxy集群代理模式：Proxy 功能，让 Cluster 拥有像单实例一样的接入方式，降低大家使用cluster的门槛。不过需要注意的是代理不改变 Cluster 的功能限制，不支持的命令还是不会支持，比如跨 slot 的多Key操作。 4、Modules API Redis 6中模块API开发进展非常大，因为Redis Labs为了开发复杂的功能，从一开始就用上Redis模块。Redis可以变成一个框架，利用Modules来构建不同系统，而不需要从头开始写然后还要BSD许可。Redis一开始就是一个向编写各种系统开放的平台。 原视频地址 原文章地址","categories":[{"name":"Redis","slug":"Redis","permalink":"https://lwy0518.github.io/categories/Redis/"}],"tags":[{"name":"Redis","slug":"Redis","permalink":"https://lwy0518.github.io/tags/Redis/"},{"name":"数据库","slug":"数据库","permalink":"https://lwy0518.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"分布式","slug":"分布式","permalink":"https://lwy0518.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"}],"author":"lwy"},{"title":"偏序介绍","slug":"偏序介绍","date":"2021-12-27T13:21:29.000Z","updated":"2022-01-18T03:16:08.716Z","comments":true,"path":"2021/12/27/偏序介绍/","link":"","permalink":"https://lwy0518.github.io/2021/12/27/%E5%81%8F%E5%BA%8F%E4%BB%8B%E7%BB%8D/","excerpt":"偏序介绍 偏序 全序 偏序与全序区别 Lattice（格） Lattice（格） Semilattice（半格） Complete Lattice（全格） Product Lattice（乘积格） 上界与下界","text":"偏序介绍 偏序 全序 偏序与全序区别 Lattice（格） Lattice（格） Semilattice（半格） Complete Lattice（全格） Product Lattice（乘积格） 上界与下界 偏序 参考链接 全序与偏序 偏序 定义 Partiallyordered set，简写poset 设R是集合A上的一个二元关系，若R满足： Ⅰ 自反性：对任意x∈A，有xRx； Ⅱ 反对称性（即反对称关系）：对任意x,y∈A，若xRy，且yRx，则x=y； Ⅲ 传递性：对任意x, y,z∈A，若xRy，且yRz，则xRz。 则称R为A上的偏序关系，通常记作≼。注意这里的≼不必是指一般意义上的“小于或等于”。 若然有x≼y，我们也说x排在y前面（x precedes y）。 分类 严格偏序，反自反偏序 给定集合S，“&lt;”是S上的二元关系，若“&lt;”满足： 反自反性：∀a∈S，有a≮a； 非对称性：∀a，b∈S，a&lt;b ⇒ b≮a； 传递性：∀a，b，c∈S，a&lt;b且b&lt;c，则a&lt;c； 则称“&lt;”是S上的严格偏序或反自反偏序 严格偏序与有向无环图（dag）有直接的对应关系。一个集合上的严格偏序的关系图就是一个有向无环图。其传递闭包是它自己 非严格偏序，自反偏序 给定集合S，“≤”是S上的二元关系，若“≤”满足： 自反性：∀a∈S，有a≤a； 反对称性：∀a，b∈S，a≤b且b≤a，则a=b； 传递性：∀a，b，c∈S，a≤b且b≤c，则a≤c； 则称“≤”是S上的非严格偏序或自反偏序。 例子 假设有 A={1,2,3,4}，假设R是集合A上的关系：{&lt;1,1&gt;,&lt;2,2&gt;,&lt;3,3&gt;,&lt;4,4&gt;,&lt;1,2&gt;,&lt;1,4&gt;,&lt;2,4&gt;,&lt;3,4&gt;}，那么： 自反性：可以看到 &lt;1,1&gt;,&lt;2,2&gt;,&lt;3,3&gt;,&lt;4,4&gt; 都在R中，满足。 反对称性：由于 &lt;1,1&gt;,&lt;2,2&gt;,&lt;3,3&gt;,&lt;4,4&gt; 不属于 x !=y ，所以不考虑这4种，对于 &lt;1,2&gt;，有 &lt;2,1&gt; 不在R中；对于&lt;2,4&gt; 有&lt;4,2&gt;不在R中；对于&lt;3,4&gt; 有&lt;4,3&gt; 不在 R中，满足。 传递性：&lt;1,1&gt;&lt;1,2&gt;在R中，并且&lt;1,2&gt;在R中；&lt;1,1&gt;&lt;1,4&gt;在R中，并且&lt;1,4&gt;在R中；&lt;2,2&gt;&lt;2,4&gt;在R中，并且&lt;2,4&gt;在R中；&lt;3,3&gt;&lt;3,4&gt;在R中，并且&lt;3,4&gt;在R中；等等其他，满足。 所以说R是偏序关系。 全序 定义 如果R是A上的偏序关系，那么对于任意的A集合上的 x,y，都有 x &lt;= y，或者 y &lt;= x，二者必居其一，那么则称R是A上的全序关系 设集合X上有一全序关系，如果我们把这种关系用 ≤ 表述，则下列陈述对于 X 中的所有 a, b 和 c 成立： 如果 a ≤ b 且 b ≤ a 则 a = b (反对称性) 如果 a ≤ b 且 b ≤ c 则 a ≤ c (传递性) a ≤ b 或 b ≤ a (完全性) 例子 假设有 A={a,b,c}，假设R是集合A上的关系：{&lt;a,a&gt;,&lt;b,b&gt;,&lt;c,c&gt;,&lt;a,b&gt;,&lt;a,c&gt;,&lt;b,c&gt;}和上述一样，可以证明具有自反性，反对称性，传递性，所以是偏序的，有因为有 &lt;a,b&gt;,&lt;a,c&gt;,&lt;b,c&gt;， 也就是说两两关系都有了，所以满足对于任意的A集合上的 x,y，都有 x &lt;= y，或者 y &lt;= x，二者必居其一，所以说是全序关系 区别 偏序集合：配备了偏序关系的集合。 偏序：只对部分要元素成立关系（部分可比） 集合内只有部分元素之间在这个关系下是可以比较的。 比如：比如复数集中并不是所有的数都可以比较大小，那么“大小”就是复数集的一个偏序关系 全序集合：配备了全序关系的集合。 全序：对集合中任意两个元素都有关系 集合内任何一对元素在在这个关系下都是相互可比较的。 比如：有限长度的序列按字典序是全序的。最常见的是单词在字典中是全序的 例子 集合的包含关系是一种偏序。 在正整数集中定义偏序：若a能整除b，我们就记为a≺b 显然它满足序公理。但整数集中，不是任何两个数都存在整除关系，这个关系是局部的（partial），太“偏颇”，于是被称为偏序 Lattice 参考链接 格 Lattice（格） 定义 如果一个偏序集的任意两个元素都有最小上界和最大下界，那么这一偏序集是一个格 Semilattice（半格） 定义 最小上界和最大下界只存在一个的偏序集称半格，只存在最小上界称为“join semilattice”，只存在最大下界称为“meet semilattice” Complete Lattice（全格） 定义 一个偏序集的任意子集均存在最小上界和最大下界，那么这个偏序集成为全格 特点 每个全格都存在一个最大元素 top（⊤=⊔P）和最小元素bottom（⊥=⊓P） 所有元素有限的格（finite lattice）均是全格。（反之不成立） Product Lattice（乘积格） 给定n个lattice，L1 = (P1, ⊑1), L2 = (P2, ⊑2), …, Ln = (Pn, ⊑n)，如果每个lattice都有对应的⊔i(最小上界)和⊓i(最大下界)，那么我们得到一个Product Lattice Ln = (P, ⊑)并有以下四个定义： 1.P = P1 × … × Pn 2.(x1, …, xn) ⊑ (y1, …, yn) ⟺ (x1 ⊑ y1) ∧ … ∧ (xn ⊑ yn) 3.(x1, …, xn) ⊔ (y1, …, yn) = (x1 ⊔1 y1, …, xn ⊔n yn) 4.(x1, …, xn) ⊓ (y1, …, yn) = (x1 ⊓1 y1, …, xn ⊓n yn) Product Lattice仍是Lattice，若每个子格为全格，那么乘积也是全格 上界与下界 前提 给定偏序集(P, ⊑)及其子集S，满足S ⊆ P，则有： 上界（不唯一） 若∀x ∈ S, x ⊑ u,那么u ∈ P是子集S的上界 下界（不唯一） 若∀x ∈ S, l ⊑ x,那么l ∈ P是子集S的下界 最小上界⊔S(lub) 最大下界⊓S(glb) 特点 不是每个偏序都有lub和glb 但是如果一个偏序有lub和glb，那么它就是唯一的","categories":[{"name":"偏序","slug":"偏序","permalink":"https://lwy0518.github.io/categories/%E5%81%8F%E5%BA%8F/"}],"tags":[{"name":"偏序","slug":"偏序","permalink":"https://lwy0518.github.io/tags/%E5%81%8F%E5%BA%8F/"},{"name":"Lattice","slug":"Lattice","permalink":"https://lwy0518.github.io/tags/Lattice/"}],"author":"lwy"},{"title":"gcc中的SSA化算法","slug":"gcc中的SSA算法","date":"2021-12-26T07:44:24.000Z","updated":"2021-12-27T02:35:49.633Z","comments":true,"path":"2021/12/26/gcc中的SSA算法/","link":"","permalink":"https://lwy0518.github.io/2021/12/26/gcc%E4%B8%AD%E7%9A%84SSA%E7%AE%97%E6%B3%95/","excerpt":"SSA化算法过程 gcc中SSA化的算法描述 计算当前函数的支配结点边界 遍历并记录函数每条指令中出现的变量的defs/uses信息 为每个变量计算phi函数的插入点 按照支配树的DFS遍历重写每个bb的每条指令并将整个函数SSA化 gcc中ssa化的整体流程 pass_build_ssa 函数指令序列的def/use分析 为每个变量插入phi函数 指令重写(SSA化)","text":"SSA化算法过程 gcc中SSA化的算法描述 计算当前函数的支配结点边界 遍历并记录函数每条指令中出现的变量的defs/uses信息 为每个变量计算phi函数的插入点 按照支配树的DFS遍历重写每个bb的每条指令并将整个函数SSA化 gcc中ssa化的整体流程 pass_build_ssa 函数指令序列的def/use分析 为每个变量插入phi函数 指令重写(SSA化) SSA化算法过程gcc中SSA化的算法描述SSA化的流程可以简化为: (1)对所有变量插入必要的phi函数 (2)依次重写各个变量的定值/使用(转化为SSA格式) 而具体到gcc中则大体分为4个步骤: 计算当前函数的支配结点边界​ 按照迭代的必经结点边界算法, 要想确定phi函数插入的位置,必须要先根据函数的CFG确定函数的支配结点边界(支配结点边界的定义和算法见[2]), 在gcc中则是 1) 先通过calculate_dominance_info计算当前函数的支配结点信息 2) 再通过compute_dominance_frontiers计算当前函数的支配结点边界信息 遍历并记录函数每条指令中出现的变量的defs/uses信息​ 此步主要是收集函数每条指令中出现的变量的defs/uses信息,这些信息保存在标量的bitmap中, 故可以按照任意的顺序遍历指令,而在gcc中则是采用基于支配树的深度优先遍历(DFS)顺序遍历各个基本块(bb)中的一条条指令 为每个变量计算phi函数的插入点​ 1)中已经计算出了每个bb的支配结点边界信息；2)中计算出了每个变量的defs/uses信息, 按照迭代的必经结点边界算法, 此变量每一个def出现的结点的必经结点边界结点均需要插入一个phi函数, 插入后的phi函数作为此变量的一个新定值(def)点参与迭代计算. 1针对当前函数中的每个变量均需要通过此算法计算出其需要插入phi函数的结点信息,并在对应结点为每个变量分别插入phi函数 按照支配树的DFS遍历重写每个bb的每条指令并将整个函数SSA化​ 这里和步骤2)不同, SSA化的过程必须按照支配树的深度优先算法遍历结点, 123456789(1)对于每个结点顺序需遍历所有语句,对于每条语句: A.若发现一个操作数中涉及变量使用(use)时,需要将此操作数改为对此变量在DFS上最新定义(SSA_NAME)的使用 B.若发现一个操作数中涉及变量定值(def)时,需要为此变量新创建一个SSA_NAME作为此变量的最新定义, 并将此此操作数改为新创建的SSA_NAME (2) 语句处理完毕后续为此结点的每个后继结点的所有phi函数确定一个参数(phi函数的第x个参数来自其第x个前驱结点) phi函数对应原始变量在4.4)-1)-2)中的最新定值(def)则为phi函数的第x个参数,代表运行时来若控制流来自第x个前驱,则当前参数x的定值应来自哪个SSA_NAME gcc中ssa化的整体流程pass_build_ssa​ 在gcc中 pass_build_ssa 负责对一个函数进行首次ssa转化,而 update_ssa则负责对转化后的ssa的更新, pass_build_ssa大体流程如下: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960/* pass_build_ssa负责对函数cfun的SSA化,整个SSA化可以分为4步: 1.计算当前函数的支配结点边界信息 //compute_dominance_frontiers (dfs); 2.以支配树的DFS遍历为顺序, 标记每条指令中的def/use信息到各个全局变量 //mark_def_dom_walker (CDI_DOMINATORS).walk (fun-&gt;cfg-&gt;x_entry_block_ptr); 3.根据1中的支配节点边界信息和2中统计的每个变量的def/use信息, 确认每个 变量需要在哪些bb中插入phi函数,并为每个变量插入对应个数的phi函数 //insert_phi_nodes (dfs); 4.以支配树的DFS遍历为顺序,遍历所有bb并重写每条指令中的所有use/def操作数(SSA化) //rewrite_blocks 1) 在遍历到每个bb时执行: 1) 将bb中所有phi函数的返回值,写入到其对应变量的当前定义中(currdef) 2) 遍历bb中所有指令序列,并重写其中的变量: 1) 当发现一个变量的use时,用其currdef这个SSA_NAME替换指令中原有的use操作数 2) 当发现一个变量的def时,为其新创建一个SSA_NAME替换指令中原有的def操作数,并将此SSA_NAME记录到此变量的currdef中,以及入栈 3) 重写此bb所有后续bb中每个phi函数的第i个参数,i为当前bb在其后续bb的前驱bbs中的编号,phi函数的第i个参数被修改为其对应变量var在当前bb中最后的def(currdef) 2) 在每个bb的所有sub bb都遍历结束后: 1) 将4-1-2-2)中栈中保存的所有def都恢复到其对应变量的currdef中, 以保证当前bb分析完毕其父bb中的变量定义不变,继续rename其兄弟bb时才能保证正确*/unsigned int pass_build_ssa::execute (function *fun)&#123; bitmap_head *dfs; basic_block bb; init_ssa_operands (fun); /* 此函数主要为当前函数分配了virtual use/def操作的虚拟操作数vop */ init_ssa_renamer (); /* 标记当前函数尚未处于SSA格式, 初始化记录此函数中每个变量def/use的 var_infos 数组 */ /* 根据此函数基本块的数目分配一个一维的bitmap并初始化, 后续use/def分析中会在此bitmap中标记出现了use/def的bb(大部分都会出现) */ interesting_blocks = sbitmap_alloc (last_basic_block_for_fn (fun)); bitmap_clear (interesting_blocks); /* 根据此函数基本块数目,分配并初始化一个二维的bitmap矩阵(dfs[x][x]),后续此bitmap用来记录此函数中每个bb的支配结点边界信息 */ dfs = XNEWVEC (bitmap_head, last_basic_block_for_fn (fun)); FOR_EACH_BB_FN (bb, fun) bitmap_initialize (&amp;dfs[bb-&gt;index], &amp;bitmap_default_obstack); /* step1:根据此函数(cfun)的CFG寄存器支配树和每个bb的支配结点边界[2], 支配节点边界是判断是否需要插入phi函数的关键,最终dfs中: * dfs[x]记录bb x的支配节点边界 * dfs[x][y] = true; 代表y是x节点的一个支配节点边界节点 */ calculate_dominance_info (CDI_DOMINATORS); /* 这里先根据CFG计算此函数的支配树,支配树以一个个支配结点结构体保存在各个 bb-&gt;doms[0]中 */ compute_dominance_frontiers (dfs); /* 根据支配树信息(bbs-&gt;doms), 计算所有bb的支配结点边界信息并记录到二维bitmap dfs中 */ /* step2:按照DFS算法遍历此函数的支配树中每个结点(bbs-&gt;doms),对于每个结点都要遍历其整个指令序列(bb-&gt;gimple_df-&gt;seqs)中的 每一条语句(stmt),并将每条语句的def/use信息更新到 stmt-&gt;vuse/vdef/use_ops的同时,也更新到全局的var_infos/interest_blocks (针对整个函数),以及m_kills(针对当前bb)中 */ mark_def_dom_walker (CDI_DOMINATORS).walk (fun-&gt;cfg-&gt;x_entry_block_ptr); /* step3: 根据支配结点边界信息和每个变量的def/use信息,为每个变量在必要的bb中插入此变量的phi函数, 一个bb中所有插入的phi函数均记录在其bb-&gt;gimple_df-&gt;phi_nodes链表中, 按照算法, 某个变量var若在某个bb中出现了def,则此bb的支配结点边界中所有的bbs都要插入var的phi函数 需要注意的是:此时插入的phi函数只有返回值结点(为var新生成的SSA_NAME),其参数列表为空,只有在其前驱结点重写后phi函数的参数才能得以填充 */ insert_phi_nodes (dfs); /* step4: 按照DFS算法再次遍历函数的支配树,并重写每个bb中的每条指令stmt(也就是SSA化), stmt中的每个use要改为对齐currdef(SSA_NAME)的use, 每个def都要新建一个新的SSA_NAME; 对于一个bb的SSA化又分为三步: 1. 将此bb中所有phi函数的返回值设置为其对应变量的currdef 2. 遍历此bb中所有stmt, 将其中所有use的操作数改为其currdef; 若碰到def则为其原始变量var生成一个新的SSA_NAME,将def的操作数和currdef替换为新的SSA_NAME 3. 遍历此bb在函数CFG中所有的后继bb, 填充后继bb中所有phi指令的一个参数(第x个参数,x为此bb在其后继bb的所有前驱结点中的编号) */ rewrite_blocks (ENTRY_BLOCK_PTR_FOR_FN (fun), REWRITE_ALL); ...... return 0;&#125; 函数指令序列的def/use分析​ 要分析当前函数中所有变量的def/use信息,这是通过函数mark_def_dom_walker::walk()实现的: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768class mark_def_dom_walker : public dom_walker /* 没有单独实现 walk函数,继承父类walk函数 */&#123;public: mark_def_dom_walker (cdi_direction direction); ~mark_def_dom_walker (); virtual edge before_dom_children (basic_block);private: bitmap m_kills; /* 此bitmap可以看做是一个以变量UID为下标的一维数组, m_kills[x] = 1;则代表UID为x的变量在当前分析的bb中出现了kill def */&#125;; class dom_walker&#123; ...... void walk (basic_block); ......&#125; /* 在此函数执行前,bb所在的函数必须已经计算过了支配树(calculate_dominance_info),此时计算结果已经保存到各个bb-&gt;doms中了, 而此函数则通过深度优先遍历算法(DFS)遍历从bb开始的整个支配树的所有结点,且: * 在刚遍历到某个结点时,为当前结点调用before_dom_children(basic_block);回调 - 若此回调返回STOP,则其所有子节点不必再遍历 - 若此回调返回非STOP,则按照DFS继续遍历其子结点 * 在某个结点(及其子节点)遍历完毕后, 为当前结点调用after_dom_children(basic_block);回调*/void dom_walker::walk (basic_block bb)&#123; int sp = 0; basic_block dest; basic_block *worklist = XNEWVEC (basic_block, n_basic_blocks_for_fn (cfun) * 2); /* 为后续递归遍历分配临时的结点栈 */ ...... while (true) &#123; /* 只遍历有前驱的bb或函数出入口bb */ if (EDGE_COUNT (bb-&gt;preds) &gt; 0 || bb == ENTRY_BLOCK_PTR_FOR_FN (cfun) || bb == EXIT_BLOCK_PTR_FOR_FN (cfun)) &#123; edge taken_edge = NULL; taken_edge = before_dom_children (bb); /* 在遍历到一个结点时,先调用 before_dom_children 回调 */ worklist[sp++] = bb; /* 将当前节点放到worklist中,作为栈帧标记(NULL为分隔符),后续会先递归处理其子节点 */ worklist[sp++] = NULL; if (taken_edge != STOP) /* 若回调返回的非 STOP(-1) */ &#123; int saved_sp = sp; /* 遍历结点bb在支配树中的所有子节点,支配树信息在walk之前应该已经计算出(calculate_dominance_info),并记录在各个bb-&gt;doms中 */ for (dest = first_dom_son (m_dom_direction, bb); dest; dest = next_dom_son (m_dom_direction, dest)) worklist[sp++] = dest; /* 在栈中记录当前bb的所有要遍历的支配树中的子结点 */ if (sp - saved_sp &gt; 1 &amp;&amp; m_dom_direction == CDI_DOMINATORS &amp;&amp; m_bb_to_rpo) /* 若对支配树的 DFS遍历有顺序要求,则将bb的子节点排序以确保后续的遍历顺序 */ sort_bbs_postorder (&amp;worklist[saved_sp], sp - saved_sp); &#125; &#125; while (sp &gt; 0 &amp;&amp; !worklist[sp - 1]) /* 若当前bb没有子结点,或所有子节点都处理完毕,则为当前结点调用处理完毕后的回调, while循环负责处理多级结点返回 */ &#123; --sp; /* 忽略前面标记的NULL分隔符 */ bb = worklist[--sp]; /* 处理上一层的下一个bb */ ...... after_dom_children (bb); /* 当前结点及其子节点遍历完毕的回调函数 */ &#125; if (sp) bb = worklist[--sp]; /* 递归遍历worklist中下一个结点 */ else break; /* 整个支配树结点的DFS遍历完毕, 退出循环 */ &#125; free (worklist);&#125; ​ 由上可知, mark_def_dom_walker (CDI_DOMINATORS).walk (fun-&gt;cfg-&gt;x_entry_block_ptr); 实际上是通过其父类的walk函数从当前函数的入口bb开始根据DFS算法遍历了此函数的支配树,并对每个bb调用了回调函数mark_def_dom_walker::before_dom_children,此函数定义如下: 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950edge mark_def_dom_walker::before_dom_children (basic_block bb)&#123; gimple_stmt_iterator gsi; bitmap_clear (m_kills); /* 重新清空记录当前bb中kill defs的数组 */ for (gsi = gsi_start_bb (bb); !gsi_end_p (gsi); gsi_next (&amp;gsi)) /* 遍历当前bb中所有gimple语句 */ /* 分析每条语句中的def/use信息,结果更新到stmt-&gt;vuse/vdef/use_ops的同时也 将统计信息更新到全局变量var_infos,interesting_blocks,以及参数m_kills中 */ mark_def_sites (bb, gsi_stmt (gsi), m_kills); return NULL;&#125; static void mark_def_sites (basic_block bb, gimple *stmt, bitmap kills)&#123; tree def; use_operand_p use_p; ssa_op_iter iter; update_stmt (stmt); /* 分析stmt中的 virutal use/def, real use信息,并将其更新到当前语句 stmt-&gt;vuse/vdef/use_ops中 */ ...... set_register_defs (stmt, false); /* 默认标记当前stmt中不存在操作数的def/use操作 */ set_rewrite_uses (stmt, false); ...... FOR_EACH_SSA_USE_OPERAND (use_p, stmt, iter, SSA_OP_ALL_USES) /* 遍历此stmt中的所有uses的操作数 */ &#123; tree sym = USE_FROM_PTR (use_p); /* 获取当前处理的操作数中的变量树节点信息 */ ...... /* 在当前bb的每条语句分析过程中(见下面的循环)如果发现了某变量的def,那么此def对于当前bb来说就是个kill def, 而如果分析到一个变量的use前还没有在当前bb中发现其kill def,则说明此变量的值来自其前驱bb, 那么此时就要 标记此变量在当前bb中出现了传播使用(set_livein_block) */ if (!bitmap_bit_p (kills, DECL_UID (sym))) /* 若此变量在当前bb中未出现kill def */ set_livein_block (sym, bb); /* 在变量的全局var_info-&gt;def_blocks-&gt;livein_blocks中标记,变量sym在基本块bb中为传播使用 */ set_rewrite_uses (stmt, true); /* 标记当前语句stmt中有需要被重写的use操作数 */ &#125; FOR_EACH_SSA_TREE_OPERAND (def, stmt, iter, SSA_OP_ALL_DEFS) /* 动态分析当前stmt的所有defs, 并遍历所有被def的变量 */ &#123; ...... set_def_block (def, bb, false); /* 在当前变量的 var_info-&gt;def_blocks-&gt;def_blocks 中标记在基本块bb中出现了此变量的def */ bitmap_set_bit (kills, DECL_UID (def)); /* 在per bb的kills数组中标记当前bb中出现了变量def的 kill def */ set_register_defs (stmt, true); /* 在当前语句stmt中标记其中出现了某操作数的def操作 */ &#125; /* 当当前分析的bb中任何一条语句(stmt)中出现了操作数的use/def操作,当前bb就要被标记为interesting. 除了特别简单的bb,基本上大部分bb都会标记到interesting_blocks中 */ if (rewrite_uses_p (stmt) || register_defs_p (stmt)) bitmap_set_bit (interesting_blocks, bb-&gt;index);&#125; 为每个变量插入phi函数​ 分析完def/use信息后,则需要为当前函数的每个变量在需要的位置(bb)处插入phi函数,此过程是在函数 insert_phi_nodes中实现的： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118/* 此函数负责为当前函数中所有变量在需要插入phi函数的bb中插入phi函数， phi函数全部记录在每个bb-&gt;gimple_df-&gt;phi_nodes中 需要注意的是此时phi函数的参数尚未填充,只有返回值填充了(其对应变量派生的SSA_NAME)*/static void insert_phi_nodes (bitmap_head *dfs)&#123; var_info *info; ...... /* var_infos[]中记录了当前函数所有变量(local_decls)在整个函数中的def/use信息, 而在def/use分析中简单的排除了一些不需要插入phi函数的变量(need_phi_state = NEED_PHI_STATE_NO的变量, 这样的变量在整个函数中只存在一个def,且其所有use点所在的bb都被def所在的bb支配),而vars数组中最终只记录了那些可能需要插入phi函数的变量做二次分析和处理 */ auto_vec&lt;var_info *&gt; vars (var_infos-&gt;elements ()); FOR_EACH_HASH_TABLE_ELEMENT (*var_infos, info, var_info_p, hi) if (info-&gt;info.need_phi_state != NEED_PHI_STATE_NO) vars.quick_push (info); vars.qsort (insert_phi_nodes_compare_var_infos); /* 根据变量的UID排序 */ FOR_EACH_VEC_ELT (vars, i, info) /* 遍历当前函数中每个可能要插入phi结点的变量(info为迭代器) */ &#123; /* * info-&gt;info.def_blocks.def_blocks 记录info-&gt;var这个变量在各个bb中的def信息(bitmap) * dtf记录整个函数所有bb的支配结点边界信息 此函数根据二者计算出需要为当前变量var插入phi函数的所有bb的信息,返回的idf同样是一个bitmap,若idf[i]-1,则代表在编号为i的bb中需要为变量info-&gt;var插入phi函数 */ bitmap idf = compute_idf (info-&gt;info.def_blocks.def_blocks, dfs); /* 根据idf这个bitmap, 向其对应的bb中为变量info-&gt;var创建一个gphi指令,并将此指令添加到 bb-&gt;gimple_df-&gt;phi_nodes中, ifd是标准算法算出的phi函数插入点,而此函数在创建前还会再次裁剪 idf bitmap(对于变量kill def且不存在livein use的bb,无需插入phi函数) */ insert_phi_nodes_for (info-&gt;var, idf, false); BITMAP_FREE (idf); &#125;&#125; /* 此函数负责根据变量var的def信息(def_blocks)和整个函数所有bb的支配结点边界信息,(dfs) 计算出需要为此函数插入phi函数的bb信息(bitmap phi_insertion_points)并返回此bitmap phi函数的算法是: 1) 当前变量x所有的def点(bb)的支配结点边界中所有结点都需要插入phi函数 2) 插入的phi函数相当于对变量x的隐式def,故插入了phi函数的结点(bb)的支配结点边界中所有结点也都要插入phi函数 3) 针对一个变量x, 任何一个bb中只需要为其插入一个phi函数即可,无需重复插入,因为phi函数参数来自于所有前驱,一个phi函数就代表了所有可能的隐式def;*/bitmap compute_idf (bitmap def_blocks, bitmap_head *dfs)&#123; bitmap_iterator bi; unsigned bb_index, i; bitmap phi_insertion_points; auto_vec&lt;int&gt; work_stack (2 * n_basic_blocks_for_fn (cfun)); /* 此bitmap为函数的返回值, 其是一个一维数组,下标为bb在当前函数的索引号， phi_insertion_points[i] = 1;则代表当前bb中需要为变量x插入phi函数 */ phi_insertion_points = BITMAP_ALLOC (NULL); /* 按照算法, phi函数需要插入到变量 x所有def点所在bb的支配结点边界结点中, 这里首先将所有出现变量x def点的bb加入到队列中 */ EXECUTE_IF_SET_IN_BITMAP (def_blocks, 0, bb_index, bi) work_stack.quick_push (bb_index); while (work_stack.length () &gt; 0) &#123; bb_index = work_stack.pop (); /* 获取下一个存在变量x def的bb的编号 */ /* &amp;dfs[bb_index] 记录索引号为bb_index的bb的支配结点边界信息(下标为bb索引号, dfs[bb_index][i] = 1; 则代表编号为i的bb为编号为 bb_index的bb的支配结点) 此宏展开后会依次比较 dfs[bb_index] 和phi_insertion_points 中的每一个bit,若 dfs[bb_index][i] = 1; 而phi_insertion_points[i] = 0; 则代表发现了 新的需要插入phi函数的bb(编号为i),此时会将i push到work_stack中,下一次循环后会递归遍历此bb; */ EXECUTE_IF_AND_COMPL_IN_BITMAP (&amp;dfs[bb_index], phi_insertion_points, 0, i, bi) &#123; work_stack.quick_push (i); /* 结点i因插入了φ函数,其支配结点边界结点也要加入到 phi_insertion_points中 */ bitmap_set_bit (phi_insertion_points, i); /* 标记结点i中需要插入φ函数 */ &#125; &#125; return phi_insertion_points;&#125; /* 此函数为变量var在需要插入phi函数的bb中插入gphi指令(包括创建,派生var的一个SSA_NAME作为gphi返回值,并将gphi插入到bb-&gt;gimple_df-&gt;phi_nodes指令序列中) 在插入前会尝试删除一些不需要插入phi函数的点(如果变量var在某个bb中不存在liven_use,且存在kill_def,则此bb无需插入phi函数)*/static void insert_phi_nodes_for (tree var, bitmap phi_insertion_points, bool update_p)&#123; unsigned bb_index; edge e; gphi *phi; basic_block bb; bitmap_iterator bi; def_blocks *def_map = find_def_blocks_for (var); /* 此结构体中存着变量var的多个bitmap,包括 kill def/livein use */ ...... /* def_blocks和 livein_blocks都是在 各个bb的mark_def_sites中分析出的: * def_blocks[i] = 1;代表变量var在编号为i的bb中出现了kill def; * livein_blocks[i] = 1; 代表变量var在编号为i的bb中出现了传播使用(也就是第一次访问是use,而不是def) 而在一个bb中,如果一个变量没有出现livein,且存在kill def,那么这个bb中是不需要插入phi函数的,简单说就是:在一个bb的顺序指令执行过程中, 一个变量先被赋值然后才被使用,那么在SSA化时其前驱bb对此变量的def就不必考虑了(因为没有人用), 但如果变量在此bb中没有def则必须插入phi函数, 否则会影响到此bb后续bb中var的定值. 这里就是从phi_insertion_points中去除了livein_use,且有kill def的bb,这些bb无需插入phi函数 */ prune_unused_phi_nodes (phi_insertion_points, def_map-&gt;def_blocks, def_map-&gt;livein_blocks); /* phi_insertion_points 中剩余的结点均需要为变量var插入phi函数,这里遍历其中每个bb */ EXECUTE_IF_SET_IN_BITMAP (phi_insertion_points, 0, bb_index, bi) &#123; bb = BASIC_BLOCK_FOR_FN (cfun, bb_index); /* 根据bb_index获取bb的basic_block结构体 */ if (update_p) mark_block_for_update (bb); if (TREE_CODE (var) == SSA_NAME) &#123; /* SSA_NAME 是在 update_ssa中用到的,先pass */ ...... &#125; else &#123; /* 正常变量是走这里 */ /* 创建一个gphi指令,为变量var生成一个新的SSA_NAME设置为此gphi的返回值结点(所以phi函数的返回值SSA_NAME的version编号都较小), gphi指令的所有参数都暂不填充(因为所有的指令尚未SSA化,无法填充), 并将新生成的gphi指令链接到当前bb-&gt;gimple_df-&gt;phi_nodes指令序列的末尾 */ phi = create_phi_node (var, bb); ...... &#125; set_register_defs (phi, true); /* 标记当前gphi语句中存在def操作(因为其返回值结点被def了) */ mark_phi_for_rewrite (bb, phi); &#125;&#125; 指令重写(SSA化)​ phi函数插入完毕后,需要对整个函数所有bb中的所有指令序列中所有的def/use变量进行重写,此过程是在函数rewrite_blocks 中完成的： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174static void rewrite_blocks (basic_block entry, enum rewrite_mode what)&#123; ...... if (what == REWRITE_ALL) /* 按照DFS遍历当前函数的支配树,并在: * 遍历到某个bb时执行回调 rewrite_dom_walker::before_dom_children * 遍历完成某个bb(及其所有sub bb)后执行回调 rewrite_dom_walker::after_dom_children */ rewrite_dom_walker (CDI_DOMINATORS).walk (entry); /* pass_build_ssa时走这里 */ else if (what == REWRITE_UPDATE) rewrite_update_dom_walker (CDI_DOMINATORS).walk (entry); else gcc_unreachable (); ......&#125; struct def_blocks&#123; bitmap def_blocks; /* 记录当前变量在哪些bb中出现了kill def */ bitmap phi_blocks; bitmap livein_blocks; /* 记录当前变量在哪些bb中出现了livein use */&#125;; struct common_info&#123; ENUM_BITFIELD (need_phi_state) need_phi_state : 2; /* 记录def/use分析中简单的判断结果,即当前变量是否不需要插入phi函数 */ tree current_def; /* 记录此变量在rewrite_stmt中当前的定义(通常是其一个SSA_NAME) */ struct def_blocks def_blocks; /* 记录此变量在所有bb中的kill def/livein use 信息 */&#125;; struct var_info&#123; tree var; /* 此var_info中记录了哪个变量的信息 */ common_info info; /* 记录此变量的def/use,currdef等相关信息 */ &#125;; /* 此函数负责在DFS遍历到支配树中每个结点bb时,将其中的所有stmt SSA化, SSA化的流程分为三步: 1. 将此bb中所有phi函数的返回值设置为其对应变量的currdef 2. 遍历此bb中所有stmt, 将其中所有use的操作数改为其currdef; 若碰到def则为其原始变量var生成一个新的SSA_NAME,将def的操作数和currdef替换为新的SSA_NAME 3. 遍历此bb在函数CFG中所有的后继bb, 填充后继bb中所有phi指令的一个参数(第x个参数,x为此bb在其后继bb的所有前驱结点中的编号)*/edge rewrite_dom_walker::before_dom_children (basic_block bb)&#123; ...... block_defs_stack.safe_push (NULL_TREE); /* 在stack中做一个标记,代表当前开始处理一个新的bb */ /* 对于一个变量x来说,其phi函数的作用就是在SSA化的过程中为phi函数所在bb整合此变量在其各个前驱bb中的定义, 故在SSA化的重写阶段,若一个变量在某个bb中有phi函数,那么phi函数的返回值(x的一个SSA_NAME,见insert_phi_nodes)会作为分析此bb时x的最新定义. 这里是将当前bb的所有phi函数的返回值,设置为其对应变量的最新定义(变量的var_info.currdef) */ for (gphi_iterator gsi = gsi_start_phis (bb); !gsi_end_p (gsi); gsi_next (&amp;gsi)) &#123; /* 获取gphi指令的返回值结点,其必为一个SSA_NAME结点,见phi函数的创建过程 pass_build_ssa =&gt; insert_phi_nodes =&gt; insert_phi_nodes_for =&gt; create_phi_node */ tree result = gimple_phi_result (gsi_stmt (gsi)); /* 获取此phi函数返回值结点 */ /* 将返回值结点设置为其原始变量的currdef, 如: 若当result 为SSA_NAME x_1, 则 SSA_NAME_VAR (result)为变量x,这里设置 x的var_info.curredef = (tree)x_1; */ register_new_def (result, SSA_NAME_VAR (result)); &#125; if (bitmap_bit_p (interesting_blocks, bb-&gt;index)) /* 若当前bb在前面def/use分析中发现其中至少存在一个def/use */ /* 遍历当前bb中的每一条语句,并修改每条语句中需要被重写的操作数，注意这里遍历的是bb-&gt;gimple_df-&gt;seqs, 而不是phi_nodes */ for (gimple_stmt_iterator gsi = gsi_start_bb (bb); !gsi_end_p (gsi); gsi_next (&amp;gsi)) rewrite_stmt (&amp;gsi); /* 此函数负责对一条指令(stmt)进行重写 */ /* 当前bb重写完毕(SSA化完毕),则所有的变量都已经变为其SSA形式了,此时需要为其所有后继结点填充所有phi函数的第i个参数, i为此bb在其后继的前驱结点中的编号 */ rewrite_add_phi_arguments (bb); return NULL;&#125; /* 此函数负责重写si-&gt;ptr这条语句(stmt)中的所有def/use,将此stmt SSA化, 其处理顺序是先处理use后处理def,这也符合正常一条指令执行的逻辑,stmt中: * 发现所有对原始变量var的use的操作数指针,都会被改为(指向)对其currdef这个SSA_NAME结点 * 发现所有对原始变量var的def,都会先为变量var新生成一个SSA_NAME,并将def操作数的指针改为(指向)新SSA_NAME结点,并将新的SSA_NAME更新到变量var的currdef*/static void rewrite_stmt (gimple_stmt_iterator *si)&#123; gimple *stmt = gsi_stmt (*si); /* 若当前语句中没有需要被重写的use/def,则直接返回 */ if (!rewrite_uses_p (stmt) &amp;&amp; !register_defs_p (stmt)) return; if (rewrite_uses_p (stmt)) /* 1.若当前语句中存在需要被重写的use operands,则先重写uses所有的uses操作数 */ &#123; ...... /* 遍历当前stmt的real use链表(stmt-&gt;use_ops),其中记录了当前stmt SSA化时需要被修改的USE操作数, 并将其修改为对应变量的当前SSA_NAME定义 如 y = x; 若此指令解析前x的当前定义为x_1,则这里将指令修改为 y = x_1; */ FOR_EACH_SSA_USE_OPERAND (use_p, stmt, iter, SSA_OP_ALL_USES) &#123; /* use_p-&gt;use 指向此stmt的一个操作数的内存,这里先获取其原始操作数var(即 var = *use_p-&gt;use) */ tree var = USE_FROM_PTR (use_p); ...... /* 修改此操作数为变量var的当前定义(currdef), 即 *use_p-&gt;use = var_1; (若currdef为var_1) */ SET_USE (use_p, get_reaching_def (var)); &#125; &#125; if (register_defs_p (stmt)) /* 2. 若当前语句中存在需要被重写的def operands(见mark_def_sites),则重写所有的defs操作数 */ FOR_EACH_SSA_DEF_OPERAND (def_p, stmt, iter, SSA_OP_ALL_DEFS) &#123; tree var = DEF_FROM_PTR (def_p); /* 从stmt中的def操作数中读取当前被def的变量的树节点 */ tree name; ...... if (TREE_CODE (var) == SSA_NAME) continue; /* 忽略SSA_NAME */ gcc_checking_assert (DECL_P (var)); ...... name = make_ssa_name (var, stmt); /* 为变量var新的def点重新分配一个var的SSA_NAME */ SET_DEF (def_p, name); /* 将stmt某操作数中的变量var修改为其SSA_NAME,将定义转化为静态单赋值形式 */ /* 在支配树的DFS遍历中, 变量var的当前定义此时已经变为了新生成的SSA_NAME name, 将其更新到变量var的currdef中,后续再出现的use则使用此新的def, 需要注意的是这里,这里会在全局block_defs_stack中保存所有旧的def,在rewrite_dom_walker::after_dom_children会恢复已有修改, 因为 在支配树的DFS遍历中若有 A=&gt;B, A=&gt;C; 那么分析B时产生的def只作用于B以及其所有sub bb, 而当回到A重新遍历C时要重新使用A中的def继续分析, 所以这里会有一个入栈操作.after_dom_children中完成出栈操作 */ register_new_def (DEF_FROM_PTR (def_p), var); ...... &#125;&#125; /* 在一个bb及其所有sub bb处理完成后,要恢复此bb中的所有currdef,恢复后才可以继续遍历此bb的兄弟bb,此函数则完成currdef的恢复操作 */void rewrite_dom_walker::after_dom_children (basic_block bb ATTRIBUTE_UNUSED)&#123; while (block_defs_stack.length () &gt; 0) /* 遍历def栈中的所有内容, 其插入点是上面的register_new_def函数 */ &#123; tree tmp = block_defs_stack.pop (); /* 获取stack中一个要恢复的SSA_NAME */ tree saved_def, var; if (tmp == NULL_TREE) break; /* 遍历当NULL_TREE代表当前bb之前的所有入栈处理完毕,NULL_TREE在栈中作为分隔符分隔各个bb的栈信息 */ if (TREE_CODE (tmp) == SSA_NAME) &#123; /* 大多数情况下之前push进来的结点为某个变量的SSA_NAME结点 */ saved_def = tmp; /* 当前需要恢复的SSA_NAME */ var = SSA_NAME_VAR (saved_def); /* 获取SSA_NAME的原始变量 */ ...... &#125; else &#123; /* 非SSA_NAME则说明push到原始变量了,说明在当前bb的父节点之前没有变量var的def出现 */ saved_def = NULL; /* 没有def出现时标记saved_def为空 */ var = tmp; /* tmo即为原始变量 */ &#125; /* var为原始变量,先找到其var_info结构体,然后恢复其之前定义为 saved_def */ get_common_info (var)-&gt;current_def = saved_def; &#125;&#125; /* 此函数负责遍历当前bb在函数CFG中的所有后继bb, 遍历每个后继bb中的每一条phi指令,其目的是为了填充所有bb中所有phi函数的一个参数, 后继bb中所有phi函数的参数都来自其前驱bb,在其前驱bb SSA化后要立即为其所有后续bb添加SSA化后的参数*/static void rewrite_add_phi_arguments (basic_block bb)&#123; edge e; edge_iterator ei; FOR_EACH_EDGE (e, ei, bb-&gt;succs) /* 遍历当前bb的所有直接后继bb,这里遍历的是边 */ &#123; gphi *phi; gphi_iterator gsi; for (gsi = gsi_start_phis (e-&gt;dest); !gsi_end_p (gsi); gsi_next (&amp;gsi)) /* 遍历每个后继bb中的所有phi语句 */ &#123; phi = gsi.phi (); res = gimple_phi_result (phi); /* 获取phi函数的返回值节点, 也就是此gphi语句中最终def的那个 SSA_NAME */ /* SSA_NAME_VAR (res))获取ssa name 对应的var节点, 而get_reaching_def 则获取var变量当前最新的def是哪个变量, 如res可能是var_1 = φ(,...) 中的var_1(SSA_NAME),而返回的currdef是var的最新定义(可能是var_3,也是个SSA_NAME) */ currdef = get_reaching_def (SSA_NAME_VAR (res)); ...... /* 将currdef记录到 phi函数的第x个参数中(phi.arg[x].def),并增加currdef这个SSA_NAME的use链表(将phi.arg[x].imm_use 链接到currdef.imm_use使用链中),这里的x是 e这条边在e-&gt;dest结点中的前驱边编号(当前bb在其某后继结点中的前驱结点编号) */ add_phi_arg (phi, currdef, e, loc); &#125; &#125;&#125; SSA基本概念及原理参考文章 程序分析之中间表示 gcc中的支配树 原文链接","categories":[{"name":"SSA","slug":"SSA","permalink":"https://lwy0518.github.io/categories/SSA/"}],"tags":[{"name":"SSA","slug":"SSA","permalink":"https://lwy0518.github.io/tags/SSA/"},{"name":"gcc","slug":"gcc","permalink":"https://lwy0518.github.io/tags/gcc/"},{"name":"算法","slug":"算法","permalink":"https://lwy0518.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"编译原理","slug":"编译原理","permalink":"https://lwy0518.github.io/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"}],"author":"lwy"},{"title":"Hexo NexT 主题更新","slug":"Hexo-NexT-主题更新","date":"2021-12-22T02:32:43.000Z","updated":"2021-12-22T02:59:01.322Z","comments":true,"path":"2021/12/22/Hexo-NexT-主题更新/","link":"","permalink":"https://lwy0518.github.io/2021/12/22/Hexo-NexT-%E4%B8%BB%E9%A2%98%E6%9B%B4%E6%96%B0/","excerpt":"更新Next主题12$ rm -rf ./themes/next$ cnpm install hexo-theme-next 安装第三方插件模块安装1$ cnpm install @next-theme/plugins 配置1$ vim _config.next.yml 复制以下代码到_config.next.yml 12vendors: plugins: local","text":"更新Next主题12$ rm -rf ./themes/next$ cnpm install hexo-theme-next 安装第三方插件模块安装1$ cnpm install @next-theme/plugins 配置1$ vim _config.next.yml 复制以下代码到_config.next.yml 12vendors: plugins: local","categories":[{"name":"hexo","slug":"hexo","permalink":"https://lwy0518.github.io/categories/hexo/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://lwy0518.github.io/tags/hexo/"},{"name":"Blog","slug":"Blog","permalink":"https://lwy0518.github.io/tags/Blog/"}],"author":"lwy"},{"title":"Typora快捷键","slug":"Typora快捷键","date":"2021-12-21T07:46:50.000Z","updated":"2021-12-22T03:24:17.737Z","comments":true,"path":"2021/12/21/Typora快捷键/","link":"","permalink":"https://lwy0518.github.io/2021/12/21/Typora%E5%BF%AB%E6%8D%B7%E9%94%AE/","excerpt":"Typora快捷键","text":"Typora快捷键 原文链接 源码模式1ctrl+/ 生成目录1[TOC]按回车 代码块1ctrl+alt+f 代码1ctrl+shift+` 标题1ctrl+数字 表格1ctrl+t 选中一行1ctrl+l 选中单词1ctrl+d 选中相同格式的文字1ctrl+e 跳转到文章开头1ctrl+home 跳转到文章结尾1ctrl+end 搜索1ctrl+f 替换1ctrl+h 下划线1ctrl+u 删除线1alt+shift+5 插入图片1直接拖动到指定位置即可或者ctrl+shift+i 插入链接1ctrl + k 引用1&gt; 加粗1ctrl + b 倾斜1ctrl + i 添加参考文献1[^1] # ^ 后面添加数字","categories":[{"name":"Markdown","slug":"Markdown","permalink":"https://lwy0518.github.io/categories/Markdown/"}],"tags":[{"name":"Typora","slug":"Typora","permalink":"https://lwy0518.github.io/tags/Typora/"},{"name":"Markdown","slug":"Markdown","permalink":"https://lwy0518.github.io/tags/Markdown/"}],"author":"lwy"},{"title":"污点分析","slug":"污点分析","date":"2021-12-21T07:22:46.029Z","updated":"2021-12-22T02:58:43.530Z","comments":true,"path":"2021/12/21/污点分析/","link":"","permalink":"https://lwy0518.github.io/2021/12/21/%E6%B1%A1%E7%82%B9%E5%88%86%E6%9E%90/","excerpt":"污点分析技术 基本原理 污点分析定义 识别污点源和汇聚点 污点传播分析 显示流分析 隐式流分析 无害处理 污点传播分析的关键技术 污点传播中的显示流分析 静态分析技术 动态分析技术 硬件 基于软件 混合型 污点传播中的隐式流分析 静态分析技术 动态分析技术","text":"污点分析技术 基本原理 污点分析定义 识别污点源和汇聚点 污点传播分析 显示流分析 隐式流分析 无害处理 污点传播分析的关键技术 污点传播中的显示流分析 静态分析技术 动态分析技术 硬件 基于软件 混合型 污点传播中的隐式流分析 静态分析技术 动态分析技术 污点分析方法实现实例分析 静态污点分析技术 基于数据流的污点分析 基于依赖关系的污点分析 静态污点分析实例分析 动态污点分析技术 动态污点分析的方法实现 污点数据标记 污点动态跟踪 污点误用检查 实例分析 污点分析在实际应用中的关键技术 检测智能手机隐私泄露 静态污点分析 组件内污点传播分析 组件间污点传播分析 组件与库函数之间的污点传播分析 动态污点分析 组件内污点传播分析 组件间污点传播分析 组件与本地库函数间的污点传播 总结 参考文献 污点分析技术基本原理污点分析定义污点分析可以抽象成一个**三元组&lt;sources,sinks,sanitizers&gt;**的形式，其中，source 即污点源，代表直接引入不受信任的数据或者机密数据到系统中；sink 即污点汇聚点，代表直接产生安全敏感操作(违反数据完整性)或者泄露隐私数据到外界(违反数据保密性)；sanitizer 即无害处理，代表通过数据加密或者移除危害操作等手段使数据传播不再对软件系统的信息安全产生危害。 污点分析就是分析程序中由污点源引入的数据是否能够不经无害处理，而直接传播到污点汇聚点。如果不能，说明系统是信息流安全的；否则，说明系统产生了隐私数据泄露或危险数据操作等安全问题。 在漏洞分析中，使用污点分析技术将所感兴趣的数据(通常来自程序的外部输入，假定所有输入都是危险的)标记为污点数据，然后通过跟踪和污点数据相关的信息的流向，可以知道它们是否会影响某些关键的程序操作，进而挖掘程序漏洞。即将程序是否存在某种漏洞的问题转化为污点信息是否会被 Sink 点上的操作所使用的问题。 污点分析常常包括以下几个部分(如图 2 所示)： 识别污点信息在程序中的产生点（Source点）并对污点信息进行标记(根据所分析的系统的不同使用定制的识别策略) 污点传播分析(利用特定的规则跟踪分析污点信息在程序中的传播过程) 漏洞检测、无害处理(在一些关键的程序点（Sink点）检测关键的操作是否会受到污点信息的影响) 识别污点源和汇聚点识别污点源和污点汇聚点是污点分析的前提。目前，在不同的应用程序中识别污点源和汇聚点的方法各不相同。缺乏通用方法的原因一方面来自系统模型、编程语言之间的差异。另一方面，污点分析关注的安全漏洞类型不同，也会导致对污点源和污点汇聚点的收集方法迥异。表 1 所示为在 Web 应用程序漏洞检测中的污点源示例[^1]，它们是 Web 框架中关键对象的属性。 现有的识别污点源和汇聚点的方法可以大致分成 3 类: 使用启发式的策略进行标记,例如把来自程序外部输入的数据统称为“污点”数据,保守地认为这些数据有可能包含恶意的攻击数据(如 PHP Aspis)； 根据具体应用程序调用的 API 或者重要的数据类型,手工标记源和汇聚点(如 DroidSafe[^2])； 使用统计或机器学习技术自动地识别和标记污点源及汇聚点[^3]。 污点传播分析污点传播分析就是分析污点标记数据在程序中的传播途径.按照分析过程中关注的程序依赖关系的不同, 可以将污点传播分析分为显式流分析和隐式流分析。 显示流分析污点传播分析中的显式流分析就是分析污点标记如何随程序中变量之间的数据依赖关系传播 以图 3 所 示的程序为例,变量 a 和 b 被预定义的污点源函数 source 标记为污点源.假设 a 和 b 被赋予的污点标记分别为taint_a 和 taint_b.由于第 5 行的变量 x 直接数据依赖于变量 a,第 6 行的变量 y 直接数据依赖于变量 b,显式流分析会分别将污点标记 taint_a 和 taint_b 传播给第 5 行的变量 x 和第 6 行的变量 y.又由于 x 和 y 分别可以到达第 7 行和第 8 行的污点汇聚点(用预定义的污点汇聚点函数 sink 标识),图 3 所示的代码存在信息泄漏的问题.我们将在后面具体介绍目前污点传播分析中显式流分析面临的主要挑战和解决方法。 隐式流分析污点传播分析中的隐式流分析是分析污点标记如何随程序中变量之间的**控制依赖关系传播,**也就是分析污点标记如何从条件指令传播到其所控制的语句。 在图 4 所示的程序中,变量 X 是被污点标记的字符串类型变量,变量 Y 和变量 X 之间并没有直接或间接的数据依赖关系(显式流关系),但 X 上的污点标记可以经过控制依赖隐式地传播到 Y。 具体来说,由第 4 行的循环条件控制的外层循环顺序地取出 X 中的每一个字符,转化成整型后赋给变量 x,再由第 7 行的循环条件控制的内层循环以累加的方式将 x 的值赋给 y,最后由外层循环将 y 逐一传给 Y.最终,第 12 行的 Y 值和 X 值相同,程序存在信息泄漏问题.但是,如果不进行隐式流污点传播分析,第 12 行 的变量 Y 将不会被赋予污点标记,程序的信息泄漏问题被掩盖. 隐式流污点传播一直以来都是一个重要的问题,和显式流一样,如果不被正确处理,会使污点分析的结果不精确.由于对隐式流污点传播处理不当导致本应被标记的变量没有被标记的问题称为欠污染(under-taint)问题.相反地,由于污点标记的数量过多而导致污点变量大量扩散的问题称为过污染(over-taint)问题.目前,针对隐式流问题的研究重点是尽量减少欠污染和过污染的情况.我们将在后面具体介绍现有技术是如何解决上述问题的。 无害处理污点数据在传播的过程中可能会经过无害处理模块,无害处理模块是指污点数据经过该模块的处理后,数据本身不再携带敏感信息或者针对该数据的操作不会再对系统产生危害.换言之,带污点标记的数据在经过无害处理模块后,污点标记可以被移除.正确地使用无害处理可以降低系统中污点标记的数量,提高污点分析的效率,并且避免由于污点扩散导致的分析结果不精确的问题 在应用过程中,为了防止敏感数据被泄露(保护保密性),通常会对敏感数据进行加密处理.此时,加密库函数应该被识别成无害处理模块.这一方面是由于库函数中使用了大量的加密算法,导致攻击者很难有效地计算出密码的可能范围;另一方面是加密后的数据不再具有威胁性,继续传播污点标记没有意义。 此外,为了防止外界数据因为携带危险操作而对系统关键区域产生危害(保护完整性),通常会对输入的数据进行验证。 综上,目前对污点源、污点汇聚点以及无害处理模块的识别通常根据系统或漏洞类型使用定制的方法.由于这些方法都比较直接,本文将不再进行更深入的探讨.下一节将重点介绍污点传播中的关键技术。 总结，使用污点分析检测程序漏洞的工作原理如下图所示： 基于数据流的污点分析。在不考虑隐式信息流的情况下，可以将污点分析看做针对污点数据的数据流分析。根据污点传播规则跟踪污点信息或者标记路径上的变量污染情况，进而检查污点信息是否影响敏感操作。 基于依赖关系的污点分析。考虑隐式信息流，在分析过程中，根据程序中的语句或者指令之间的依赖关系，检查 Sink 点处敏感操作是否依赖于 Source 点处接收污点信息的操作。 污点传播分析的关键技术污点传播分析是当前污点分析领域的研究重点.与程序分析技术相结合,可以获得更加高效、精确的污点分析结果.根据分析过程中是否需要运行程序,可以将污点传播分析分为静态污点分析和动态污点分析.本节主要介绍如何使用动/静态程序分析技术来解决污点传播中的显式流分析和隐式流分析问题 显式流分析和隐式流分析是从两种不同的角度(数据流和控制流)来观察污点传播 污点传播中的显式流分析静态分析技术静态污点传播分析(简称静态污点分析)是指在不运行且不修改代码的前提下,通过分析程序变量间的数据依赖关系来检测数据能否从污点源传播到污点汇聚点. 静态污点分析的对象一般是程序的源码或中间表示.可以将对污点传播中显式流的静态分析问题转化为对程序中静态数据依赖的分析: 首先,根据程序中的函数调用关系构建调用图(call graph,简称CG); 然后,在函数内或者函数间根据不同的程序特性进行具体的数据流传播分析.常见的显式流污点传播方式包括直接赋值传播、通过函数(过程)调用传播以及通过别名(指针)传播 以图 5 所示的 Java 程序为例: 第 3 行的变量 b 为初始的污点标记变量,程序第 4 行将一个包含变量 b 的算术表达式的计算结果直接赋给变量 c.由于变量 c 和变量 b 之间具有直接的赋值关系,污点标记可直接从赋值语句右部的变量传播到左部,也就是上述 3种显式流污点传播方式中的直接赋值传播 接下来,变量 c 被作为实参传递给程序第 5 行的函数 foo,c 上的污点标记也通过函数调用传播到 foo 的形参 z,z 的污点标记又通过直接赋值传播到程序第 8 行的 x.f.由于 foo 的另外两个参数对象 x 和 y 都是对对象 a 的引用,二者之间存在别名,因此,x.f的污点标记可以通过别名传播到第 9 行的污点汇聚点,程序存在泄漏问题. 目前,利用数据流分析解决显式污点传播分析中的直接赋值传播和函数调用传播已经相当成熟,研究的重点是如何为别名传播的分析提供更精确、高效的解决方案.由于精确度越高(上下文敏感、流敏感、域敏感、对象敏感等)的程序静态分析技术往往伴随着越大的时空开销,追求全敏感且高效的别名分析难度较大.又由于静态污点传播分析关注的是从污点源到污点汇聚点之间的数据流关系,分析对象并非完整的程序,而是确定的入口和出口之间的程序片段.这就意味着可以尝试采用按需(on-demand)定制的别名分析方法来解决显式流态污点分析中的别名传播问题 动态分析技术动态污点传播分析(简称动态污点分析)是指在程序运行过程中,通过实时监控程序的污点数据在系统程序中的传播来检测数据能否从污点源传播到污点汇聚点.动态污点传播分析首先需要为污点数据扩展一个污点标记(tainted tag)的标签并将其存储在存储单元(内存、寄存器、缓存等)中,然后根据指令类型和指令操作数设计相应的传播逻辑传播污点标记 动态污点传播分析按照实现层次被分为基于硬件、基于软件以及混合型的污点传播分析这3类 1.硬件基于硬件的污点传播分析需要定制的硬件支持,一般需要在原有体系结构上为寄存器或者内存扩展一个标记位,用来存储污点标记 2.基于软件基于软件的污点传播分析通过修改程序的二进制代码来进行污点标记位的存储与传播 基于软件的污点传播的优点在于不必更改处理器等底层的硬件,并且可以支持更高的语义逻辑的安全策略(利用其更贴近源程序层次的特点),但缺点是使用插桩(instrumentation 在保证被测程序原有逻辑完整性的基础上在程序中插入一些探针)或代码重写(code rewriting)修改程序往往会给分析系统带来巨大的开销.相反地,基于硬件的污点传播分析虽然可以利用定制硬件降低开销,但通常不能支持更高的语义逻辑的安全策略,并且需要对处理器结构进行重新设计 3.混合型混合型的污点分析是对上述两类方法的折中,即,通过尽可能少的硬件结构改动以保证更高的语义逻辑的安全策略 目前,针对动态污点传播分析的研究工作关注的首要问题是如何设计有效的污点传播逻辑,以确保精确的污点传播分析 污点传播中的隐式流分析污点传播分析中的隐式流分析就是分析污点数据如何通过控制依赖进行传播,如果忽略了对隐式流污点传播的分析,则会导致欠污染的情况;如果对隐式流分析不当,那么除了欠污染之外,还可能出现过污染的情况.与显式流分析类似,隐式流分析技术同样也可以分为静态分析和动态分析两类 静态分析技术静态隐式流分析面临的核心问题是精度与效率不可兼得的问题.精确的隐式流污点传播分析需要分析每一个分支控制条件是否需要传播污点标记.路径敏感的数据流分析往往会产生路径爆炸问题,导致开销难以接受.为了降低开销,一种简单的静态传播(标记)分支语句的污点标记方法是将控制依赖于它的语句全部进行污点标记,但该方法会导致一些并不携带隐私数据的变量被标记,导致过污染情况的发生.过污染会引起污点的大量扩散,最终导致用户得到的报告中信息过多,难以使用 动态分析技术动态隐式流分析关注的首要问题是如何确定污点控制条件下需要标记的语句的范围.由于动态执行轨迹并不能反映出被执行的指令之间的控制依赖关系,目前的研究多采用离线的静态分析辅助判断动态污点传播中的隐式流标记范围.Clause等人提出,利用离线静态分析得到的控制流图节点间的后支配(post-dominate)关系来解决动态污点传播中的隐式流标记问题 例如,如图 6(a)所示,程序第 3 行的分支语句被标记为污点源,当document.cookie 的值为 abc 时,会发生污点数据泄露.根据基于后支配关系的标记算法,会对该示例第 4 行语句的指令目的地,即 x 的值进行污点标记.(ps:因为根据该分支控制下的语句的执行结果可以判定污染源document.cookie的值,造成污点数据泄露) 动态分析面临的第 2 个问题是由于部分泄漏(partially leaked)导致的漏报.部分泄漏是指污点信息通过动态未执行部分进行传播并泄漏.Vogt等人发现,只动态地标记分支条件下的语句会发生这种情况 仍以图 6(a)中的程序为例:当第 3 行的控制条件被执行时,对应的 x 会被标记.此时,x 的值为 true,而 y 值没有变化,仍然为 false.在后续执行过程中,由于第 9行的污点汇聚点不可达,而第 12 行的汇聚点可达,动态分析没有检测到污点数据泄漏.但攻击者由第 11 行 y 等于 false 的条件能够反推出程序执行了第 3 行的分支条件,程序实际上存在信息泄漏的问题.这个信息泄露是由第 6 行未被执行到的 y 的赋值语句所触发的.因此,y 应该被动态污点传播分析所标记.为了解决部分泄漏问题,Vogt等人在传统的动态污点分析基础上增加了离线的静态分析,以跟踪动态执行过程中的控制依赖关系,对污点分支控制范围内的所有赋值语句中的变量都进行标记.具体到图 6(a)所示的例子,就是第 4 行和第 6 行中的变量均会被污点标记.但是,Vogt 等人的方法仍然会产生过污染的情况. 动态分析需要解决的第 3 个问题是如何选择合适的污点标记分支进行污点传播.鉴于单纯地将所有包含污点标记的分支进行传播会导致过污染的情况,可以根据信息泄漏范围的不同,定量地设计污点标记分支的选择策略. 以图 6(b)所示的程序为例,第 2 行的变量 a 为初始的污点标记变量.第 5 行、第 7 行、第 9 行均为以 a作为源操作数的污点标记的分支.如果传播策略为只要分支指令中包含污点标记就对其进行传播,那么第 5 行、第 7 行、第 9 行将分别被传播给第 6 行、第 8 行、第 10 行,并最终传播到第 12 行的污点汇聚点.如果对这段程序进行深入分析会发现,3个分支条件所提供的信息值(所能泄露的信息范围)并不相同,分别是 a 等于 10、a大于 10 且小于或等于 13(将 w 值代入计算)以及 a 小于 10.对于 a 等于 10 的情况,攻击者可以根据第 12 行泄漏的 x 的值直接还原出污点源处 a 的值(这类分支也被称为能够保存完整信息的分支);对于 a 大于 10 且小于或等于 13 的情况,攻击者也只需要尝试 3 次就可以还原信息;而对于 a 小于 10 的情况,攻击者所获得的不确定性较大,成功还原信息的几率显著低于前两种,对该分支进行污点传播的实际意义不大. Bao等人只将严格控制依赖(strict control dependence)识别成需要污点传播的分支,其中,严格控制依赖即分支条件表达式的两端具有常数差异的分支.但是,Bao 的方法只适用于能够在编译阶段计算出常数差异的分支. 污点分析方法实现静态污点分析技术静态污点分析系统首先对程序代码进行解析，获得程序代码的中间表示，然后在中间表示的基础上对程序代码进行控制流分析等辅助分析，以获得需要的控制流图、调用图等。在辅助分析的过程中，系统可以利用污点分析规则在中间表示上识别程序中的 Source 点和 Sink 点。最后检测系统根据污点分析规则，利用静态污点分析检查程序是否存在污点类型的漏洞。 基于数据流的污点分析在基于数据流的污点分析中，常常需要一些辅助分析技术，例如别名分析、取值分析等，来提高分析精度。辅助分析和污点分析交替进行，通常沿着程序路径的方向分析污点信息的流向，检查 Source 点处程序接收的污点信息是否会影响到 Sink 点处的敏感操作。 过程内的分析中，按照一定的顺序分析过程内的每一条语句或者指令，进而分析污点信息的流向。 记录污点信息。在静态分析层面，程序变量的污染情况为主要关注对象。为记录污染信息，通常为变量添加一个污染标签。最简单的就是一个布尔型变量，表示变量是否被污染。更复杂的标签还可以记录变量的污染信息来自哪些 Source 点，甚至精确到 Source 点接收数据的哪一部分。当然也可以不使用污染标签，这时我们通过对变量进行跟踪的方式达到分析污点信息流向的目的。例如使用栈或者队列来记录被污染的变量。 程序语句的分析。在确定如何记录污染信息后，将对程序语句进行静态分析。通常我们主要关注赋值语句、控制转移语句以及过程调用语句三类。 赋值语句。 对于简单的赋值语句，形如 a = b 这样的，记录语句左端的变量和右端的变量具有相同的污染状态。程序中的常量通常认为是未污染的，如果一个变量被赋值为常量，在不考虑隐式信息流的情况下，认为变量的状态在赋值后是未污染的。 对于形如 a = b + c 这样带有二元操作的赋值语句，通常规定如果右端的操作数只要有一个是被污染的，则左端的变量是污染的（除非右端计算结果为常量）。 对于和数组元素相关的赋值，如果可以通过静态分析确定数组下标的取值或者取值范围，那么就可以精确地判断数组中哪个或哪些元素是污染的。但通常静态分析不能确定一个变量是污染的，那么就简单地认为整个数组都是污染的。 对于包含字段或者包含指针操作的赋值语句，常常需要用到指向分析的分析结果。 控制转移语句。 在分析条件控制转移语句时，首先考虑语句中的路径条件可能是包含对污点数据的限制，在实际分析中常常需要识别这种限制污点数据的条件，以判断这些限制条件是否足够包含程序不会受到攻击。如果得出路径条件的限制是足够的，那么可以将相应的变量标记为未污染的。 对于循环语句，通常规定循环变量的取值范围不能受到输入的影响。例如在语句 for (i = 1； i &lt; k； i++)&#123;&#125; 中，可以规定循环的上界 k 不能是污染的。 过程调用语句。 可以使用过程间的分析或者直接应用过程摘要进行分析。污点分析所使用的过程摘要主要描述怎样改变与该过程相关的变量的污染状态，以及对哪些变量的污染状态进行检测。这些变量可以是过程使用的参数、参数的字段或者过程的返回值等。例如在语句 flag = obj。method(str)； 中，str 是污染的，那么通过过程间的分析，将变量 obj 的字段 str 标记为污染的，而记录方法的返回值的变量 flag 标记为未污染的。 在实际的过程间分析中，可以对已经分析过的过程构建过程摘要。例如前面的语句，其过程摘要描述为：方法 method 的参数污染状态决定其接收对象的实例域 str 的污染状态，并且它的返回值是未受污染的。那么下一次分析需要时，就可以直接应用摘要进行分析。 代码的遍历。一般情况下，常常使用流敏感的方式或者路径敏感的方式进行遍历，并分析过程中的代码。如果使用流敏感的方式，可以通过对不同路径上的分析结果进行汇集，以发现程序中的数据净化规则。如果使用路径敏感的分析方式，则需要关注路径条件，如果路径条件中涉及对污染变量取值的限制，可认为路径条件对污染数据进行了净化，还可以将分析路径条件对污染数据的限制进行记录，如果在一条程序路径上，这些限制足够保证数据不会被攻击者利用，就可以将相应的变量标记为未污染的。 过程间的分析与数据流过程间分析类似，使用自底向上的分析方法，分析调用图中的每一个过程，进而对程序进行整体的分析。 基于依赖关系的污点分析在基于依赖关系的污点分析中，首先利用程序的中间表示、控制流图和过程调用图构造程序完整的或者局部的程序的依赖关系。在分析程序依赖关系后，根据污点分析规则，检测 Sink 点处敏感操作是否依赖于 Source 点。 分析程序依赖关系的过程可以看做是构建程序依赖图的过程。程序依赖图是一个有向图。它的节点是程序语句，它的有向边表示程序语句之间的依赖关系。程序依赖图的有向边常常包括数据依赖边和控制依赖边。在构建有一定规模的程序的依赖图时，需要按需地构建程序依赖关系，并且优先考虑和污点信息相关的程序代码。 静态污点分析实例分析在使用污点分析方法检测程序漏洞时，污点数据相关的程序漏洞是主要关注对象，如 SQL 注入漏洞、命令注入漏洞和跨站脚本漏洞等。 下面是一个存在 SQL 注入漏洞 ASP 程序的例子： 1234567891011121314&lt;% Set pwd = &quot;bar&quot; Set sql1 = &quot;SELECT companyname FROM &quot; &amp; Request。Cookies(&quot;hello&quot;) Set sql2 = Request。QueryString(&quot;foo&quot;) MySqlStuff pwd， sql1， sql2 Sub MySqlStuff(password， cmd1， cmd2) Set conn = Server。CreateObject(&quot;ADODB。Connection&quot;) conn。Provider = &quot;Microsoft。Jet。OLEDB。4。0&quot; conn。Open &quot;c：/webdata/foo。mdb&quot;， &quot;foo&quot;， password Set rs = conn。Execute(cmd2) Set rs = Server。CreateObject(&quot;ADODB。recordset&quot;) rs。Open cmd1， conn End Sub%&gt; 首先对这段代码表示为一种三地址码的形式，例如第 3 行可以表示为： 1234567a = &quot;SELECT companyname FROM &quot;b = &quot;hello&quot;param0 Requestparam1 bcallCookiesreturn csql1 = a &amp; c 解析完毕后，需要对程序代码进行控制流分析，这里只包含了一个调用关系（第 5 行）。 接下来，需要识别程序中的 Source 点和 Sink 点以及初始的被污染的数据。 具体的分析过程如下： 调用 Request。Cookies(“hello”) 的返回结果是污染的，所以变量 sql1 也是污染的。 调用 Request。QueryString(“foo”) 的返回结果 sql2 是污染的。 函数 MySqlStuff 被调用，它的参数 sql1，sql2 都是污染的。分了分析函数的处理过程，根据第 6 行函数的声明，标记其参数 cmd1，cmd2 是污染的。 第 10 行是程序的 Sink 点，函数 conn。Execute 执行 SQL 操作，其参数 cmd2 是污染的，进而发现污染数据从 Source 点传播到 Sink 点。因此，认为程序存在 SQL 注入漏洞 动态污点分析技术动态污点分析是在程序运行的基础上，对数据流或控制流进行监控，从而实现对数据在内存中的显式传播、数据误用等进行跟踪和检测。动态污点分析与静态污点分析的唯一区别在于静态污点分析技术在检测时并不真正运行程序，而是通过模拟程序的执行过程来传播污点标记，而动态污点分析技术需要运行程序，同时实时传播并检测污点标记。 动态污点分析技术可分为三个部分： 污点数据标记：程序攻击面是程序接受输入数据的接口集，一般由程序入口点和外部函数调用组成。在污点分析中，来自外部的输入数据会被标记为污点数据。根据输入数据来源的不同，可分为三类：网络输入、文件输入和输入设备输入。 污点动态跟踪：在污点数据标记的基础上，对进程进行指令粒度的动态跟踪分析，分析每一条指令的效果，直至覆盖整个程序的运行过程，跟踪数据流的传播。 动态污点跟踪通常基于以下三种机制 动态代码插桩：可以跟踪单个进程的污点数据流动，通过在被分析程序中插入分析代码，跟踪污点信息流在进程中的流动方向。 全系统模拟：利用全系统模拟技术，分析模拟系统中每条指令的污点信息扩散路径，可以跟踪污点数据在操作系统内的流动。 虚拟机监视器：通过在虚拟机监视器中增加分析污点信息流的功能，跟踪污点数据在整个客户机中各个虚拟机之间的流动。 污点动态跟踪通常需要影子内存（shadow memory）来映射实际内存的污染情况，从而记录内存区域和寄存器是否是被污染的。对每条语句进行分析的过程中，污点跟踪攻击根据影子内存判断是否存在污点信息的传播，从而对污点信息进行传播并将传播结果保存于影子内存中，进而追踪污点数据的流向。 一般情况下，数据移动类和算数类指令都将造成显示的信息流传播。为了跟踪污点数据的显示传播，需要在每个数据移动指令和算数指令执行前做监控，当指令的结果被其中一个操作数污染后，把结果数据对应的影子内存设置为一个指针，指向源污染点操作数指向的数据结构。 污点误用检查：在正确标记污点数据并对污点数据的传播进行实时跟踪后，就需要对攻击做出正确的检测即检测污点数据是否有非法使用的情况。 动态污点分析的优缺点： 优点：误报率较低，检测结果的可信度较高。 缺点： 漏报率较高：由于程序动态运行时的代码覆盖率决定的。 平台相关性较高：特定的动态污点分析工具只能够解决在特定平台上运行的程序。 资源消耗大：包括空间上和时间上。 动态污点分析的方法实现1.污点数据标记污点数据通常主要是指软件系统所接受的外部输入数据，在计算机中，这些数据可能以内存临时数据的形式存储，也可能以文件的形式存储。当程序需要使用这些数据时，一般通过函数或系统调用来进行数据访问和处理，因此只需要对这些关键函数进行监控，即可得到程序读取或输出了什么污点信息。另外对于网络输入，也需要对网络操作函数进行监控。 识别出污点数据后，需要对污点进行标记。污点生命周期是指在该生命周期的时间范围内，污点被定义为有效。污点生命周期开始于污点创建时刻，生成污点标记，结束于污点删除时刻，清除污点标记。 污点创建 将来自于非可靠来源的数据分配给某寄存器或内存操作数时 将已经标记为污点的数据通过运算分配给某寄存器或内存操作数时 污点删除 将非污点数据指派给存放污点的寄存器或内存操作数时 将污点数据指派给存放污点的寄存器或内存地址时，此时会删除原污点，并创建新污点 一些会清除污点痕迹的算数运算或逻辑运算操作时 2.污点动态跟踪当污点数据从一个位置传递到另一个位置时，则认为产生了污点传播。污点传播规则： 指令类型 传播规则 举例说明 拷贝或移动指令 T(a)&lt;-T(b) mov a， b 算数运算指令 T(a)&lt;-T(b) add a， b 堆栈操作指令 T(esp)&lt;-T(a) push a 拷贝或移动类函数调用指令 T(dst)&lt;-T(src) call memcpy 清零指令 T(a)&lt;-false xor a， a 注：T(x) 的取值分为 true 和 false 两种，取值为 true 时表示 x 为污点，否则 x 不是污点。 对于污点信息流，通过污点跟踪和函数监控，已经能够进行污点信息流流动方向的分析。但由于缺少对象级的信息，仅靠指令级的信息流动并不能完全给出要分析的软件的确切行为。因此，需要在函数监控的基础上进行视图重建，如获取文件对象和套接字对象的详细信息，以方便进一步的分析工作。 根据漏洞分析的实际需求，污点分析应包括两方面的信息： 污点的传播关系，对于任一污点能够获知其传播情况。 对污点数据进行处理的所有指令信息，包括指令地址、操作码、操作数以及在污点处理过程中这些指令执行的先后顺序等。 污点动态跟踪的实现通常使用： 影子内存：真实内存中污点数据的镜像，用于存放程序执行的当前时刻所有的有效污点。 污点传播树：用于表示污点的传播关系。 污点处理指令链：用于按时间顺序存储与污点数据处理相关的所有指令。 当遇到会引起污点传播的指令时，首先对指令中的每个操作数都通过污点快速映射查找影子内存中是否存在与之对应的影子污点从而确定其是否为污点数据，然后根据污点传播规则得到该指令引起的污点传播结果，并将传播产生的新污点添加到影子内存和污点传播树中，同时将失效污点对应的影子污点删除。同时由于一条指令是否涉及污点数据的处理，需要在污点分析过程中动态确定，因此需要在污点处理指令链中记录污点数据的指令信息。 3.污点误用检查污点敏感点，即 Sink 点，是污点数据有可能被误用的指令或系统调用点，主要分为： 跳转地址：检查污点数据是否用于跳转对象，如返回地址、函数指针、函数指针偏移等。具体操作是在每个跳转类指令（如call、ret、jmp等）执行前进行监控分析，保证跳转对象不是污点数据所在的内存地址。 格式化字符串：检查污点数据是否用作printf系列函数的格式化字符串参数。 系统调用参数：检查特殊系统调用的特殊参数是否为污点数据。 标志位：跟踪标志位是否被感染，及被感染的标志位是否用于改变程序控制流。 地址：检查数据移动类指令的地址是否被感染。 在进行污点误用检查时，通常需要根据一些漏洞模式来进行检查，首先需要明确常见漏洞在二进制代码上的表现形式，然后将其提炼成漏洞模式，以更有效地指导自动化的安全分析。 动态污点分析的实例分析下面我们来看一个使用动态污点分析的方法检测缓冲区溢出漏洞的例子。 12345678910111213141516void fun(char *str)&#123; char temp[15]； printf(&quot;in strncpy， source： %s\\n&quot;， str)； strncpy(temp， str， strlen(str))； // Sink 点&#125;int main(int argc， char *argv[])&#123; char source[30]； gets(source)； // Source 点 if (strlen(source) &lt; 30) fun(source)； else printf(&quot;too long string， %s\\n&quot;， source)； return 0；&#125; 漏洞很明显， 调用 strncpy 函数存在缓冲区溢出。 程序接受外部输入字符串的二进制代码如下： 12345670x08048609 &lt;+51&gt;： lea eax，[ebp-0x2a]0x0804860c &lt;+54&gt;： push eax0x0804860d &lt;+55&gt;： call 0x8048400 &lt;gets@plt&gt;...0x0804862c &lt;+86&gt;： lea eax，[ebp-0x2a]0x0804862f &lt;+89&gt;： push eax0x08048630 &lt;+90&gt;： call 0x8048566 &lt;fun&gt; 程序调用 strncpy 函数的二进制代码如下： 1234567890x080485a1 &lt;+59&gt;： push DWORD PTR [ebp-0x2c]0x080485a4 &lt;+62&gt;： call 0x8048420 &lt;strlen@plt&gt;0x080485a9 &lt;+67&gt;： add esp，0x100x080485ac &lt;+70&gt;： sub esp，0x40x080485af &lt;+73&gt;： push eax0x080485b0 &lt;+74&gt;： push DWORD PTR [ebp-0x2c]0x080485b3 &lt;+77&gt;： lea eax，[ebp-0x1b]0x080485b6 &lt;+80&gt;： push eax0x080485b7 &lt;+81&gt;： call 0x8048440 &lt;strncpy@plt&gt; 首先，在扫描该程序的二进制代码时，能够扫描到 call &lt;gets@plt&gt;，该函数会读入外部输入，即程序的攻击面。确定了攻击面后，我们将分析污染源数据并进行标记，即将 [ebp-0x2a] 数组（即源程序中的source）标记为污点数据。程序继续执行，该污染标记会随着该值的传播而一直传递。在进入 fun() 函数时，该污染标记通过形参实参的映射传递到参数 str 上。然后运行到 Sink 点函数 strncpy()。该函数的第二个参数即 str 和 第三个参数 strlen(str) 都是污点数据。最后在执行 strncpy() 函数时，若设定了相应的漏洞规则（目标数组小于源数组），则漏洞规则将被触发，检测出缓冲区溢出漏洞。 污点分析在实际应用中的关键技术污点分析被广泛地应用在系统隐私数据泄露、安全漏洞等问题的检测中.在实际应用过程中,由于系统框架、语言特性等方面的差异,通用的污点分析技术往往难以适用.比如:系统框架的高度模块化以及各模块之间复杂的调用关系导致污点源到汇聚点的传播路径变得复杂、庞大,采用通用的污点分析技术可能面临开销难以接受的问题;通用的污点分析技术对新的语言特性支持有限等.为此,需要针对不同的应用场景,对通用的污点分析技术进行扩展或定制 本节以两个代表性的应用场景——智能手机的隐私泄漏检测和Web应用安全漏洞检测为切入点,总结近10年来污点分析技术在上述领域的应用实践过程中所面临的问题和关键解决技术 检测智能手机隐私泄露针对Android的污点传播分析也围绕组件展开,按照传播可能通过的模块的不同,分为组件内污点传播、组件间污点传播、组件与库函数之间的污点传播这3类(如图7所示).接下来将分别介绍针对这3类传播问题的静态和动态污点传播分析技术 静态污点分析1.组件内污点传播分析组件内部污点分析面临的主要问题是如何构建完整的分析模型.不同于传统的C/C++程序(有唯一的Main函数入口),Android应用程序存在有多个入口函数的情况.这个情况源于Android应用程序复杂的运行生命周期(例如onCreate,onStart,onResume,onPause等)以及程序中大量存在的回调函数和异步函数调用.由于任何的程序入口都有可能是隐私数据的来源,在静态的污点分析开始之前必须构建完整的应用程序模型,以确保程序中每一种可能的执行路径都会被静态污点传播分析覆盖到 LeakMiner[^4]和CHEX[^5]尝试使用增量的方法构建系统调用图. Arzt等人设计的FlowDroid[^6]提出了一种更系统的构建Android程序完整分析模型的方法:首先,通过XML配置文件提取与Android生命周期相关的入口函数,将这些方法作为节点,并根据Android生命周期构建调用图(如图8所示);其次,对于生命周期内的回调函数,在该调用图的基础上增加不透明谓词节点(即图8中菱形的P节点);然后,增量式地将回调函数加入这个函数调用图;最后,将调用图上所有的执行入口连接到一个虚假的Main函数上.FlowDroid中的一次合法的执行,就是对调用图进行的一次遍历 Gordon等人[^7]提出的DroidSafe使用Android设备实现(Android device implementation)来构建Android的完整分析模型.Android设备实现是对Android运行环境的一个简单模拟,它使用Java语言,结合Android Open Source Porject(AOSP),实现了与原Android接口语义等价的模型,并使用精确分析存根(accurate analysis stub)将AOSP代码之外的函数加入到模型中 2.组件间污点传播分析即使正确分析了组件内的数据流关系,污点数据仍然可能通过组件间的数据流来传递,从而造成信息泄露.如上图7左侧所示,即使保证了对组件A内部污点传播的精确分析,组件A仍然可能通过调用方法startActivityforResult()将信息传递给组件B,再通过组件B产生泄露.因此,针对Android应用的污点分析还需要分析出组件间所有可能的数据流信息.组件间通信是通过组件发送Intent消息对象完成的.Intent按照参数字段是否包含目标组件名称分为显式Intent和隐式Intent.如图9所示:显式Intent对象使用一个包含目标组件名称的参数显式地指定通信的下一个组件;隐式Intent使用action,category等域隐式地让Android系统通过Intent Filter自动选择一个组件调用.目前,解决该问题的主要思想是利用Intent参数信息分析组件间的数据流 解决组件间数据流的前提是解析Intent的目的地,解析Intent目的地包括解析显式Intent的目的地和隐式Intent的目的地.由于显式Intent的目的地可以直接通过初始化Intent的地址字符串参数获得,目前,解析显式Intent目的地的常用方法是使用字符串分析工具提取Intent中字符串参数的信息. 解析隐式Intent目的地的主要方法是分析配置文件信息与Intent Filter注册器之间的映射关系,建立发送Intent组件和接受Intent组件之间的配对关系.在解析出Intent目的地之后,问题的重点转移到如何提高组件间数据流分析的精度上. Klieber等人尝试在已经建立好的组件内污点分析的基础上,结合推导规则来分析组件间数据流.在分析之前,需要收集组件内部的污点源和汇聚点以及组件内Intent的发送目的地标签等信息.表4和表5给出了推导规则的前提定义和具体的推导规则,其中,一次完整的分析是指根据已知组件内部的信息src→sinksrc→sin**k以及推导规则识别所有src′→sink′src′→sin**k′的流集合 Octeau等人尝试使用现有的程序分析方法提高组件间数据流分析的精度,他们将组件间数据流分析问题转化成IDE(interprocedural distributive environment)问题[58]进行求解.DroidSafe设计了一种对象敏感的别名分析技术,在此基础上提供的精度优化方法包括:提取Intent的目的地的字符串参数、将Intent目的地的初始化函数嵌入到目的组件当中以提高别名分析的精度,同时,增加处理Android Service的支持 组件与库函数之间的污点传播分析组件与库函数之间的污点传播分析面临的主要问题包括对Android库函数自身庞大的代码量的分析以及组件和某些库函数使用的实现语言不同(Android组件通常用Java实现,而本地库则采用C/C++代码编写)这两方面 动态污点分析Android系统中的动态污点同样需要分析组件内污点传播、组件间污点传播以及组件代码与本地库之间的污点传播.动态污点分析面临的主要挑战是系统信息除了在系统内部通过DEX指令传播以外,还会经过其他的通道,如本地库、网络、文件等. 1.组件内的污点传播2.组件间的污点传播3.组件与本地库函数间的污点传播包括污点数据通过本地库代码或文件进行传播.TaintDroid通过设计后置条件对本地代码的函数进行污点传播.后置条件为: (a) 所有本地代码访问的外部变量都会被标记上污点标签; (b) 根据预定义规则将被赋值的函数返回值也标记上污点标签 总结污点分析作为信息流分析的一种实践技术,被广泛应用于互联网及移动终端平台上应用程序的信息安全保障中.本文介绍了污点分析的基本原理和通用技术,并针对近年来污点分析在解决实际应用程序安全问题时遇到的问题和关键解决技术进行了分析综述.不同于基于安全类型系统的信息流分析技术,污点分析可以不改变程序现有的编程模型或语言特性,并提供精确信息流传播跟踪.在实际应用过程中,污点分析还需要借助传统的程序分析技术的支持,例如静态分析中的数据流分析、动态分析中的代码重写等技术.另外,结合测试用例生成技术、符号执行技术以及虚拟机技术,也会给污点分析带来更多行之有效的解决方案 参考文献[^1]:Vogt P, Nentwich F, Jovanovic N, Kirda E, Kruegel C, Vigna G. Cross site scripting prevention with dynamic data tainting and static analysis. In: Proc. of the NDSS 2007. 2007. 12. http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.72.4505[^2]:Gordon MI, Kim D, Perkins JH, Gilham L, Nguyen N, Rinard MC. Information flow analysis of Android applications in DroidSafe. In: Proc. of the NDSS 2015. 2015. [doi: 10.14722/ndss.2015.23089][^3]:Rasthofer S, Arzt S, Bodden E. A machine-learning approach for classifying and categorizing android sources and sinks. In: Proc. of the Network and Distributed System Security Symp. (NDSS). 2014. [doi: 10.14722/ndss.2014.23039] [^4]:Yang Z, Yang M. Leakminer: Detect information leakage on Android with static taint analysis. In: Proc. of the Software Engineering. IEEE, 2012. 101−104. [doi: 10.1109/WCSE.2012.26][^5]:Lu L, Li Z, Wu Z, Lee W, Jiang G. Chex: Statically vetting Android apps for component hijacking vulnerabilities. In: Proc. of the 2012 ACM Conf. on Computer and Communications Security. ACM Press, 2012. 229−240. [doi: 10.1145/2382196.2382223][^6]:Arzt S, Rasthofer S, Fritz C, Bodden E, Bartel A, Klein J, Le Traon Y, Octeau D, McDaniel P. Flowdroid: Precise context, flow, field, object-sensitive and lifecycle-aware taint analysis for Android apps. ACM SIGPLAN Notices, 2014,49(6):259−269. [doi: 10.1145/2594291.2594299][^7]:Gordon MI, Kim D, Perkins JH, Gilham L, Nguyen N, Rinard MC. Information flow analysis of Android applications in DroidSafe. In: Proc. of the NDSS 2015. 2015. [doi: 10.14722/ndss.2015.23089] 原文链接","categories":[{"name":"程序分析","slug":"程序分析","permalink":"https://lwy0518.github.io/categories/%E7%A8%8B%E5%BA%8F%E5%88%86%E6%9E%90/"}],"tags":[{"name":"数据流","slug":"数据流","permalink":"https://lwy0518.github.io/tags/%E6%95%B0%E6%8D%AE%E6%B5%81/"},{"name":"污点分析","slug":"污点分析","permalink":"https://lwy0518.github.io/tags/%E6%B1%A1%E7%82%B9%E5%88%86%E6%9E%90/"},{"name":"依赖关系","slug":"依赖关系","permalink":"https://lwy0518.github.io/tags/%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB/"},{"name":"Source/Sink","slug":"Source-Sink","permalink":"https://lwy0518.github.io/tags/Source-Sink/"}],"author":"lwy"},{"title":"程序分析之中间表示（Intermediate Representation）","slug":"程序分析之中间表示（Intermediate-Representation）","date":"2021-12-10T07:43:30.000Z","updated":"2021-12-26T07:47:51.812Z","comments":true,"path":"2021/12/10/程序分析之中间表示（Intermediate-Representation）/","link":"","permalink":"https://lwy0518.github.io/2021/12/10/%E7%A8%8B%E5%BA%8F%E5%88%86%E6%9E%90%E4%B9%8B%E4%B8%AD%E9%97%B4%E8%A1%A8%E7%A4%BA%EF%BC%88Intermediate-Representation%EF%BC%89/","excerpt":"静态分析程序静态分析（Program Static Analysis）是指在不运行代码的方式下，通过词法分析、语法分析、控制流、数据流分析等技术对程序代码进行扫描，验证代码是否满足规范性、安全性、可靠性、可维护性等指标的一种代码分析技术。静态分析技术向模拟执行的技术发展以能够发现更多传统意义上动态测试才能发现的缺陷，从而提高开发效率和软件质量。本文介绍部分在静态代码分析中使用的中间表示的概念，主要包括抽象语法树、三地址码、SSA形式，及CFG和RTL等概念","text":"静态分析程序静态分析（Program Static Analysis）是指在不运行代码的方式下，通过词法分析、语法分析、控制流、数据流分析等技术对程序代码进行扫描，验证代码是否满足规范性、安全性、可靠性、可维护性等指标的一种代码分析技术。静态分析技术向模拟执行的技术发展以能够发现更多传统意义上动态测试才能发现的缺陷，从而提高开发效率和软件质量。本文介绍部分在静态代码分析中使用的中间表示的概念，主要包括抽象语法树、三地址码、SSA形式，及CFG和RTL等概念 中间表示（Intermediate Representation）抽象语法树（Abstract Syntax Tree，AST）在介绍AST时，我们用一个简单的例子：x = 3 + 4 * y 这个表达式作为例子来进行介绍： AST： 是源代码的抽象语法结构的树状表示，树上的每个节点都表示源代码中的一种结构，之所以说是抽象的，是因为抽象语法树并不会表示出真实语法出现的每一个细节 抽象语法树，其实就是用树状结构表示语法结构，也没有说必须是什么形式，只要能忠实地反映出源码的格式即可 当然，一般资料中在进行介绍时，都是以操作符作为根节点，画个树状的结构来表示，这里咱们也简单画一个意思一下，如下图： 三地址码（Three Address Code，TAC/3AC）三地址码是一种有意思的中间表示，当前也是编译原理中用得最火的。这里我给大家梳理一种我的理解上的概念，那就是，刨掉赋值操作，最多只有一个操作符，先看下面的几个例子 x = y bop z x = uop y x = y goto L 如上，第一个，除了赋值之外，只有 bop 一个操作符，第二个，只有 uop 一个操作符，第三个没有操作符，同时，操作符的概念可以进一步扩展到函数调用等，例如 call fun(a, b, c, d)，虽然有四个操作数，但是我们认为 call fun只是一个操作符。 如上面 x = 3 + 4 * y 转换为三地址码如下： 123456t1 = 4t2 = yt3 = t1 * t2t4 = 3t5 = t4 + t3x = t5 当然，有些形式下会减少临时变量的使用，尽量复用原来的变量或者常量，形成的三地址码如下： 12t1 = 4 * yx = 3 + t1 三地址码最初只是处理类似于 x = y bop z 这种形式的语句，而提出来的“三个操作数”的意思，随着语法的扩充，也并不是完全就是“三地址”。 AST和三地址码对比以下介绍几点AST和三地址码的对比特点： 源码相关性 AST中的节点与输入源代码中的各个语法元素一一对应，忠实地体现了源码的内容和语法特性，因此AST与源码强相关；三地址码就是从AST进一步抽象的一种中间表示，更接近机器语言，可以认为和语言无关，是连接前后端的一种中间表示 变化频繁程度 因为AST需要忠实地体现出源代码的语法元素，因此在对应的编程语言升级时，对应的AST必然会跟着发生变化，比如Java，从Java7变成Java8，增加了大量的Lambda表达式、函数引用等特性，所以AST节点也需要增加这些语法节点，所以AST的版本需要随着语言发布而不断变化。 但是三地址码是一种经过处理的语言无关的中间表示，即使源代码结构变化，AST结构变化，但是转换后的三地址码是稳定的，不会经常发生变化，构造在三地址码上面的分析算法就相对比较稳定 结构 AST体现源码的结构，需要匹配源码的语法，因此一般结构比较复杂，而三地址经过处理，一般比较紧凑，简单。例如，Java中，对 for，while，do while 有多种不同的循环方式，但是，其实内容大同小异，但是在AST层面，就是不一样的，但是转换为三地址码后，所表达的控制语义是完全一样的 表达信息 AST表达了源码的信息，因此可以在AST上做程序结构的检查，但是三地址码中，可以更好地包含了程序控制流和数据流信息，能进行更深层次的流敏感分析，过程间分析，上下文敏感分析和对象敏感分析等等，从而实现各种更高难度的程序漏洞检查。 同时，三地址码因为是语言无关的， 所以在部分静态代码分析工具实现时，会对不同的三地址码，实现一个分析引擎，只是通过开发不同语言的规则，实现对不同语言的能力的覆盖，而AST是无法做到这一点的。因此，三地址码也被认为是静态代码分析的基础 **静态单赋值形式**（Static Single Assignment Form，SSA）文件地址 SSA概念及分类 SSA概念 在编译器的设计中，静态单赋值形式通常简写为SSA form或是SSA，是中介码的特性，每个变数仅被赋值一次。在原始的IR中，已存在的变数可被分割成许多不同的版本，在许多教科书当中通常会将旧的变数名称加上一个下标而成为新的变数名称，以至于标明每个变数及其不同版本。在SSA中，UD链（use-define chain，赋值代表define，使用变数代表use）是非常明确，而且每个仅包含单一元素 以下面的代码为例： 12345678910public void test(int x) &#123; int y = 0; if (x &gt; 3) &#123; y = x + 4; &#125; else &#123; y = 10; &#125; int z = y + 3; System.out.println(z);&#125; 以下图为例，左图是原始代码，里面有分支， y 变量在不同路径中有不同赋值，最后打印 z的值。右图是等价的 SSA 形式，y 变量在两个分支中被改写为 y2, y3，在控制流交汇处插入φ函数，合并了来自不同边的 y2, y3值, 赋给 y4 最后z由y4生成 其实要讲SSA形式，就不能离开对DU Chain（Define-Use Chain）和UD Chain（Use-Define Chain）的介绍，因为很多地方对SSA的概念的介绍，都是从DU Chain和UD Chain引起的。Use-Define Chain 是一个数据结构，包含一个Define变量，以及它的全部Use的集合。相对的，Define-Use Chain 包含一个Use变量，以及它的全部 Define的集合。 另外一种SSA的描述，就是在 Define-Use Chain中，每一个Use变量，只会有一个Define，例如，在前面例子中，z = y + 3 中，因为此时 y可能在两个分支中赋值，因此，对于变量 z = y + 3 中，y 的Use来说，有两个 Define，但是，通过更改为 SSA形式，z = y + 3 中，y只有一个 Define，那就是 y4。因此，通过将三地址码转为SSA形式，可以很大程度上，简化Use-Define Chain和Define-Use Chain。 SSA分类 SSA 有几种不同分类（主要是最小SSA、剪枝SSA、半剪枝SSA，另外两种严格SSA和最大SSA，大部分资料上都没有看到，只是在少部分资料中有见到，所以简单提一下） 最小SSA 最小SSA有以下特点：同一原始名字的两个不同定义的路径汇合处都插入一个φ函数。这样得到符合两大特征的且拥有最少φ函数数量的 SSA 形式。但是这里的最小不包含优化效果，比如死代码消除。如上面图2.1节，就是一个最小SSA形式 剪枝SSA 如果变量在基本块的入口处不是活跃 (live) 的，就不必插入φ函数。一种方法是在插入 φ函数的时候计算活跃变量分析。另一种剪枝方式是在最小SSA上做死代码消除，删掉多余的φ函数。如下面的例子，y在分支执行完后，在最后的BB块中不再使用，y已经不再活跃，此时没必要在这个节点添加φ函数，如下面右图红色标出来的位置（说明：虽然不是剪枝SSA，但是仍然是最小SSA） 半剪枝SSA 鉴于剪枝 SSA 的成本，可以稍微折衷一点。插入φ函数前先去掉非跨越基本块的变量名。这样既减少了名字空间也没有计算活跃变量集的开销。如下图所示，y变量除了Define，并没有Use，所以，变量y其实可以去掉，如下右图 严格SSA 如果一个 SSA中，每个Use被其Define支配（如果从程序入口到一个结点 A 的所有路径，都先经过结点B，则称A被B支配），那么称为严格 SSA（实际上，在强类型语言中，这种情况比较少，因为没有定义，就不允许使用，在少数动态类型语言中，允许没有定义就可以使用的才有这类问题） 最大SSA 最大SSA是相对最小SSA而言的，就是在每个汇合点处为每个变量放置一个φ函数。很显然，这种方法会导致SSA的使用效率最差，用户体验也很差，我估计谁生成的SSA是这样的形式，会被使用的人打死的 SSA形式和普通三地址码对比其实对比SSA形式和普通的三地址码形式，只有一个区别，那就是，SSA形式，对于每个Use，只会有一个Define。两者在一定程度上，还是非常类似的。那么主要对比在于两种形式的各自的优缺点： SSA形式相对于三地址码，会引入大量的额外的临时变量，同时需要插入φ函数，还需要维护这些临时变量到原始变量的映射关系（当然，仁者见仁，智者见智，也有资料觉得这些额外的临时变量可以忽略，驳斥这个观点为谬论，不过的确还是有其不舒服的地方的） SSA形式的优势在于，SSA形式简化了DU Chain和UD chain，构建了一种稀疏结构，可以简化数据流分析（一般基于三地址码，需要基于传统的数据流分析来进行分析，称为dense分析，基于SSA形式，可以构造值依赖关系，基于值流分析，也称为sparse分析，同时，SSA形式也隐含了一定的程序流信息） SSA形式相比于普通三地址码，可以优化常量传播、值依赖分析、死代码、重复代码删除等 控制流图（Control Flow Graph，CFG） 也叫控制流程图，是一个过程或程序的抽象表现，是用在编译器中的一个抽象数据结构，由编译器在内部维护，代表了一个程序执行过程中会遍历到的所有路径。它用图的形式表示一个过程内所有基本块执行的可能流向, 也能反映一个过程的实时执行过程 基本块（Basic Block） 特点： 单入口、单出口、每个块内的语句都是按顺序执行的，不能有分支和跳转 只能从第一条语句进入该基本块，不能够以某种方式跳入该基本块的中间 基本块内的语句在执行时必须从最后一条语句离开，不能够执行到一半跳转到其它的基本块 CFG CFG是一个由基本块组成的有向图，每个节点都是一个基本块。如果程序的执行路径可能从一个基本块$B_1$进入另一个基本块**$B_2$，$B_1$有一条指向$ B_2 $**的边 其中： $N:$表示所有基本块节点的集合 $E:$表示所有边的集合 $n_0:$表示首节点 CFG具有如下的两条性质： CFG 必然有唯一的一个入口点 首节点必然支配CFG中其他的所有节点（即从首节点到CFG上其他任何一个节点都有一条路可以连通） 寄存器传输语言（Register Transfer Language，RFL） 又译为暂存器转换语言、寄存器转换语言，一种中间语言，使用于编译器中。与汇编语言很接近。寄存器传递语言被用于描述一个架构中寄存器传输级上的数据流。 在学术论文和教科书中，寄存器传递语言被认为是一种与架构无关的汇编语言。GCC的中间语言，也被称为寄存器传递语言，风格类似于LISP。GCC的前端（front-end）会先将编程语言转译成RTL，之后再利用后端（back-end）转化成机器代码 以下一些文章用到相关技术： Vandal Securify2 MadMax Gigahorse Ethainter 原文链接","categories":[{"name":"程序分析","slug":"程序分析","permalink":"https://lwy0518.github.io/categories/%E7%A8%8B%E5%BA%8F%E5%88%86%E6%9E%90/"}],"tags":[{"name":"中间表示（IR）","slug":"中间表示（IR）","permalink":"https://lwy0518.github.io/tags/%E4%B8%AD%E9%97%B4%E8%A1%A8%E7%A4%BA%EF%BC%88IR%EF%BC%89/"},{"name":"CFG","slug":"CFG","permalink":"https://lwy0518.github.io/tags/CFG/"},{"name":"AST","slug":"AST","permalink":"https://lwy0518.github.io/tags/AST/"},{"name":"TAC","slug":"TAC","permalink":"https://lwy0518.github.io/tags/TAC/"},{"name":"SSA","slug":"SSA","permalink":"https://lwy0518.github.io/tags/SSA/"},{"name":"RTL","slug":"RTL","permalink":"https://lwy0518.github.io/tags/RTL/"}],"author":"lwy"},{"title":"hexo博客迁移到另外一台电脑","slug":"hexo博客迁移到另外一台电脑","date":"2021-12-10T07:10:46.000Z","updated":"2022-02-13T13:14:52.467Z","comments":true,"path":"2021/12/10/hexo博客迁移到另外一台电脑/","link":"","permalink":"https://lwy0518.github.io/2021/12/10/hexo%E5%8D%9A%E5%AE%A2%E8%BF%81%E7%A7%BB%E5%88%B0%E5%8F%A6%E5%A4%96%E4%B8%80%E5%8F%B0%E7%94%B5%E8%84%91/","excerpt":"复制原电脑上的数据 注：不需要全部复制 _config.yml：站点配置文件 package.json：应用程序数据，指明hexo的版本等信息，类似于一般软件中的关于按钮 scaffolds/：layout模板文件目录，其中的md文件可以添加编辑 source/： 文章源码目录，该目录下的markdown和html文件均会被hexo处理。该页面对应repo的根目录，404文件、favicon.ico文件，CNAME文件等都应该放这里，该目录下可新建页面目录。 drafts：草稿文章 posts：发布文章 themes/：主题文件","text":"复制原电脑上的数据 注：不需要全部复制 _config.yml：站点配置文件 package.json：应用程序数据，指明hexo的版本等信息，类似于一般软件中的关于按钮 scaffolds/：layout模板文件目录，其中的md文件可以添加编辑 source/： 文章源码目录，该目录下的markdown和html文件均会被hexo处理。该页面对应repo的根目录，404文件、favicon.ico文件，CNAME文件等都应该放这里，该目录下可新建页面目录。 drafts：草稿文章 posts：发布文章 themes/：主题文件 安装配置环境 安装node 安装git 再参考上一篇文章 下载相关插件123456# 将文章部署到github上的模块$ cnpm install hexo-deployer-git --save# 安装RSS插件$ cnpm install hexo-generator-feed --save# 添加Sitemap,加速网页收录速度$ cnpm install hexo-generator-sitemap --save","categories":[{"name":"博客","slug":"博客","permalink":"https://lwy0518.github.io/categories/%E5%8D%9A%E5%AE%A2/"}],"tags":[{"name":"博客","slug":"博客","permalink":"https://lwy0518.github.io/tags/%E5%8D%9A%E5%AE%A2/"},{"name":"hexo","slug":"hexo","permalink":"https://lwy0518.github.io/tags/hexo/"}],"author":"lwy"},{"title":"EOSIO环境搭建及创建账户","slug":"EOSIO环境搭建及创建账户","date":"2021-12-10T02:01:59.000Z","updated":"2021-12-22T02:58:52.714Z","comments":true,"path":"2021/12/10/EOSIO环境搭建及创建账户/","link":"","permalink":"https://lwy0518.github.io/2021/12/10/EOSIO%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E5%8F%8A%E5%88%9B%E5%BB%BA%E8%B4%A6%E6%88%B7/","excerpt":"环境搭建官方文档 钱包和账户","text":"环境搭建官方文档 钱包和账户 Nodeos（node + eos = nodeos）：运行节点的核心服务守护进程，用于区块生产、API 端点或本地开发 Cleos (cli + eos = cleos) ： 与区块链交互的命令行界面 (via nodeos) 和管理钱包 (via keosd) Keosd (key + eos = keosd) ： 管理钱包中的 EOSIO 密钥并为数字签名提供安全飞地的组件 可以参考下列解读图： 创建钱包1$ cleos wallet create -n wallet_name --to-console 其中：**-n是指定钱包名称（如果不指定则会默认生成一个default钱包）， wallet_name是你需要创建的钱包名（以下用lwy代替），–to-console**是将密钥输出到控制台（记得一定要保存这个密钥，因为要用于钱包的解锁） 查看创建的钱包1$ cleos wallet list 其中带**”*”**号指已解锁的钱包 钱包的解锁与加锁 解锁（会提示你输入钱包密钥） 1$ cleos wallet unlock -n lwy --password 加锁（不需要输入密钥） 1$ cleos wallet lock -n lwy 生成和导入公钥-私钥对生成公钥-私钥对 为了简单可以直接用一个公钥-私钥对（记得保存私钥，一定！一定 ！） 1$ cleos create key --to-console 导入公钥-私钥对 如果lwy钱包还没解锁，记得先解锁 将生成的公钥-私钥对导入lwy钱包内 1$ cleos wallet import -n lwy --private-key 5J9rzfgPNLuCL9j4AEc4RYcchvJQPaPNUtWuToRSeFAEayitM8v 创建和管理账户命令行及参数说明1$ cleos create account authorizing_account NEW_ACCOUNT OWNER_KEY ACTIVE_KEY 注：在这OWNER_KEY与ACTIVE_KEY是同一个，因为我为了简单只生成了一个公钥-私钥对 authorizing_account 是为帐户创建提供资金的帐户的名称 new_account 是您要创建的帐户的名称 owner_key是分配给帐户owner权限的公钥 active_key是分配给您帐户的active权限的公钥 新帐户名称必须符合以下准则： 必须在12个字符以内，包括12字符。 只能包含以下符号：.12345abcdefghijklmnopqrstuvwxyz 请注意，账户名称不允许使用6,7,8,9,0。 使用eosio初始账户创建新账户eosio帐户是用于引导EOSIO节点的特殊帐户。由于我们没有其他账户，所以用初始eosio账户来创建新帐户 eosio帐户的密钥可以在nodeos配置文件中找到，位于~/.local/share/eosio/nodeos/config/config.ini。 同时，记得解锁lwy钱包 首先，需要将默认账号eosio的私钥导入lwy钱包内： 1$ cleos wallet import -n lwy --private-key 5J9rzfgPNLuCL9j4AEc4RYcchvJQPaPNUtWuToRSeFAEayitM8v 然后，用初始eosio账户来创建新帐户testaccount： 1$ cleos create account eosio testaccount 5J9rzfgPNLuCL9j4AEc4RYcchvJQPaPNUtWuToRSeFAEayitM8v 查看账户信息1$ cleos get account testaccount -j -j 指信息以json格式输出 至此，EOSIO环境搭建以及账户创建已完成！！","categories":[{"name":"EOSIO","slug":"EOSIO","permalink":"https://lwy0518.github.io/categories/EOSIO/"}],"tags":[{"name":"EOSIO","slug":"EOSIO","permalink":"https://lwy0518.github.io/tags/EOSIO/"},{"name":"智能合约","slug":"智能合约","permalink":"https://lwy0518.github.io/tags/%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6/"}],"author":"lwy"},{"title":"EOSIO部署全过程","slug":"EOSIO部署全过程","date":"2021-12-10T01:19:12.000Z","updated":"2021-12-22T02:58:49.459Z","comments":true,"path":"2021/12/10/EOSIO部署全过程/","link":"","permalink":"https://lwy0518.github.io/2021/12/10/EOSIO%E9%83%A8%E7%BD%B2%E5%85%A8%E8%BF%87%E7%A8%8B/","excerpt":"","text":"","categories":[{"name":"EOSIO","slug":"EOSIO","permalink":"https://lwy0518.github.io/categories/EOSIO/"}],"tags":[{"name":"EOSIO","slug":"EOSIO","permalink":"https://lwy0518.github.io/tags/EOSIO/"},{"name":"数据流","slug":"数据流","permalink":"https://lwy0518.github.io/tags/%E6%95%B0%E6%8D%AE%E6%B5%81/"},{"name":"编译器","slug":"编译器","permalink":"https://lwy0518.github.io/tags/%E7%BC%96%E8%AF%91%E5%99%A8/"},{"name":"漏洞检测","slug":"漏洞检测","permalink":"https://lwy0518.github.io/tags/%E6%BC%8F%E6%B4%9E%E6%A3%80%E6%B5%8B/"},{"name":"静态分析","slug":"静态分析","permalink":"https://lwy0518.github.io/tags/%E9%9D%99%E6%80%81%E5%88%86%E6%9E%90/"},{"name":"Datalog","slug":"Datalog","permalink":"https://lwy0518.github.io/tags/Datalog/"},{"name":"CFG","slug":"CFG","permalink":"https://lwy0518.github.io/tags/CFG/"},{"name":"IR","slug":"IR","permalink":"https://lwy0518.github.io/tags/IR/"}],"author":"lwy"},{"title":"数据流分析","slug":"数据流分析","date":"2021-12-07T07:31:36.000Z","updated":"2022-02-13T13:10:00.586Z","comments":true,"path":"2021/12/07/数据流分析/","link":"","permalink":"https://lwy0518.github.io/2021/12/07/%E6%95%B0%E6%8D%AE%E6%B5%81%E5%88%86%E6%9E%90/","excerpt":"基本原理数据流分析是一种用来获取相关数据沿着程序执行路径流动的信息分析技术。分析对象是程序执行路径上的数据流动或可能的取值 优点：具有更强的分析能力，适合需要考虑控制流信息且变量属性之操作十分简单的静态分析问题 缺点：分析效率低，过程间分析和优化算法复杂，编程工作量大，容易出错且效率低 一个数据流分析框架(D, L, F)包含: D：数据流的方向，前向或者后向 L：包含数值作用域V和操作符meet ⊓ 或 join ⊔ 的lattice F：一系列V to V的传递函数 Tips: 数据流分析可以看成在一个lattice的数值域上，迭代地使用传递函数和操作符","text":"基本原理数据流分析是一种用来获取相关数据沿着程序执行路径流动的信息分析技术。分析对象是程序执行路径上的数据流动或可能的取值 优点：具有更强的分析能力，适合需要考虑控制流信息且变量属性之操作十分简单的静态分析问题 缺点：分析效率低，过程间分析和优化算法复杂，编程工作量大，容易出错且效率低 一个数据流分析框架(D, L, F)包含: D：数据流的方向，前向或者后向 L：包含数值作用域V和操作符meet ⊓ 或 join ⊔ 的lattice F：一系列V to V的传递函数 Tips: 数据流分析可以看成在一个lattice的数值域上，迭代地使用传递函数和操作符 数据流分析的分类 对程序路径的分析精度分类 流不敏感分析（flow insensitive）：不考虑语句的先后顺序，按照程序语句的物理位置从上往下顺序分析每一语句，忽略程序中存在的分支 流敏感分析（flow sensitive）：考虑程序语句可能的执行顺序，通常需要利用程序的控制流图（CFG） 路径敏感分析（path sensitive）：不仅考虑语句的先后顺序，还对程序执行路径条件加以判断，以确定分析使用的语句序列是否对应着一条可实际运行的程序执行路径 分析程序路径的深度分类 过程内分析（intra-procedure analysis）：只针对程序中函数内的代码 过程间分析（inter-procedure analysis）：考虑函数之间的数据流，即需要跟踪分析目标数据在函数之间的传递过程 上下文不敏感分析（context-insensitive）：将每个调用或返回看做一个 “goto” 操作，忽略调用位置和函数参数取值等函数调用的相关信息 上下文敏感分析（context-sensitive）：对不同调用位置调用的同一函数加以区分 检测程序漏洞基于数据流的源代码漏洞分析的原理如下图所示： 代码建模 该过程通过一系列的程序分析技术获得程序代码模型。首先通过词法分析生成词素的序列，然后通过语法分析将词素组合成抽象语法树。如果需要三地址码，则利用中间代码生成过程解析抽象语法树生成三地址码。如果采用流敏感或路径敏感的方式，则可以通过分析抽象语法树得到程序的控制流图。构造控制流图的过程是过程内的控制流分析过程。控制流还包含分析各个过程之间的调用关系的部分。通过分析过程之间的调用关系，还可以构造程序的调用图。另外，该过程还需要一些辅助支持技术，例如变量的别名分析，Java 反射机制分析，C/C++ 的函数指针或虚函数调用分析等 代码解析：指词法分析、语法分析、中间代码生成以及过程内的控制流分析等基础的分析过程 辅助分析：包括控制流分析等为数据流分析提供支持的分析过程 程序代码建模 漏洞分析系统通常使用树型结构的抽象语法树或者线性的三地址码来描述程序代码的语义。控制流图描述了过程内程序的控制流路径，较为精确的数据流分析通常利用控制流图分析程序执行路径上的某些行为。调用图描述了过程之间的调用关系，是过程间分析需要用到的程序结构 漏洞分析规则 漏洞分析规则是检测程序漏洞的依据。对于分析变量状态的规则，可以使用状态自动机来描述。对于需要分析变量取值的情况，则需要指出应该怎样记录变量的取值，以及在怎样的情况下对变量的取值进行何种的检测 程序漏洞通常和程序中变量的状态或者变量的取值相关。状态自动机可以描述和程序变量状态相关的漏洞分析规则，自动机的状态和变量相应的状态对应 静态漏洞分析 数据流分析可以看做一个根据检测规则在程序的可执行路径上跟踪变量的状态或者变量取值的过程。在该过程中，如果待分析的程序语句是函数调用语句，则需要利用调用图进行过程间的分析，以分析被调用函数的内部代码。另外，数据流分析还可以作为辅助技术，用于完善程序调用图和分析变量别名等 赋值语句、控制转移语句和过程调用语句是数据流分析最关心的三类语句 过程内分析 对于抽象语法树的分析，可以按照程序执行语句的过程从右向左、自底向上地进行分析 对于三地址码的分析，则可以直接识别其操作以及操作相关的变量 在流不敏感分析中，常常使用线性扫描的方式依次分析每一条中间表示形式的语句 流敏感的分析或路径敏感的分析，则根据控制流图进行分析。对控制流图的遍历主要是深度优先和广度优先两种方式 如果在分析某段程序中遇到过程调用语句，就分析其调用过程的内部的代码，完成分析之后再回到原来的程序段继续分析 借鉴基本块的分析，给过程设置上摘要，也包含前置条件和后置条件 前置条件记录对基本块分析前已有的相关分析结果 后置条件是分析基本块后得到的结果 处理分析结果 对检测出的漏洞进行危害程度分类等 方法实现数据流分析使用的程序代码模型主要包括程序代码的中间表示以及一些关键的数据结构，利用程序代码的中间表示可以对程序语句的指令语义进行分析 抽象语法树（AST）是程序抽象语法结构的树状表现形式，其每个内部节点代表一个运算符，该节点的子节点代表这个运算符的运算分量。通过描述控制转移语句的语法结构，抽象语法树在一定程度上也描述了程序的过程内代码的控制流结构 例子： 123456while b ≠ 0 if a &gt; b a := a − b else b := b − areturn a 对应的抽象语法树为： 由一组类似于汇编语言的指令组成，每个指令具有不多于三个的运算分量。每个运算分量都像是一个寄存器 静态单赋值形式（SSA）是一种程序语句或者指令的表示形式，在这种表示形式中，所有的赋值都是针对具有不同名字的变量，也就是说，如果某个变量在不同的程序点被赋值，那么在这些程序点上，该变量在静态单赋值形式的表示中应该使用不同的名字。在使用下标的赋值表示中，变量的名字用于区分程序中的不同的变量，下标用于区分不同程序点上变量的赋值情况。另外，如果在一个程序中，同一个变量可能在两个不同的控制流路径中被赋值，并且在路径交汇后，该变量被使用，那么就需要一种被称为 Φ 函数的的表示规则将变量的赋值合并起来 作用：静态单赋值形式对于数据流分析的意义在于，可以简单而直接地发现变量的赋值和使用情况，以此分析数据的流向并发现程序不安全的行为 控制流图（CFG）是指用于描述程序过程内的控制流的有向图。控制流由节点和有向边组成。典型的节点是基本块（BB），即程序语句的线性序列。有向边表示节点之间存在潜在的控制流路径，通常都带有属性（如if语句的true分支和false分支） 调用图（CG）是描述程序中过程之间的调用和被调用关系的有向图。控制图是一个节点和边的集合，并满足如下原则 对程序中的每个过程都有一个节点 对每个调用点都有一个节点 如果调用点 c 调用了过程 p，就存在一条从 c 的节点到 p 的节点的边 静态漏洞分析数据流分析检测漏洞是利用分析规则按照一定的顺序分析代码中间表示的过程 过程内分析：对于抽象语法树的分析，可以按照程序执行语句的过程从右向左、自底向上地进行分析。对于三地址码的分析，则可以直接识别其操作以及操作相关的变量 过程间分析：如果在分析某段程序中遇到过程调用语句，就分析其调用过程的内部的代码，完成分析之后再回到原来的程序段继续分析。另一种思路是借鉴基本块的分析，给过程设置上摘要，也包含前置条件和后置条件 原文链接","categories":[{"name":"程序分析","slug":"程序分析","permalink":"https://lwy0518.github.io/categories/%E7%A8%8B%E5%BA%8F%E5%88%86%E6%9E%90/"}],"tags":[{"name":"程序分析","slug":"程序分析","permalink":"https://lwy0518.github.io/tags/%E7%A8%8B%E5%BA%8F%E5%88%86%E6%9E%90/"},{"name":"数据流","slug":"数据流","permalink":"https://lwy0518.github.io/tags/%E6%95%B0%E6%8D%AE%E6%B5%81/"},{"name":"Def-Use","slug":"Def-Use","permalink":"https://lwy0518.github.io/tags/Def-Use/"},{"name":"中间表示（IR）","slug":"中间表示（IR）","permalink":"https://lwy0518.github.io/tags/%E4%B8%AD%E9%97%B4%E8%A1%A8%E7%A4%BA%EF%BC%88IR%EF%BC%89/"}],"author":"lwy"},{"title":"EOSIO-Vulneribilities","slug":"EOSIO-Vulneribilities","date":"2021-12-07T07:29:37.000Z","updated":"2022-02-13T13:08:46.693Z","comments":true,"path":"2021/12/07/EOSIO-Vulneribilities/","link":"","permalink":"https://lwy0518.github.io/2021/12/07/EOSIO-Vulneribilities/","excerpt":"EOSIO漏洞复现实验环境 nodeos ：2.0.12 EOSIO.CDT（编译器）：1.2.1 实验数据","text":"EOSIO漏洞复现实验环境 nodeos ：2.0.12 EOSIO.CDT（编译器）：1.2.1 实验数据 准备（前一篇文章已经介绍如何创建账户等等操作） 解锁账户（默认锁定时间较短，可以自己修改配置文件使得时间更长） yourcount：指你自己创建的钱包名 1$ cleos wallet unlock -n yourcount --password EOS Fake Transfer复现过程 存在漏洞的合约示例（test.cpp）： 1234567891011if( code == self || action == ::eosio::string_to_name(&quot;onerror&quot;) || code == N(eosio.token)) &#123; print(&quot;receiver:&quot;, name&#123;receiver&#125;, &quot;, code:&quot;, name&#123;code&#125;, &quot;, action:&quot;, name&#123;action&#125;, &quot;\\n&quot;); notified thiscontract( self ); switch( action ) &#123; case ::eosio::string_to_name( &quot;transfer&quot; ): eosio::execute_action( &amp;thiscontract, &amp;notified::transfer ); break; &#125; &#125; // doSomething()&#125; 漏洞产生的原因 由于eosio.token源代码完全公开的，所以任何人都能复制其源代码，并发布一个token（相同的名字、符号和代码），虚假的EOS和官方的唯一不同就是具有不同的发布人（issuer）。或者直接调用漏洞合约的transfer函数进行转账 过程 创建受害者账户 your key：是你自己创建的公钥，需要把公钥导入到你的钱包以及官方钱包eosio 1$ cleos create account eosio victim4 &quot;your key&quot; 部署测试合约test.cpp cleos set contract + 账户名 + 测试合约的wasm字节码所在目录 + -p + 账户@active（默认权限为active，故可加可不加） 1$ cleos set contract victim4 test/ -p victim4 开始模拟攻击 直接调用test.cpp合约的transfer（主要目的看是否trasnfer中的print是否有输出） cleos push action + 账户名 + 需要调用的action + ‘调用action的参数’ + -p + 转账账户 （+ -j） 1$ cleos push action victim4 transfer &#x27;[&quot;eosio&quot;,&quot;victim4&quot;,&quot;10.0000 EOS&quot;,&quot;inlined call&quot;]&#x27; -p eosio -j 说明：加了一个 -j ，说明结果以json的格式进行输出的 结果显示：测试合约的transfer函数被调用了 查询账户余额 cleos get currency balance eosio.token + 查询账户 + token名（EOS) 1$ cleos get currency balance eosio.token victim4 EOS Forged Transfer Notification复现过程 存在漏洞的合约示例（test.cpp）： 123456789void transfer(account_name from, account_name to, asset quantity, string memo) &#123; print(&quot;\\n Receiving transfer message: from &quot;, name&#123;from&#125;, &quot; to &quot;, name&#123;to&#125;, &quot;,&quot;, quantity, &quot;,&quot;, memo); if (from == _self) &#123; print(&quot;have a vulnerability!&quot;); return; &#125; print(&quot;in eosbet transfer,&quot;, name&#123; from &#125;, &quot;,&quot;, name&#123; to &#125;); // doSomething()&#125; 漏洞产生的原因 攻击者在 EOS 网络中控制两个账户 A（发起攻击的账户） 和 B（将收到的通知立即转发给账户C）。通过账户 A 向账户 B 发送真正的 EOS，如图所示，eosio.token 合约在转账成功后会向 账户A、B 发送 notification。当账户 B 收到 notification后，通过调用require_recipient(C)随即将收到的通知转发给部署受害者智能合约的账户C。 过程 创建攻击者账户sender，用于向另一个由攻击者控制的账户notifier your key：是你自己创建的公钥，需要把公钥导入到你的钱包以及官方钱包eosio 1$ cleos create account eosio sender &quot;your key&quot; 创建账户notifier，用于将收到的转账通知转发给受害者 1$ cleos create account eosio notifier &quot;your key&quot; 创建受害者账户 1$ cleos create account eosio victim5 &quot;your key&quot; 账户notifier部署攻击合约eosbethack.cpp cleos set contract + 账户名 + 测试合约的wasm字节码所在目录 + -p + 账户@active（默认权限为active，故可加可不加） 1$ cleos set contract notifier eosbethack/ -p notifier 账户victim5部署攻击合约eosbet.cpp 1$ cleos set contract victim5 eosbet/ -p victim5 开始模拟攻击 账户sender（攻击者）向账户notifier（攻击者）发送EOS（主要目的看是否trasnfer中的print是否有输出） cleos push action + 账户名 + 需要调用的action + ‘调用action的参数’ + -p + 转账账户 （+ -j） 1$ cleos push action eosio.token transfer &#x27;[&quot;sender&quot;,&quot;notifier&quot;,&quot;10.0000 EOS&quot;,&quot;transfer himself&quot;]&#x27; -p sender -j 说明：加了一个 -j ，说明结果以json的格式进行输出的 结果显示：测试合约的transfer函数被调用了 结果显示：","categories":[{"name":"EOSIO","slug":"EOSIO","permalink":"https://lwy0518.github.io/categories/EOSIO/"}],"tags":[{"name":"EOSIO","slug":"EOSIO","permalink":"https://lwy0518.github.io/tags/EOSIO/"},{"name":"漏洞","slug":"漏洞","permalink":"https://lwy0518.github.io/tags/%E6%BC%8F%E6%B4%9E/"},{"name":"智能合约","slug":"智能合约","permalink":"https://lwy0518.github.io/tags/%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6/"},{"name":"防护","slug":"防护","permalink":"https://lwy0518.github.io/tags/%E9%98%B2%E6%8A%A4/"}],"author":"lwy"},{"title":"构建hexo博客过程","slug":"构建hexo博客过程","date":"2021-12-07T07:23:17.000Z","updated":"2021-12-22T02:58:34.388Z","comments":true,"path":"2021/12/07/构建hexo博客过程/","link":"","permalink":"https://lwy0518.github.io/2021/12/07/%E6%9E%84%E5%BB%BAhexo%E5%8D%9A%E5%AE%A2%E8%BF%87%E7%A8%8B/","excerpt":"环境（Windows） git（git version 2.29.2.windows.3） node（v14.17.0）","text":"环境（Windows） git（git version 2.29.2.windows.3） node（v14.17.0） 本地部署 安装cnpm（国内镜像源很慢） 1$ npm install -g cnpm --registry=https://registry.npm.taobao.org “-g”：表示全局安装 验证是否安装成功： 1$ cnpm -v 设置源： 1$ npm config set registry https://registry.npm.taobao.org 安装hexo 1$ cnpm install -g hexo-cli 验证是否安装成功： 1$ hexo -v 创建一个目录（出错直接删掉目录即可） 12$ mkdir myBlog$ cd myBlog 初始化 1$ hexo init 启动hexo 1$ hexo s 访问地址:localhost:4000 GitHub部署 创建一个仓库，仓库名必须为： 1yourname.github.io 安装插件 1$ cnpm install --save hexo-deployer-git 修改配置文件（在文件最下面修改） Windows 下直接可以用文本打开 其他系统可以用vim 1$ vim _config.yml 添加以下内容 1234deploy: type: git repo: https://github.com:yourname/yourname.github.io.git branch: master 部署到远端（可以提前配置好git账号和密码） 1$ hexo d 访问远程博客 1https://yourname.github.io.git 换主题 下载主题（下载到themes目录下） 1$ git clone https://github.com/litten/hexo-theme-yilia.git themes.yilia 修改 Windows 下直接可以用文本打开 其他系统可以用vim 1$ vim _config.yml 修改以下内容 1theme: yilia 重新生成 1$ hexo clean &amp; hexo g &amp; hexo s 远程部署到github 1$ hexo d 遇到的问题 如遇到这个问题，先检查是否是网络的原因 ，多部署几次，如果还是不行，则采用以下方式： 方法一：在当前目录下操作 12345678## 删除git提交内容文件夹$ rm -rf .deploy_git/##执行$ git config --global core.autocrlf false##最后hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 方法二：有可能是你的git repo配置地址不正确,可以将http方式变更为ssh方式，在当前目录下操作 123456789101112##删除git提交内容文件夹$ vim _config.yml##修改deploy: type: git repo:https://github.com/yourname/yourname.github.io.git -&gt; git@github.com:a956551943/weixiaohui.github.io.git branch: master##最后$ hexo clean &amp;&amp; hexo g &amp;&amp; hexo d 方法三：备选，在当前目录下操作 12345##进入depoly文件夹$ cd .deploy_git/##强制推送$ git push -f 遇到新建博客文章部署之后图片不显示的问题 在Typora中的工具栏中的“格式” –&gt; “图像” –&gt; “全局图像设置”中设置如下，此后会在当前目录下生成包含图片同名的文件 另外，在文章开始部分加上以上命令即可解决图片不显示的问题 1&lt;meta name=&quot;referrer&quot; content=&quot;no-referrer&quot;","categories":[{"name":"Blog","slug":"Blog","permalink":"https://lwy0518.github.io/categories/Blog/"}],"tags":[{"name":"hexo","slug":"hexo","permalink":"https://lwy0518.github.io/tags/hexo/"},{"name":"Blog","slug":"Blog","permalink":"https://lwy0518.github.io/tags/Blog/"}],"author":"lwy"}],"categories":[{"name":"Docker","slug":"Docker","permalink":"https://lwy0518.github.io/categories/Docker/"},{"name":"Redis","slug":"Redis","permalink":"https://lwy0518.github.io/categories/Redis/"},{"name":"偏序","slug":"偏序","permalink":"https://lwy0518.github.io/categories/%E5%81%8F%E5%BA%8F/"},{"name":"SSA","slug":"SSA","permalink":"https://lwy0518.github.io/categories/SSA/"},{"name":"hexo","slug":"hexo","permalink":"https://lwy0518.github.io/categories/hexo/"},{"name":"Markdown","slug":"Markdown","permalink":"https://lwy0518.github.io/categories/Markdown/"},{"name":"程序分析","slug":"程序分析","permalink":"https://lwy0518.github.io/categories/%E7%A8%8B%E5%BA%8F%E5%88%86%E6%9E%90/"},{"name":"博客","slug":"博客","permalink":"https://lwy0518.github.io/categories/%E5%8D%9A%E5%AE%A2/"},{"name":"EOSIO","slug":"EOSIO","permalink":"https://lwy0518.github.io/categories/EOSIO/"},{"name":"Blog","slug":"Blog","permalink":"https://lwy0518.github.io/categories/Blog/"}],"tags":[{"name":"Docker","slug":"Docker","permalink":"https://lwy0518.github.io/tags/Docker/"},{"name":"Redis","slug":"Redis","permalink":"https://lwy0518.github.io/tags/Redis/"},{"name":"数据库","slug":"数据库","permalink":"https://lwy0518.github.io/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"分布式","slug":"分布式","permalink":"https://lwy0518.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"偏序","slug":"偏序","permalink":"https://lwy0518.github.io/tags/%E5%81%8F%E5%BA%8F/"},{"name":"Lattice","slug":"Lattice","permalink":"https://lwy0518.github.io/tags/Lattice/"},{"name":"SSA","slug":"SSA","permalink":"https://lwy0518.github.io/tags/SSA/"},{"name":"gcc","slug":"gcc","permalink":"https://lwy0518.github.io/tags/gcc/"},{"name":"算法","slug":"算法","permalink":"https://lwy0518.github.io/tags/%E7%AE%97%E6%B3%95/"},{"name":"编译原理","slug":"编译原理","permalink":"https://lwy0518.github.io/tags/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"},{"name":"hexo","slug":"hexo","permalink":"https://lwy0518.github.io/tags/hexo/"},{"name":"Blog","slug":"Blog","permalink":"https://lwy0518.github.io/tags/Blog/"},{"name":"Typora","slug":"Typora","permalink":"https://lwy0518.github.io/tags/Typora/"},{"name":"Markdown","slug":"Markdown","permalink":"https://lwy0518.github.io/tags/Markdown/"},{"name":"数据流","slug":"数据流","permalink":"https://lwy0518.github.io/tags/%E6%95%B0%E6%8D%AE%E6%B5%81/"},{"name":"污点分析","slug":"污点分析","permalink":"https://lwy0518.github.io/tags/%E6%B1%A1%E7%82%B9%E5%88%86%E6%9E%90/"},{"name":"依赖关系","slug":"依赖关系","permalink":"https://lwy0518.github.io/tags/%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB/"},{"name":"Source/Sink","slug":"Source-Sink","permalink":"https://lwy0518.github.io/tags/Source-Sink/"},{"name":"中间表示（IR）","slug":"中间表示（IR）","permalink":"https://lwy0518.github.io/tags/%E4%B8%AD%E9%97%B4%E8%A1%A8%E7%A4%BA%EF%BC%88IR%EF%BC%89/"},{"name":"CFG","slug":"CFG","permalink":"https://lwy0518.github.io/tags/CFG/"},{"name":"AST","slug":"AST","permalink":"https://lwy0518.github.io/tags/AST/"},{"name":"TAC","slug":"TAC","permalink":"https://lwy0518.github.io/tags/TAC/"},{"name":"RTL","slug":"RTL","permalink":"https://lwy0518.github.io/tags/RTL/"},{"name":"博客","slug":"博客","permalink":"https://lwy0518.github.io/tags/%E5%8D%9A%E5%AE%A2/"},{"name":"EOSIO","slug":"EOSIO","permalink":"https://lwy0518.github.io/tags/EOSIO/"},{"name":"智能合约","slug":"智能合约","permalink":"https://lwy0518.github.io/tags/%E6%99%BA%E8%83%BD%E5%90%88%E7%BA%A6/"},{"name":"编译器","slug":"编译器","permalink":"https://lwy0518.github.io/tags/%E7%BC%96%E8%AF%91%E5%99%A8/"},{"name":"漏洞检测","slug":"漏洞检测","permalink":"https://lwy0518.github.io/tags/%E6%BC%8F%E6%B4%9E%E6%A3%80%E6%B5%8B/"},{"name":"静态分析","slug":"静态分析","permalink":"https://lwy0518.github.io/tags/%E9%9D%99%E6%80%81%E5%88%86%E6%9E%90/"},{"name":"Datalog","slug":"Datalog","permalink":"https://lwy0518.github.io/tags/Datalog/"},{"name":"IR","slug":"IR","permalink":"https://lwy0518.github.io/tags/IR/"},{"name":"程序分析","slug":"程序分析","permalink":"https://lwy0518.github.io/tags/%E7%A8%8B%E5%BA%8F%E5%88%86%E6%9E%90/"},{"name":"Def-Use","slug":"Def-Use","permalink":"https://lwy0518.github.io/tags/Def-Use/"},{"name":"漏洞","slug":"漏洞","permalink":"https://lwy0518.github.io/tags/%E6%BC%8F%E6%B4%9E/"},{"name":"防护","slug":"防护","permalink":"https://lwy0518.github.io/tags/%E9%98%B2%E6%8A%A4/"}]}